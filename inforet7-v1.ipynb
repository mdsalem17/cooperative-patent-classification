{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"inforet7-v1.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"F3PaGM7cebuX"},"source":["#Group member:\n","Phan Anh VU, Mohamed Salem MESSOUD\n","\n","#Domain:\n","G06T IMAGE DATA PROCESSING OR GENERATION"]},{"cell_type":"markdown","metadata":{"id":"auZ9uI86vpan"},"source":["# Download & Install"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0mQYb80taX0D","executionInfo":{"status":"ok","timestamp":1615217821786,"user_tz":-60,"elapsed":25145,"user":{"displayName":"MD Salem Messoud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBYoVQGAImGxJfhgMMK1daBThGNpiaASkBXr4EYpE=s64","userId":"15534871031717137879"}},"outputId":"b758e6c4-be67-4eff-ca37-c48eb3df8240"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2UMDVjOMXIZL"},"source":["import os, shutil\n","from pathlib import Path"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"01X0k5Jgal8R","executionInfo":{"status":"ok","timestamp":1615217861048,"user_tz":-60,"elapsed":1274,"user":{"displayName":"MD Salem Messoud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBYoVQGAImGxJfhgMMK1daBThGNpiaASkBXr4EYpE=s64","userId":"15534871031717137879"}},"outputId":"5b962989-8f6f-4623-9304-e1c617d5d63d"},"source":["#basedir = Path('/content/drive/MyDrive/Education/Master Informatique IA Computer Science AI Paris Saclay/AI/TC3 Information Retrieval/inforet7')\n","basedir = Path('/content/drive/MyDrive/Information Retrieval/inforet/inforet7')\n","print(os.listdir(basedir))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['inforet7-v1.ipynb', 'prodigy-1.10.6-cp36.cp37.cp38.cp39-cp36m.cp37m.cp38.cp39-linux_x86_64.whl', 'manyterms.lower.txt', 'termsg06tpatterns.jsonl', 'annotatedg06t.jsonl', 'annotatedg06t_correct.jsonl', 'tech_model.zip', 'inforet7.complete.ipynb']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nFuU1ffZjFpe","executionInfo":{"status":"ok","timestamp":1615217865701,"user_tz":-60,"elapsed":2391,"user":{"displayName":"MD Salem Messoud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBYoVQGAImGxJfhgMMK1daBThGNpiaASkBXr4EYpE=s64","userId":"15534871031717137879"}},"outputId":"c79f5ec8-ed16-4a29-dbf3-41cc2c0a5b04"},"source":["!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1tyeax9QzSl37GuDXaAZjhIq_PSwQViVD' -O 'manyterms.lower.txt'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-03-08 15:37:43--  https://docs.google.com/uc?export=download&id=1tyeax9QzSl37GuDXaAZjhIq_PSwQViVD\n","Resolving docs.google.com (docs.google.com)... 172.217.203.138, 172.217.203.113, 172.217.203.100, ...\n","Connecting to docs.google.com (docs.google.com)|172.217.203.138|:443... connected.\n","HTTP request sent, awaiting response... 302 Moved Temporarily\n","Location: https://doc-0s-84-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/qp09o9ba6bk9fu5mjo8ucb329qahncqb/1615217850000/16751912121320332077/*/1tyeax9QzSl37GuDXaAZjhIq_PSwQViVD?e=download [following]\n","Warning: wildcards not supported in HTTP.\n","--2021-03-08 15:37:45--  https://doc-0s-84-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/qp09o9ba6bk9fu5mjo8ucb329qahncqb/1615217850000/16751912121320332077/*/1tyeax9QzSl37GuDXaAZjhIq_PSwQViVD?e=download\n","Resolving doc-0s-84-docs.googleusercontent.com (doc-0s-84-docs.googleusercontent.com)... 172.217.203.132, 2607:f8b0:400c:c07::84\n","Connecting to doc-0s-84-docs.googleusercontent.com (doc-0s-84-docs.googleusercontent.com)|172.217.203.132|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/plain]\n","Saving to: ‘manyterms.lower.txt’\n","\n","manyterms.lower.txt     [ <=>                ]  14.87M  85.0MB/s    in 0.2s    \n","\n","2021-03-08 15:37:45 (85.0 MB/s) - ‘manyterms.lower.txt’ saved [15596213]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BKopGe2XjWF9"},"source":["#https://drive.google.com/file/d/1tyeax9QzSl37GuDXaAZjhIq_PSwQViVD/view?usp=sharing"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2LxfPvUrW2dJ","executionInfo":{"status":"ok","timestamp":1615217869109,"user_tz":-60,"elapsed":3073,"user":{"displayName":"MD Salem Messoud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBYoVQGAImGxJfhgMMK1daBThGNpiaASkBXr4EYpE=s64","userId":"15534871031717137879"}},"outputId":"e99b772e-9b57-4307-bd99-b86f9d177d76"},"source":["!wget https://gerdes.fr/saclay/informationRetrieval/prodigy/prodigy-1.10.6-cp36.cp37.cp38-cp36m.cp37m.cp38-linux_x86_64.whl"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-03-08 15:37:46--  https://gerdes.fr/saclay/informationRetrieval/prodigy/prodigy-1.10.6-cp36.cp37.cp38-cp36m.cp37m.cp38-linux_x86_64.whl\n","Resolving gerdes.fr (gerdes.fr)... 198.245.51.157\n","Connecting to gerdes.fr (gerdes.fr)|198.245.51.157|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 15294884 (15M)\n","Saving to: ‘prodigy-1.10.6-cp36.cp37.cp38-cp36m.cp37m.cp38-linux_x86_64.whl’\n","\n","prodigy-1.10.6-cp36 100%[===================>]  14.59M  10.3MB/s    in 1.4s    \n","\n","2021-03-08 15:37:48 (10.3 MB/s) - ‘prodigy-1.10.6-cp36.cp37.cp38-cp36m.cp37m.cp38-linux_x86_64.whl’ saved [15294884/15294884]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ek-_m5jXUvl","executionInfo":{"status":"ok","timestamp":1615217899667,"user_tz":-60,"elapsed":29511,"user":{"displayName":"MD Salem Messoud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBYoVQGAImGxJfhgMMK1daBThGNpiaASkBXr4EYpE=s64","userId":"15534871031717137879"}},"outputId":"57207ced-5421-4608-e712-dadc664ab67f"},"source":["!pip install ./prodigy*.whl"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Processing ./prodigy-1.10.6-cp36.cp37.cp38-cp36m.cp37m.cp38-linux_x86_64.whl\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.2 in /usr/local/lib/python3.7/dist-packages (from prodigy==1.10.6) (0.8.2)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.8 in /usr/local/lib/python3.7/dist-packages (from prodigy==1.10.6) (1.0.0)\n","Collecting spacy<2.4.0,>=2.3.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/89/1539c4024c339650c222b0b2ca2b3e3f13523b7a02671f8001b7b1cee6f2/spacy-2.3.5-cp37-cp37m-manylinux2014_x86_64.whl (10.4MB)\n","\u001b[K     |████████████████████████████████| 10.4MB 5.7MB/s \n","\u001b[?25hRequirement already satisfied: cachetools>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from prodigy==1.10.6) (4.2.1)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from prodigy==1.10.6) (1.1.3)\n","Collecting starlette<=0.12.9,>=0.12.9\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/95/2220fe5bf287e693a6430d8ee36c681b0157035b7249ec08f8fb36319d16/starlette-0.12.9.tar.gz (46kB)\n","\u001b[K     |████████████████████████████████| 51kB 5.3MB/s \n","\u001b[?25hRequirement already satisfied: toolz<=1.0.0,>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from prodigy==1.10.6) (0.11.1)\n","Collecting aiofiles\n","  Downloading https://files.pythonhosted.org/packages/ba/21/df5eae4b6db2eb00d58428dd7f793ecbf99942fcafcea141cbf108fa72f4/aiofiles-0.6.0-py3-none-any.whl\n","Collecting uvicorn<0.12.0,>=0.11.6\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/9a/5f619c02f36e751071c2b7eaa37a7c4b767feb41e4c2de48e8fbe4e7b451/uvicorn-0.11.8-py3-none-any.whl (43kB)\n","\u001b[K     |████████████████████████████████| 51kB 6.3MB/s \n","\u001b[?25hRequirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.7/dist-packages (from prodigy==1.10.6) (1.0.5)\n","Collecting pyjwt<2.0.0,>=1.6.1\n","  Downloading https://files.pythonhosted.org/packages/87/8b/6a9f14b5f781697e51259d81657e6048fd31a113229cf346880bb7545565/PyJWT-1.7.1-py2.py3-none-any.whl\n","Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from prodigy==1.10.6) (2.23.0)\n","Collecting pydantic<2.0.0,>=1.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/a3/0ffdb6c63f45f10d19b8e8b32670b22ed089cafb29732f6bf8ce518821fb/pydantic-1.8.1-cp37-cp37m-manylinux2014_x86_64.whl (10.1MB)\n","\u001b[K     |████████████████████████████████| 10.1MB 30.5MB/s \n","\u001b[?25hCollecting fastapi<0.45.0,>=0.44.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/0d/4b6150256b086ea643079ddfad81e9999dde910236b60ea464d9bde10a44/fastapi-0.44.1-py3-none-any.whl (42kB)\n","\u001b[K     |████████████████████████████████| 51kB 5.7MB/s \n","\u001b[?25hCollecting peewee<4.0.0,>=3.12.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0d/36/48178d91633de5d0e28f8bfcb9693426f5a415911e8421133263be857c2a/peewee-3.14.2.tar.gz (866kB)\n","\u001b[K     |████████████████████████████████| 870kB 42.4MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.8->prodigy==1.10.6) (3.7.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->prodigy==1.10.6) (1.19.5)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->prodigy==1.10.6) (3.0.5)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->prodigy==1.10.6) (4.41.1)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->prodigy==1.10.6) (2.0.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->prodigy==1.10.6) (54.0.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->prodigy==1.10.6) (1.0.5)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->prodigy==1.10.6) (0.4.1)\n","Collecting thinc<7.5.0,>=7.4.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/92/71ab278f865f7565c37ed6917d0f23342e4f9a0633013113bd435cf0a691/thinc-7.4.5-cp37-cp37m-manylinux2014_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 35.6MB/s \n","\u001b[?25hCollecting httptools==0.1.*; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"PyPy\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/52/295101ea5a60f9bee805a3ca422863600ba5cac4e2778ac7bd56efab1231/httptools-0.1.1-cp37-cp37m-manylinux1_x86_64.whl (217kB)\n","\u001b[K     |████████████████████████████████| 225kB 40.2MB/s \n","\u001b[?25hCollecting websockets==8.*\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/0b/3ebc752392a368af14dd24ee041683416ac6d2463eead94b311b11e41c82/websockets-8.1-cp37-cp37m-manylinux2010_x86_64.whl (79kB)\n","\u001b[K     |████████████████████████████████| 81kB 7.4MB/s \n","\u001b[?25hRequirement already satisfied: click==7.* in /usr/local/lib/python3.7/dist-packages (from uvicorn<0.12.0,>=0.11.6->prodigy==1.10.6) (7.1.2)\n","Collecting h11<0.10,>=0.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/fd/3dad730b0f95e78aeeb742f96fa7bbecbdd56a58e405d3da440d5bfb90c6/h11-0.9.0-py2.py3-none-any.whl (53kB)\n","\u001b[K     |████████████████████████████████| 61kB 6.4MB/s \n","\u001b[?25hCollecting uvloop>=0.14.0; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"PyPy\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/05/805df4850d9659efd69d00076269ae6adcb0e151d1922cff822ead2c432a/uvloop-0.15.2-cp37-cp37m-manylinux2010_x86_64.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 42.9MB/s \n","\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->prodigy==1.10.6) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->prodigy==1.10.6) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->prodigy==1.10.6) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->prodigy==1.10.6) (2.10)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydantic<2.0.0,>=1.0.0->prodigy==1.10.6) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.8->prodigy==1.10.6) (3.4.0)\n","Building wheels for collected packages: starlette, peewee\n","  Building wheel for starlette (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for starlette: filename=starlette-0.12.9-cp37-none-any.whl size=57244 sha256=8d6e7c55bf6d246ff562e61982f231aa0dc48411b33713859e86543873a3c990\n","  Stored in directory: /root/.cache/pip/wheels/1c/51/5b/3828d52e185cafad941c4291b6f70894d0794be28c70addae5\n","  Building wheel for peewee (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for peewee: filename=peewee-3.14.2-cp37-cp37m-linux_x86_64.whl size=592935 sha256=cdb43618064c7fb33a362606a0aeec59e7f9ab09949615df8512ea82f266ab25\n","  Stored in directory: /root/.cache/pip/wheels/36/b6/72/3a247541e0a53512dcecdf98f31b95b47a2944c6d9b78a3755\n","Successfully built starlette peewee\n","Installing collected packages: thinc, spacy, starlette, aiofiles, httptools, websockets, h11, uvloop, uvicorn, pyjwt, pydantic, fastapi, peewee, prodigy\n","  Found existing installation: thinc 7.4.0\n","    Uninstalling thinc-7.4.0:\n","      Successfully uninstalled thinc-7.4.0\n","  Found existing installation: spacy 2.2.4\n","    Uninstalling spacy-2.2.4:\n","      Successfully uninstalled spacy-2.2.4\n","Successfully installed aiofiles-0.6.0 fastapi-0.44.1 h11-0.9.0 httptools-0.1.1 peewee-3.14.2 prodigy-1.10.6 pydantic-1.8.1 pyjwt-1.7.1 spacy-2.3.5 starlette-0.12.9 thinc-7.4.5 uvicorn-0.11.8 uvloop-0.15.2 websockets-8.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iP-kErm72umn","executionInfo":{"status":"ok","timestamp":1615217905259,"user_tz":-60,"elapsed":30236,"user":{"displayName":"MD Salem Messoud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBYoVQGAImGxJfhgMMK1daBThGNpiaASkBXr4EYpE=s64","userId":"15534871031717137879"}},"outputId":"e910d934-cd26-4c05-bfb3-14b647bf7707"},"source":["# Fix version incompatibility\n","# https://support.prodi.gy/t/importerror-cannot-import-name-schema-from-pydantic/3948\n","!pip install pydantic==1.7"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting pydantic==1.7\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/c9/e0ee87e424aa9abdb827eede6bfcc5449a3918a6f2dd53f441ef5768e807/pydantic-1.7-cp37-cp37m-manylinux2014_x86_64.whl (9.1MB)\n","\u001b[K     |████████████████████████████████| 9.1MB 5.2MB/s \n","\u001b[?25hInstalling collected packages: pydantic\n","  Found existing installation: pydantic 1.8.1\n","    Uninstalling pydantic-1.8.1:\n","      Successfully uninstalled pydantic-1.8.1\n","Successfully installed pydantic-1.7\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hbe4FcTEW3eS","executionInfo":{"status":"ok","timestamp":1615217908141,"user_tz":-60,"elapsed":28499,"user":{"displayName":"MD Salem Messoud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBYoVQGAImGxJfhgMMK1daBThGNpiaASkBXr4EYpE=s64","userId":"15534871031717137879"}},"outputId":"cbd9d6f7-d6ec-4650-85eb-15c9ec0fe149"},"source":["!wget https://gerdes.fr/saclay/informationRetrieval/patentFiles/G06T.txt.gz"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-03-08 15:38:24--  https://gerdes.fr/saclay/informationRetrieval/patentFiles/G06T.txt.gz\n","Resolving gerdes.fr (gerdes.fr)... 198.245.51.157\n","Connecting to gerdes.fr (gerdes.fr)|198.245.51.157|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 27504031 (26M) [application/x-gzip]\n","Saving to: ‘G06T.txt.gz’\n","\n","G06T.txt.gz         100%[===================>]  26.23M  10.7MB/s    in 2.4s    \n","\n","2021-03-08 15:38:27 (10.7 MB/s) - ‘G06T.txt.gz’ saved [27504031/27504031]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bZrw_IU2XVOj","executionInfo":{"status":"ok","timestamp":1615218097445,"user_tz":-60,"elapsed":211042,"user":{"displayName":"MD Salem Messoud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBYoVQGAImGxJfhgMMK1daBThGNpiaASkBXr4EYpE=s64","userId":"15534871031717137879"}},"outputId":"9eb8ab52-326d-4842-ce65-14383f0f2955"},"source":["! python -m pip install sense2vec==1.0.3\n","# ! python -m pip install sense2vec\n","\n","! python -m pip install pandas nltk spacy srsly\n","! python -m spacy download en_core_web_sm\n","! python -m spacy download en_vectors_web_lg"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting sense2vec==1.0.3\n","  Downloading https://files.pythonhosted.org/packages/5e/37/9c6f544ad0e2e273e4ca01b710361998c4a350773d46a91915124eda5686/sense2vec-1.0.3-py2.py3-none-any.whl\n","Requirement already satisfied: catalogue>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from sense2vec==1.0.3) (1.0.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from sense2vec==1.0.3) (0.8.2)\n","Requirement already satisfied: spacy<3.0.0,>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from sense2vec==1.0.3) (2.3.5)\n","Requirement already satisfied: srsly>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from sense2vec==1.0.3) (1.0.5)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from sense2vec==1.0.3) (3.7.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from sense2vec==1.0.3) (1.19.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.3->sense2vec==1.0.3) (2.23.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.3->sense2vec==1.0.3) (2.0.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.3->sense2vec==1.0.3) (54.0.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.3->sense2vec==1.0.3) (1.0.5)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.3->sense2vec==1.0.3) (3.0.5)\n","Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.3->sense2vec==1.0.3) (7.4.5)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.3->sense2vec==1.0.3) (1.1.3)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.3->sense2vec==1.0.3) (0.4.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.2.3->sense2vec==1.0.3) (4.41.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->sense2vec==1.0.3) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->sense2vec==1.0.3) (3.4.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.2.3->sense2vec==1.0.3) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.2.3->sense2vec==1.0.3) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.2.3->sense2vec==1.0.3) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.2.3->sense2vec==1.0.3) (3.0.4)\n","Installing collected packages: sense2vec\n","Successfully installed sense2vec-1.0.3\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.3.5)\n","Requirement already satisfied: srsly in /usr/local/lib/python3.7/dist-packages (1.0.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n","Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n","Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.5)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (54.0.0)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (3.7.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n","Collecting en_core_web_sm==2.3.1\n","\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0MB)\n","\u001b[K     |████████████████████████████████| 12.1MB 5.1MB/s \n","\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.3.1) (2.3.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.2)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.5)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.41.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.23.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (54.0.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.5)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n","Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.5)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.7.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.12.5)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.7.4.3)\n","Building wheels for collected packages: en-core-web-sm\n","  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-cp37-none-any.whl size=12047109 sha256=46b1f32eae2a791110d0f96124e33c9f19ee5b69aeec950b8be11161a9ac127d\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-v40qpr0b/wheels/2b/3f/41/f0b92863355c3ba34bb32b37d8a0c662959da0058202094f46\n","Successfully built en-core-web-sm\n","Installing collected packages: en-core-web-sm\n","  Found existing installation: en-core-web-sm 2.2.5\n","    Uninstalling en-core-web-sm-2.2.5:\n","      Successfully uninstalled en-core-web-sm-2.2.5\n","Successfully installed en-core-web-sm-2.3.1\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_sm')\n","Collecting en_vectors_web_lg==2.3.0\n","\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_vectors_web_lg-2.3.0/en_vectors_web_lg-2.3.0.tar.gz (634.0MB)\n","\u001b[K     |████████████████████████████████| 634.0MB 1.1MB/s \n","\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from en_vectors_web_lg==2.3.0) (2.3.5)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_vectors_web_lg==2.3.0) (2.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_vectors_web_lg==2.3.0) (1.0.5)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_vectors_web_lg==2.3.0) (0.4.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_vectors_web_lg==2.3.0) (2.23.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_vectors_web_lg==2.3.0) (4.41.1)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_vectors_web_lg==2.3.0) (3.0.5)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_vectors_web_lg==2.3.0) (1.1.3)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_vectors_web_lg==2.3.0) (1.0.5)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_vectors_web_lg==2.3.0) (0.8.2)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_vectors_web_lg==2.3.0) (1.19.5)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_vectors_web_lg==2.3.0) (1.0.0)\n","Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_vectors_web_lg==2.3.0) (7.4.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<2.4.0,>=2.3.0->en_vectors_web_lg==2.3.0) (54.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_vectors_web_lg==2.3.0) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_vectors_web_lg==2.3.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_vectors_web_lg==2.3.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_vectors_web_lg==2.3.0) (3.0.4)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_vectors_web_lg==2.3.0) (3.7.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_vectors_web_lg==2.3.0) (3.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_vectors_web_lg==2.3.0) (3.7.4.3)\n","Building wheels for collected packages: en-vectors-web-lg\n","  Building wheel for en-vectors-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for en-vectors-web-lg: filename=en_vectors_web_lg-2.3.0-cp37-none-any.whl size=633155517 sha256=cd7fcccb925a9acd1e3bfaf19fe154babff57a1552fcf1e84edbdd32539fb219\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-slsbvxk1/wheels/48/27/d6/3a817056204e119a551c2a707624de8fbe1f7f6b8fe0c21991\n","Successfully built en-vectors-web-lg\n","Installing collected packages: en-vectors-web-lg\n","Successfully installed en-vectors-web-lg-2.3.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_vectors_web_lg')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"acoY-V7HXtVd","executionInfo":{"status":"ok","timestamp":1615218116319,"user_tz":-60,"elapsed":225388,"user":{"displayName":"MD Salem Messoud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBYoVQGAImGxJfhgMMK1daBThGNpiaASkBXr4EYpE=s64","userId":"15534871031717137879"}},"outputId":"2e9daeaf-1bc6-403c-a6a3-826258a15dbd"},"source":["!wget https://github.com/explosion/sense2vec/releases/download/v1.0.0/s2v_reddit_2015_md.tar.gz"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-03-08 15:41:37--  https://github.com/explosion/sense2vec/releases/download/v1.0.0/s2v_reddit_2015_md.tar.gz\n","Resolving github.com (github.com)... 140.82.113.3\n","Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://github-releases.githubusercontent.com/50261113/52126080-0993-11ea-8190-8f0e295df22a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210308%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210308T154137Z&X-Amz-Expires=300&X-Amz-Signature=e63b6aab0c3f45e5efc99806e3e7316b1034458e488563fd248bf4fbfd37cd04&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=50261113&response-content-disposition=attachment%3B%20filename%3Ds2v_reddit_2015_md.tar.gz&response-content-type=application%2Foctet-stream [following]\n","--2021-03-08 15:41:37--  https://github-releases.githubusercontent.com/50261113/52126080-0993-11ea-8190-8f0e295df22a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210308%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210308T154137Z&X-Amz-Expires=300&X-Amz-Signature=e63b6aab0c3f45e5efc99806e3e7316b1034458e488563fd248bf4fbfd37cd04&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=50261113&response-content-disposition=attachment%3B%20filename%3Ds2v_reddit_2015_md.tar.gz&response-content-type=application%2Foctet-stream\n","Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.108.154, 185.199.109.154, 185.199.110.154, ...\n","Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.108.154|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 600444501 (573M) [application/octet-stream]\n","Saving to: ‘s2v_reddit_2015_md.tar.gz’\n","\n","s2v_reddit_2015_md. 100%[===================>] 572.63M  36.2MB/s    in 18s     \n","\n","2021-03-08 15:41:56 (31.0 MB/s) - ‘s2v_reddit_2015_md.tar.gz’ saved [600444501/600444501]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JHKl6GKLdBsr"},"source":["!tar xf s2v_reddit_2015_md.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SF_WTXoVX1eQ","executionInfo":{"status":"ok","timestamp":1615218127648,"user_tz":-60,"elapsed":230671,"user":{"displayName":"MD Salem Messoud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBYoVQGAImGxJfhgMMK1daBThGNpiaASkBXr4EYpE=s64","userId":"15534871031717137879"}},"outputId":"54af8a85-95b8-480a-9ad9-f3468a91533f"},"source":["!wget https://github.com/explosion/projects/releases/download/tok2vec/tok2vec_cd8_model289.bin"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-03-08 15:42:05--  https://github.com/explosion/projects/releases/download/tok2vec/tok2vec_cd8_model289.bin\n","Resolving github.com (github.com)... 140.82.114.3\n","Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://github-releases.githubusercontent.com/223165649/c897b280-0c60-11ea-95bd-7656ac7c1c54?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210308%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210308T154205Z&X-Amz-Expires=300&X-Amz-Signature=21c4774ad9fb28bcd3fc15449147841d808bd9dd6245111e7bf10ded8363558c&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=223165649&response-content-disposition=attachment%3B%20filename%3Dtok2vec_cd8_model289.bin&response-content-type=application%2Foctet-stream [following]\n","--2021-03-08 15:42:05--  https://github-releases.githubusercontent.com/223165649/c897b280-0c60-11ea-95bd-7656ac7c1c54?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210308%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210308T154205Z&X-Amz-Expires=300&X-Amz-Signature=21c4774ad9fb28bcd3fc15449147841d808bd9dd6245111e7bf10ded8363558c&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=223165649&response-content-disposition=attachment%3B%20filename%3Dtok2vec_cd8_model289.bin&response-content-type=application%2Foctet-stream\n","Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.108.154, 185.199.111.154, 185.199.109.154, ...\n","Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.108.154|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 18683449 (18M) [application/octet-stream]\n","Saving to: ‘tok2vec_cd8_model289.bin’\n","\n","tok2vec_cd8_model28 100%[===================>]  17.82M  39.5MB/s    in 0.5s    \n","\n","2021-03-08 15:42:06 (39.5 MB/s) - ‘tok2vec_cd8_model289.bin’ saved [18683449/18683449]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9t8hNQtIv3ev"},"source":["# Find Wikipedia terms in patents"]},{"cell_type":"code","metadata":{"id":"HmzZxrJlkQS4"},"source":["import re, timeit, random\n","from collections import Counter\n","from nltk.tokenize import MWETokenizer\n","from nltk.util import Trie\n","\n","import pandas as pd\n","import gzip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1OCbWeFHX7YJ","executionInfo":{"status":"ok","timestamp":1615218179137,"user_tz":-60,"elapsed":1369,"user":{"displayName":"MD Salem Messoud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBYoVQGAImGxJfhgMMK1daBThGNpiaASkBXr4EYpE=s64","userId":"15534871031717137879"}},"outputId":"ebfeb073-839a-4145-e9c7-884821cf3a03"},"source":["# mwes = open(\"manyterms.lower.txt\").read().strip().split('\\n') # 743274 lines\n","mwes = [mw for mw in open(\"manyterms.lower.txt\").read().strip().split('\\n') if 'the ' not in mw and mw!='number of']\n","print(mwes[22222:22226])\n","len(mwes)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['advantages of bridge circuits', 'advantest corporation', 'adve sarita', 'advected contours']\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["717961"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"EvYhGxk8ZgAN"},"source":["\n","class FreqMWETokenizer(MWETokenizer):\n","    \"\"\"A tokenizer that processes tokenized text and merges multi-word expressions\n","    into single tokens.\n","    \"\"\"\n","\n","    def __init__(self, mwes=None, separator=\"_\"):\n","        super().__init__(mwes, separator)\n","\n","    def freqs(self, text):\n","        \"\"\"\n","        :param text: A list containing tokenized text\n","        :type text: list(str)\n","        :return: A frequency dictionary with multi-words merged together as keys\n","        :rtype: dict\n","        :Example:\n","        >>> tokenizer = FreqMWETokenizer([ mw.split() for mw in ['multilayer ceramic', 'multilayer ceramic capacitor', 'ceramic capacitor']], separator=' ')\n","        >>> tokenizer.freqs(\"Gimme that multilayer ceramic capacitor please!\".split())\n","        {'multilayer ceramic': 1, 'multilayer ceramic capacitor': 1, 'ceramic capacitor': 1}\n","        \"\"\"\n","        i = 0\n","        n = len(text)\n","        result = Counter()\n","\n","        while i < n:\n","            if text[i] in self._mwes:\n","                # possible MWE match\n","                j = i\n","                trie = self._mwes\n","                while j < n and text[j] in trie:\n","                    if Trie.LEAF in trie:\n","                        # success!\n","                        mw = self._separator.join(text[i:j])\n","                        result[mw]=result.get(mw,0)+1\n","                    trie = trie[text[j]] # diving one step deeper into the trie\n","                    j = j + 1\n","                else: # executed if while did not break\n","                    if Trie.LEAF in trie:\n","                        # success!\n","                        mw = self._separator.join(text[i:j])\n","                        result[mw]=result.get(mw,0)+1\n","                    i += 1\n","            else:\n","                i += 1\n","\n","        return result\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CHm-YfSZZiJ9","executionInfo":{"status":"ok","timestamp":1615218195073,"user_tz":-60,"elapsed":6240,"user":{"displayName":"MD Salem Messoud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBYoVQGAImGxJfhgMMK1daBThGNpiaASkBXr4EYpE=s64","userId":"15534871031717137879"}},"outputId":"69f9aed3-7ce4-4331-f307-393829578c9d"},"source":["splimwes = [ mw.split() for mw in mwes]\n","tokenizer = FreqMWETokenizer(splimwes, separator=' ')\n","tokenizer.freqs(\"Gimme that multilayer ceramic capacitor please!\".split())"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Counter({'ceramic capacitor': 1,\n","         'multilayer ceramic': 1,\n","         'multilayer ceramic capacitor': 1})"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":501},"id":"GylCHPxaZmY5","executionInfo":{"status":"ok","timestamp":1615218233232,"user_tz":-60,"elapsed":38147,"user":{"displayName":"MD Salem Messoud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBYoVQGAImGxJfhgMMK1daBThGNpiaASkBXr4EYpE=s64","userId":"15534871031717137879"}},"outputId":"f1f1619d-b8cc-437f-b245-5659dd9bdb1a"},"source":["\n","patent = '\\n'.join([str(li) for li in gzip.open('G06T.txt.gz', 'r') if li and li[:4]!='____'])\n","c,w = len(patent), len(patent.split())\n","print('sanity check:',c,'characters',w,'words',round(c/w,1),'chars per word')\n","print('looking for all potential multiwords in the patent text...')\n","start = timeit.default_timer()\n","counter = tokenizer.freqs(patent.split())\n","secs = timeit.default_timer()-start\n","print('it took', secs, 'seconds,', secs/len(mwes),'per term')\n","patentdf = pd.DataFrame.from_dict(counter, orient='index').sort_values(by = 0, ascending = False)\n","patentdf.index.name = 'term'\n","patentdf.columns = ['freq']\n","patentdf"],"execution_count":null,"outputs":[{"output_type":"stream","text":["sanity check: 148900118 characters 23767892 words 6.3 chars per word\n","looking for all potential multiwords in the patent text...\n","it took 30.36488696799995 seconds, 4.2293226189166196e-05 per term\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>freq</th>\n","    </tr>\n","    <tr>\n","      <th>term</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>image processing</th>\n","      <td>19006</td>\n","    </tr>\n","    <tr>\n","      <th>electronic device</th>\n","      <td>10371</td>\n","    </tr>\n","    <tr>\n","      <th>control unit</th>\n","      <td>9279</td>\n","    </tr>\n","    <tr>\n","      <th>information processing</th>\n","      <td>7511</td>\n","    </tr>\n","    <tr>\n","      <th>point cloud</th>\n","      <td>6692</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>membrane switch</th>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>electrical path</th>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>potential drop</th>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>press fit</th>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>computer languages</th>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>13220 rows × 1 columns</p>\n","</div>"],"text/plain":["                         freq\n","term                         \n","image processing        19006\n","electronic device       10371\n","control unit             9279\n","information processing   7511\n","point cloud              6692\n","...                       ...\n","membrane switch             1\n","electrical path             1\n","potential drop              1\n","press fit                   1\n","computer languages          1\n","\n","[13220 rows x 1 columns]"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sA03hVc3nDH6","executionInfo":{"status":"ok","timestamp":1615218240495,"user_tz":-60,"elapsed":923,"user":{"displayName":"MD Salem Messoud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBYoVQGAImGxJfhgMMK1daBThGNpiaASkBXr4EYpE=s64","userId":"15534871031717137879"}},"outputId":"ae64ec9a-d6ec-4550-a7e6-40d689c34566"},"source":["patentdf.iloc[:22].index.values"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['image processing', 'electronic device', 'control unit',\n","       'information processing', 'point cloud', 'display device',\n","       'block diagram', 'user interface', 'image sensor', 'light source',\n","       'coordinate system', 'computing device', 'augmented reality',\n","       'neural network', 'computer program', 'field of view',\n","       'medical image', 'storage medium', 'region of interest',\n","       'data processing', 'computer readable', 'virtual reality'],\n","      dtype=object)"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"y0MVMoAYcngd"},"source":["patentdf.to_csv('g06t_df.tsv',sep='\\t')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gv1GQb7aczC0"},"source":["# patentdf=pd.read_csv('g06f_df.tsv',sep='\\t',index_col='term')\n","# patentdf"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NW7VI2KKw-ts"},"source":["## Annotation scope\n","We will focus on terms specific to section G06T: processing, generation, animation of image and video \\\n","We will exclude generic terms such as software, computer program, algorithm, etc."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i_Gu1d-tc4Hk","executionInfo":{"status":"ok","timestamp":1615218261032,"user_tz":-60,"elapsed":2805,"user":{"displayName":"MD Salem Messoud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBYoVQGAImGxJfhgMMK1daBThGNpiaASkBXr4EYpE=s64","userId":"15534871031717137879"}},"outputId":"cd665499-cd8b-4897-8e9b-b00a949e082d"},"source":["# import timeit\n","from spacy import displacy\n","from spacy.matcher import PhraseMatcher\n","from tqdm import tqdm\n","from sense2vec import Sense2Vec, Sense2VecComponent\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n","  \"\"\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"n0oBdm9KVl7X"},"source":["# s2v = Sense2Vec().from_disk(\"s2v_old\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Xi2ypLckWcu"},"source":["# text = 'animation|NOUN'\n","# print(s2v.get_freq(text))\n","\n","# vector = doc._.s2v_vec\n","# print(vector.shape)\n","\n","# most_similar = doc._.s2v_most_similar(22)\n","# print(s2v.most_similar(\"image generation|NOUN\", n=10))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_GE_YhU8gxwF","executionInfo":{"status":"ok","timestamp":1615218282396,"user_tz":-60,"elapsed":14008,"user":{"displayName":"MD Salem Messoud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBYoVQGAImGxJfhgMMK1daBThGNpiaASkBXr4EYpE=s64","userId":"15534871031717137879"}},"outputId":"c5b0a3f9-dd44-48b3-c9c9-523435157d0d"},"source":["# restart runtime\n","import spacy\n","\n","nlp = spacy.load(\"en_core_web_sm\")\n","# s2v = nlp.add_pipe(\"sense2vec\")\n","# s2v.from_disk(\"s2v_old\")\n","s2v = Sense2VecComponent(nlp.vocab).from_disk(\"s2v_old\")\n","nlp.add_pipe(s2v)\n","\n","doc = nlp(\"computer graphics\")\n","for token in doc: print(token)\n","freq = doc[0:]._.s2v_freq\n","vector = doc[0:]._.s2v_vec\n","print(freq, vector.shape)\n","\n","most_similar = doc[0:]._.s2v_most_similar(11)\n","print(most_similar)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["computer\n","graphics\n","592 (128,)\n","[(('3D animation', 'NOUN'), 0.8302), (('computer animation', 'NOUN'), 0.8238), (('computer vision', 'NOUN'), 0.8097), (('digital design', 'NOUN'), 0.804), (('physics engines', 'NOUN'), 0.7966), (('3D modeling', 'NOUN'), 0.796), (('3D modelling', 'NOUN'), 0.791), (('modeling', 'NOUN'), 0.786), (('programming', 'NOUN'), 0.7807), (('3D graphics', 'NOUN'), 0.7798), (('3d modeling', 'NOUN'), 0.7795)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g8TgAveenHOW","executionInfo":{"status":"ok","timestamp":1615218299584,"user_tz":-60,"elapsed":2152,"user":{"displayName":"MD Salem Messoud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBYoVQGAImGxJfhgMMK1daBThGNpiaASkBXr4EYpE=s64","userId":"15534871031717137879"}},"outputId":"83bd93d0-45cd-40bd-82f2-d7ec64f53cff"},"source":["def extract_term(match):\n","  word = match[0].replace('_', ' ')\n","  return word[:word.find('|')]\n","  \n","# most_similar2 = [word[0] for word in s2v.s2v.most_similar('image_processing|NOUN', n=2222)]\n","most_similar2 = list(map(extract_term, s2v.s2v.most_similar('image_processing|NOUN', n=2222)))\n","print(most_similar2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['software side', 'motion tracking', 'signal processing', 'post-processing', 'image processor', 'face detection', 'video processing', 'post processing', 'camera hardware', 'eye tracking', 'video codecs', 'computer vision', 'rendering', 'display technology', 'speech recognition', 'better software', 'audio processing', 'compositing', 'RAW images', 'sound processing', 'object recognition', 'software', 'screen technology', 'software solution', 'compositing', 'hardware design', 'photo editing', 'processing power', 'gesture recognition', 'Lightroom', 'graphics pipeline', 'RAW files', 'CMOS sensors', 'video compression', 'sensor data', 'raytracing', 'RAW data', 'RAW support', 'sensor fusion', 'camera software', 'hardware level', 'edge detection', 'HDR', 'software features', 'image manipulation', 'RAW image', 'VR applications', 'high resolution', 'FPGAs', 'hardware', '4K video', 'hardware side', 'OIS', 'decoding', 'Photoshop', 'camera sensor', 'color calibration', 'computer graphics', 'video playback', 'advanced features', 'very high resolution', 'image recognition', 'video capabilities', 'OpenCV', 'image quality', 'GPU acceleration', 'software level', 'rasterization', '3D stuff', 'color accuracy', 'FPGA', 'game engines', 'software interface', 'image analysis', 'color gamut', '3D rendering', 'user interface', 'more processing power', 'user interfaces', 'RAW photos', 'more megapixels', '3D video', 'workflow', 'higher resolution screen', 'hardware support', 'AutoCAD', 'coprocessor', 'based devices', 'most cameras', 'rendering pipeline', 'colour accuracy', 'positional tracking', 'machine vision', 'FPGA', 'less processing power', 'eye-tracking', 'deep learning', 'framebuffer', 'heavy processing', 'mobile devices', 'best software', 'Lightroom', 'external hardware', 'data processing', 'head-tracking', 'hardware architecture', 'hardware features', 'renderers', 'other camera apps', 'good software', 'extra resolution', 'phone hardware', '10-bit', 'autofocus', 'physics engines', 'current software', 'Adobe suite', 'modern software', 'software implementation', 'only software', 'digital processing', 'ProRes', 'optimization', 'low light performance', 'internal software', 'image editing', '3D graphics', 'CMOS sensor', 'C4D.', 'foveated rendering', 'higher resolution screens', 'Magic Lantern', 'HMD', 'Adobe Lightroom', 'OLED panels', 'optimizations', 'OIS', 'Sony cameras', 'low persistence', 'audio hardware', 'higher resolution', 'OLED', 'I/O', 'LCDs', 'low resolution', 'encoding/decoding', 'image stabilization', '3D design', 'circuit design', '3D modelling', 'stock camera app', 'larger sensor', 'Photoshop', 'camera sensors', 'hardware implementation', 'processing', 'hardware decoding', 'phone cameras', 'optical image stabilization', 'digital photography', 'deinterlacing', 'optical zoom', 'data compression', 'error correction', 'postprocessing', 'just software', 'manual controls', 'motion detection', 'lower resolution', 'modern websites', 'high pixel density', 'Lightroom', 'colour reproduction', 'image compression', 'global shutter', 'increased resolution', 'tiny sensor', 'pressure sensitivity', 'SolidWorks', 'color depth', 'low-light performance', 'modern cameras', 'Lightroom', 'much higher resolution', 'editing software', '3D software', 'computer software', 'custom software', 'more advanced features', 'ray tracing', 'sensor size', 'color space', 'design software', 'Google camera app', '20MP', 'hardware/software', 'HDR mode', 'color reproduction', 'modeling', 'bigger sensor', 'software support', 'batch processing', 'encoding', 'graphics processing', 'Photoshop', 'raytracer', 'sensor technology', 'display tech', 'Auto mode', 'voice recognition', 'AutoCAD', 'parallel computing', 'stereoscopy', 'color spaces', 'larger pixels', 'moving subjects', 'postprocessing', 'actual hardware', 'photogrammetry', '3D modeling', '3D data', 'sRGB', 'OLED screens', 'color data', 'EVF', 'LCD screens', 'animation software', 'V-Ray', 'audio latency', 'motion graphics', 'renderer', 'rendering process', 'small sensor', 'newer cameras', 'professional software', 'VR apps', 'better algorithms', 'OS design', 'right software', 'camera2 api', 'SketchUp', 'graphics hardware', 'functionality', 'processor power', 'software/hardware', 'OLEDs', 'mobile hardware', 'graphical design', 'larger sensors', 'workflow', 'C4D', 'analog circuits', 'manual settings', 'software options', 'video software', 'auto-focus', 'digital signal processing', 'gesture control', 'color science', 'amazing software', 'camera technology', 'rolling shutter', 'digital cameras', 'other software', 'iPhone camera', 'better color accuracy', 'Magic Lantern', 'specific software', 'video mode', 'interpolation', 'HDR', 'basic functionality', 'digital work', 'better image quality', 'megapixels', 'oversampling', 'new APIs', 'UI design', 'histograms', 'normal camera', 'live view', 'ARM architecture', 'Gear VR', 'embedded systems', 'After Effects', 'codec', '4k video', 'chip design', 'hardware solution', 'imaging', 'real devices', 'vector graphics', '3D scanning', 'hardware feature', '3D images', 'camera module', 'processing software', 'low light', 'SDKs', 'raw images', 'low light shots', 'LCD screen', 'high resolution displays', 'rendering engine', 'DSLRs', 'motion interpolation', 'depth map', 'video encoding', 'digital zoom', 'desktop monitors', 'technical details', 'specialized software', 'powerful computer', 'editing programs', 'noise reduction', 'lightroom', 'optical system', 'global illumination', 'low latency', 'extra pixels', 'retina displays', 'better resolution', 'work flow', 'color information', '3D audio', 'texture mapping', 'software part', 'JPEGs', 'Photoshop', 'Sony sensors', 'large resolution', 'video work', 'retina screens', 'depth camera', 'other cameras', 'smartphone camera', 'CAD programs', 'RAW format', 'user experience', 'color profiles', 'Solidworks', 'physics simulations', 'decent software', 'computing', 'digital sensor', 'actual interface', 'SDK', 'GearVR', 'other applications', 'fine detail', 'higher dynamic range', 'native applications', 'multicore', 'hardware limitation', 'solidworks', 'camera experience', 'OpenGL', 'raytracing', 'video hardware', 'LCD panels', 'graphics engines', 'FCPX', 'OVF', '3d rendering', 'dual displays', 'Final Cut Pro', 'Photoshop', 'point clouds', 'digital systems', 'Blender', 'best image quality', 'OLED display', 'great software', 'real world applications', 'extra processing power', 'fewer pixels', 'renders', 'faster processing', 'Visio', 'certain applications', 'Cinema 4D', 'very low resolution', 'realtime', 'dynamic range', 'final output', '3d modeling', 'GH4', 'PhotoShop', 'similar software', 'new API', 'raw format', 'camera settings', 'color profile', 'VSCO', 'coding', 'rendering software', 'analog stuff', 'modelling software', 'embedded programming', 'audio playback', 'smaller sensor', 'native software', 'algorithms', 'general computing', 'dedicated hardware', 'raster graphics', 'deconvolution', 'head tracking', 'programing', 'touch input', 'video capture', 'render engine', 'different screen sizes', 'Autocad', 'prototyping', 'accurate colors', 'smart objects', 'larger displays', 'smaller sensors', 'new algorithms', 'screen tech', 'extra processing', 'low light situations', 'high ISOs', 'audio software', 'OIS.', 'Aperture', 'WebGL.', 'upscaling', 'GIS software', 'phone software', 'video editing software', 'RAW file', 'architectures', 'multimedia', 'camera manufacturers', 'specific hardware', 'Autodesk', 'h.264', 'performance hit', 'custom hardware', 'Retina displays', 'camera tech', 'hardware specs', 'shutter lag', 'Canon cameras', 'Revit', '3D', 'variable refresh rate', 'subpixel', 'hardware differences', 'graphics APIs', 'modern devices', 'Blackmagic', 'H.264', 'softwares', 'color grading', 'HEVC', 'smaller pixels', 'most cell phones', 'software suite', 'low light capabilities', 'pixel density', 'useful features', 'modern camera', 'AutoCAD', 'dithering', 'wireless technology', 'parallel processing', 'BMCC', 'rendered images', 'new software', 'render target', 'UI work', 'colour grading', 'ArcGIS', 'black levels', 'HoloLens', 'battery impact', 'IMU', 'picture quality', 'finer detail', 'CPU architecture', 'file formats', '3d stuff', 'AF system', 'face recognition', 'DK2', 'nice screen', 'interface', 'computing power', 'final render', 'multitouch', 'debugging', 'stereoscopic', 'HMD', 'raster images', 'hardware acceleration', 'image data', 'Premiere', 'screen sizes', 'front facing camera', 'actual device', 'creative software', '3D modeling software', 'codecs', 'S3D', 'OLED', 'dedicated chip', 'colour depth', 'maximum resolution', 'most Android devices', 'Final Cut Pro X', 'auto focus', 'audio production', 'current hardware', 'open standard', 'embedded software', 'parallax', 'color correction', 'video codec', 'OpenCV', 'barrel distortion', 'darktable', 'killer feature', 'camera tracking', 'opencv', 'Wacom', 'GearVR', 'consumer devices', 'nifty features', 'SoCs', 'EVF', 'ISO settings', 'VR', 'h264', 'workflows', 'UX', 'HoloLens', 'Adobe products', 'fancy features', 'dedicated software', 'GUI stuff', 'high resolution display', 'image sensor', 'tracking camera', 'human vision', 'Adobe Photoshop', 'current workflow', 'memory management', 'sensor', 'viewfinder', 'GUI', 'algorithms', '3D work', 'C4D', 'input device', 'computer processing', 'battery efficiency', 'DSPs', 'windowing', 'optical viewfinder', 'Premiere Pro', 'great camera', 'L Camera', 'neural nets', 'miniaturization', 'same software', 'large screen', ' Photoshop', 'native support', 'pixel shader', 'other features', 'VR', 'stock camera', 'FCP', 'final image', 'native solution', 'FCPX', '20mp', 'iOS simulator', 'many applications', 'rendering system', 'low light photos', 'Gear VR.', 'real world use', 'Autocad', 'hardware work', 'much lower resolution', 'usability', '1080p video', 'Adobe Lightroom', 'Adobe Premiere Pro', 'modern GPUs', 'display lag', 'machine learning', 'embedded devices', 'display controller', 'HMD.', 'Cycles', 'HMDs', 'hardware interface', 'post-process', 'touch interface', 'lens blur', 'OpenCL.', 'processor speed', '3D support', 'large sensor', 'file management', 'most tablets', 'Phone cameras', 'Modern computers', 'input latency', 'enough resolution', '3D', 'AMOLED', 'optimized version', 'lossy compression', 'pyglet', 'compositor', 'design work', 'GPGPU', 'digital design', 'color management', 'audio work', 'smartphone cameras', 'rapid prototyping', 'google camera', 'UE4', 'camera apps', 'SolidWorks', 'H.265', 'mobile software', 'better low light performance', 'larger display', 'modelling', 'encoders', 'board design', 'high resolution screens', 'CSS animations', 'specific chip', 'motion blur', 'AutoCad', 'sharper images', 'microcontroller', 'programming', 'design programs', 'ray tracer', 'CAD work', 'phone display', 'interfacing', 'most software', 'h.265', 'display resolution', 'interfaces', '3D scene', 'cloud computing', 'GPU performance', 'HMIs', 'iPhone', 'important features', 'Premiere Pro', 'Arduino', 'based apps', 'hololens', 'video editing', 'modern monitors', 'high dpi', 'standard camera', 'optimisations', 'greater resolution', 'many devices', 'software bugs', 'CAD software', 'DPI scaling', 'computer side', 'graphic work', 'editing photos', 'mechanical shutter', 'modeling software', 'actual applications', 'Aperture', 'graphics program', 'real hardware', '3d graphics', 'additional hardware', 'noise performance', 'AVID', 'Bokeh', 'medical imaging', 'high ISO', 'GPU hardware', 'interpolate', 'large data sets', 'video decoding', 'touch screen devices', 'sufficient resolution', 'OLED displays', 'much higher resolutions', 'curved display', 'own software', 'special software', 'optimizing', 'hardware part', 'good camera', 'audio engine', 'Arduino', 'software/firmware', 'Surface', 'faster processor', 'useful functionality', 'IPS', 'more resolution', 'color filters', 'leap motion', 'GearVR.', 'rendering engines', 'Arduino', \"Blender's\", 'Cintiq', 'design tools', 'APIs', 'debugging tools', 'native apps', 'resulting image', 'calibrated monitor', 'newer devices', 'SDK', 'mapping', 'haptics', 'camera quality', 'modern smartphones', 'software work', 'HD resolution', 'DSLR', 'FPGA.', 'compositors', 'Krita', 'digital video', 'i/o', 'aided design', 'GPU rendering', 'modern computers', 'mobile processor', 'video chip', 'user interface design', 'Blender', 'touch interfaces', 'image capture', 'slow processor', 'HoloLens', 'low res', 'mobile screens', 'capacitive touchscreen', 'rendered image', 'desktop software', 'multitasking', '3d', 'accelerometer', 'production software', 'certain hardware', 'overall user experience', 'WebGL', 'much processing', '3D objects', 'user testing', 'scientific computing', 'different software', 'FCP X', 'large datasets', 'OpenGL', 'high end cameras', 'Sketchup', 'MATLAB', 'mouse input', 'liveview', 'additional processing', 'lightfields', 'Sony Vegas', 'screen recording', 'InDesign', '3d modelling', 'actual resolution', 'headtracking', 'multiple exposures', 'intensive applications', 'just the software', 'micro-controllers', 'pixel size', 'most devices', 'screen size', 'OIS.', 'lens distortion', 'Adobe After Effects', 'EVFs', 'new API.', 'iOS.', 'OpenGL.', 'other camera', 'manual focus', 'Photoshop work', 'body tracking', 'variable refresh rates', 'UI', 'high resolution screen', 'AdobeRGB', 'OpenGL', 'optimisation', 'GPU power', 'integrated solution', 'bitmaps', 'other phones', 'HRTF', 'extra hardware', 'pixel level', 'external sensors', 'PC software', 'Mental Ray', 'Most cameras', 'software limitation', 'feature sets', 'amazing camera', 'hardware', 'more pixels', 'panoramas', 'Solidworks', 'playback', 'FaceTrackNoIR', 'ARM processors', 'H.264', 'graphics engine', 'Vegas Pro', 'electronic viewfinder', 'different hardware', 'embedded system', 'phone camera', 'enough processing power', 'individual pixels', 'hardware limitations', 'FCP7', 'raw files', 'Post processing', 'desktop use', 'contrast ratio', 'good hardware', 'scaler', 'A7S', 'projectors', 'tracking software', 'h.264', 'distributed computing', 'LED flash', 'visual data', '3D animation', 'baked lighting', '2d game', 'technical specs', 'light meter', 'kernel driver', 'shutter speeds', 'Adobe', 'Google Camera', 'still images', 'Android devices', 'PhotoShop', 'photo work', 'feature set', 'preprocessing', 'features', 'OS level', 'higher ISOs', 'manual exposure', 'regular camera', 'holography', 'HDMI output', 'bad camera', 'more hardware', 'wireless communications', 'GPGPU', 'CRT displays', 'more powerful computer', 'chromatic aberration', 'graphics design', 'battery consumption', 'color images', 'font rendering', 'main camera', 'raw computing power', 'VR headsets', 'various devices', 'appropriate software', 'human visual system', 'screen quality', 'most apps', 'animation program', 'rasterized', 'computational work', 'interfaces', 'Camera2 API', 'low level programming', 'performance cost', 'other hardware', 'Cardboard', 'hardware requirements', 'specialized hardware', 'AA filter', 'technical documentation', 'device support', 'compression algorithms', 'LCD displays', 'H.264', 'analog signals', 'retina display', 'larger aperture', 'augmented reality', 'photoshop', 'microprocessor', 'more latency', 'hdr', 'other sensors', 'Revit', 'photoshop work', 'metering', 'square screen', 'raw image', 'accelerometer data', 'synchronization', 'BackyardEOS', 'regular cameras', 'Photoshop', 'software thing', 'display quality', 'magnetometer', 'HDMI passthrough', 'input devices', '3d objects', 'software tool', '3D programs', 'UX', 'latency', 'microprocessors', 'most phones', 'curved screen', '8MP', 'SAI', 'software', 'software rendering', 'system performance', '3d', 'resampling', 'AR headset', 'encoding process', 'Krita', 'less resolution', 'OSD', 'existing hardware', 'vray', 'data analysis', 'existing devices', 'photo editing software', 'antialiasing', 'faster processors', 'compute', 'CAD program', 'screen resolution', 'ArcGIS', 'pre-processing', 'use cases', 'lower end devices', 'technical capabilities', 'WebGL', 'like functionality', 'digital images', 'amazing screen', 'standard software', 'word processing', 'iOS devices', 'most smart phones', 'small screen', 'ARM chip', 'LLE', 'F.lux', 'modern phones', 'h264', 'most smartphones', 'larger screens', 'pen input', 'GIMP', 'Adobe software', 'OLED', 'custom features', 'end user experience', 'human-computer interaction', 'accelerometers', 'most applications', 'LCD display', 'user interaction', 'specific applications', 'configuring', 'less blur', 'cheaper cameras', 'small screens', 'OCR', 'playback', 'colorspace', 'visual processing', 'compression artifacts', 'CUDA.', 'data transmission', 'simple apps', 'video recording', 'three.js', 'antenna design', 'simple interface', 'older devices', 'composite', 'visual information', 'workflow', '3d software', 'audio support', 'Higher resolution', 'physical design', 'vector math', 'interface design', 'iMovie', 'multi-window', 'Manual mode', 'ProRes', 'Blackmagic', 'computer architecture', 'sensor performance', 'audio editing', 'downsampling', 'SketchUp', 'lens correction', 'video production', 'low-light', '20MP', 'RTOS', 'digital side', 'software design', 'screen resolutions', 'desktop applications', 'color adjustments', 'software renderer', 'external controller', 'older cameras', 'camera2', 'windowing', 'color display', 'small file size', 'same resolution', 'higher ISO', 'fcpx', 'most android devices', 'file compression', 'data storage', 'better cameras', 'Google Cardboard', 'decoding', 'web apps', 'internal design', 'digital camera', 'Gear VR.', 'full manual control', 'recording software', 'modern TVs', 'video signal', 'control systems', 'color screen', 'G-Sync', 'AutoCAD.', 'actual camera', 'driver level', 'UX', 'graphics programming', 'speed improvements', 'cross-platform support', 'UV mapping', 'JPGs', 'basic editing', 'megapixel count', 'camera system', 'older technology', 'other technologies', 'CAD/CAM', 'AMOLED screen', '3ds Max', 'highest resolution', 'rendering', 'render', 'super high resolution', 'SOC', 'lighting conditions', 'TIR', 'OS side', 'android devices', 'Adobe Photoshop', '30Hz', 'QML', 'required hardware', 'small apps', 'debuggers', 'embedded device', 'better hardware', 'audio/video', 'microcontrollers', 'Arduino', 'x86', 'LCD.', 'too much latency', 'other game engines', 'raw photos', 'SPSS', 'audio compression', 'Microsoft Research', 'IR blasters', 'parallelization', 'Premiere', 'video rendering', 'digital conversion', 'different resolutions', 'interaction design', 'compute', 'Illustrator', 'L camera', 'bokeh', 'OSD', '2D screen', 'PCI bus', 'natural language processing', 'mobile tech', 'smartphone apps', 'moiré', 'Photoshop', '3ds max', '3DS Max', 'hot pixels', 'gimmicky features', 'different technologies', 'Sketchup', 'processing speed', 'IR filter', 'JPEG', 'Unity Free', 'DSLR cameras', 'computation', 'interlacing', 'touchscreen', 'operating systems', '3d images', 'wireless connectivity', 'custom driver', 'facial recognition', 'more computing power', 'hardware thing', 'Upscaling', 'software tools', 'contrast ratios', '3D applications', 'video side', 'updated software', 'more memory', 'Digital cameras', 'how much processing power', 'computing', 'curved screens', 'Inkscape', 'computer hardware', 'UI stuff', '3d effect', 'processor architecture', 'technical side', 'kernel level', 'high DPI', 'serial communication', '240fps', 'multithreading', 'additional functionality', 'NLE', 'AMOLED displays', 'multiple cameras', 'IPS displays', 'good UI', 'lighting effects', 'engineering software', 'media players', 'VR.', 'other optimizations', 'lightfield', 'more sensors', 'CUDA', 'hand tracking', 'scanlines', 'modern hardware', 'compression algorithm', 'entire interface', 'basic features', 'physical controls', 'tablet app', 'Processing power', 'point cloud', 'higher PPI', '6D', 'FFTs', 'particular camera', 'BLE', 'DK2', 'word processors', 'better functionality', 'hardware power', 'JIT compiler', 'desktop apps', 'MSP430', 'alpha channels', 'Hololens', 'HD screen', 'vector images', 'anti-aliasing', 'curved displays', 'UE4', 'software programs', 'multiple resolutions', 'mental ray', 'z-buffer', 'ZBrush', 'spatial analysis', 'pure data', 'Magic Lantern', 'portable devices', 'x86', 'downsampling', 'interface', 'upscale', 'other functionality', 'based device', 'higher resolution displays', 'viewing angles', 'handheld devices', 'older hardware', 'camera speed', 'UIs', 'Adobe Premiere', 'extra latency', '4K displays', 'gpu power', 'physics simulation', '3D scenes', 'awesome camera', 'underlying hardware', 'driver station', 'hardware performance', 'upscaling', 'Android', 'Wacom tablets', 'external recorder', 'recording function', 'visual quality', 'real-world usage', 'Adobe stuff', 'media playback', 'modern smartphone', 'MATLAB', 'occlusion', 'colour space', 'encoder', 'most android phones', 'optimised', 'OpenCL', 'small devices', 'Leap Motion', 'iPhone cameras', 'game graphics', 'Illustrator', 'convolution', 'photo manipulation', 'color mode', 'fingerprint reader', 'audio/video', 'Leap Motion', 'hardware engineering', 'Cinema4D', 'phone functions', 'great hardware', 'video data', 'capture software', 'render times', '3D models', 'Solidworks', 'SDE.', 'industrial design', 'app design', 'software perspective', '3d animation', 'AR device', 'screen display', 'software stuff', 'Premier Pro', 'older camera', 'iPhone apps', 'frame interpolation', 'underlying code', 'special hardware', 'Windows desktop', 'Oculus SDK', 'GUIs', 'render time', 'real applications', 'dedicated camera', 'computer programs', 'CPU design', 'own processing', 'iOS', 'higher res screen', 'IR blaster', 'main processor', 'video conferencing', 'developer kit', 'graphical elements', 'optical tracking', 'UI elements', 'Most smartphones', 'touch events', 'computations', 'Crescent Bay', 'GIS', 'rasterizing', 'screen recorder', 'Lumia camera', 'vectorization', 'hardware &amp', 'android apps', 'Illustrator', 'terrible resolution', 'front camera', '3D capabilities', 'light sensor', 'office software', 'software stack', 'DirectX', 'seamless integration', 'Photoshop', 'commercial software', 'Gimp', 'moving images', 'phone screens', 'flat image', 'mobile stuff', 'good image quality', 'smaller screens', 'pentile', 'USB3', 'native app', 'camera app', 'HDR', '4K', 'basic apps', 'Game engines', 'back camera', 'manual lenses', 'Many devices', 'business applications', 'preprocessing', 'Open GL', 'just the hardware', 'big data', 'high ISO.', 'intensive tasks', 'certain software', 'Solidworks', 'capture devices', 'non-native resolution', 'computing time', 'digital electronics', 'defined radio', 'data connections', 'Responsive design', 'multi-tasking', 'CAD design', 'most game engines', 'proper software', 'VR', 'x86 processors', 'editing', 'Adobe Illustrator', 'Camera app', 'Video editing', 'Sketchup', 'display size', 'computational power', 'web browsing', 'baseband', 'Arduino', 'PC hardware', 'software program', 'real-world applications', 'proper hardware', 'many cameras', 'retina screen', 'Snapseed', 'still photography', 'aspect ratios', 'solid hardware', '3D camera', 'SDR', 'networking stuff', 'firmware', 'astrophotography', 'digital noise', 'feature parity', 'productivity software', 'color temperature', 'Manual Camera', 'pixel pitch', 'other tech', 'lightworks', 'OSD.', 'rasters', 'OpenGL ES', 'coding', 'Computer vision', 'optimized', 'night shots', 'AfterEffects', 'automatic settings', 'video stabilization', 'Avid', 'reprojection', 'LIDAR', 'image editing software', 'raw file', 'GLSL', 'application software', 'multitouch', 'CUDA', 'specific technology', 'less data', 'OSVR', 'IOS apps', 'HD audio', 'iOS', 'chromatic abberation', 'subpixels', 'newer features', 'Raspberry Pi', 'IP6', 'ARM', 'different sensors', 'web browsers', 'data side', 'proper exposure', 'Android apps', 'Gimp', 'pixellation', 'apps', 'video streaming', 'much processing power', 'drawing program', 'data manipulation', 'prototyping', 'autocad', 'android side', 'multi monitor setups', 'Gear VR', 'scientific applications', 'Inventor', 'larger screen', 'form-factor', 'Camera', 'refresh rates', 'software experience', 'great user experience', 'available RAM', 'photo retouching', 'wireless networking', 'HoloLens', 'multiplexing', 'Nikon cameras', 'Krita', 'G-sync', 'display device', 'internal settings', 'Quicktime', 'capture cards', 'actual software', 'functionalities', 'static images', 'benchmarking', 'pixel data', 'use case', 'CAM software', 'camera shake', 'ARM CPU', 'higher frame rate', 'S-Pen', '3d effects', 'camera performance', 'Revit', '2D images', 'voice control', 'adobe programs', 'openGL', 'control theory', 'wireframes', 'software feature', 'Google camera', 'better dynamic range', 'warp stabilizer', 'auto layout', 'more functionality', 'real-time', 'pixel count', 'mobile computing', 'moving objects', 'Blender', '3d environment', 'workflows', 'full resolution', 'Visio', 'external device', 'poor contrast', 'driver support', 'post processing effects', 'SoCs', 'existing tools', 'hardware devices', 'face tracking', 'simple editing', 'performance impact', 'separate chip', 'black smear', 'static image', 'basic software', 'desktop experience', 'modelling', 'LaTeX', 'Arduino', 'existing apps', 'battery savings', 'Fresnel lenses', 'apis', 'weak hardware', 'good resolution', 'data flow', 'software updates', 'LUT', '3d program', 'game engine', 'tilt-shift effect', 'pretty low resolution', 'jitter', 'proprietary hardware', 'micro controller', 'so much data', 'higher frame rates', 'exact resolution', 'shit camera', 'higher resolution images', 'UE4', 'low-level programming', 'realtime', 'proprietary system', 'wide FOV', 'Stellarium', 'high ISO performance', 'advanced functionality', 'industry standard', 'form factor', 'modern device', '3D engine', 'other softwares', 'zbrush', 'gyroscope', 'portrait monitor', 'hand drawing', 'raster', 'modern lenses', 'RPi', 'manual adjustments', 'cloud processing', 'actual phone', 'audio', 'cubemaps', 'wider FOV', 'I/O.', 'Autofocus', 'screen door effect', 'various algorithms', 'visual artifacts', 'wacom', 'powerful hardware', 'hardware components', 'LaTeX', 'hardware perspective', 'GUI', 'PS4 hardware', 'core OS', 'Autodesk Maya', 'hmd', 'Android', 'higher res', 'audio chip', 'Capture One', 'professional use', 'C4D', '3D elements', 'default camera', 'Android applications', '60D', 'Aperture', 'missing frames', 'multi touch', 'based software', 'most emulators', 'Visio', 'just a prototype', 'Cinema 4D', 'custom shaders', 'Adobe Flash', 'JPEG compression', 'digital audio', 'Sony sensor', 'more computational power', 'internal hardware', 'palm rejection', 'Machine learning', 'Aperture', 'exact same hardware', 'recording feature', 'normal mapping', 'current camera', 'compilers', 'white balance', 'remote shutter', '3D effect', 'fpga', 'Android phones', 'slower performance', 'S-pen', 'auto exposure', 'virtual objects', 'performance improvements', 'HDR+', 'low level access', 'right resolution', 'more processing', 'basic settings', 'ISO performance', '4k screen', 'laptop mode', 'GH4', 'arduino code', 'Matlab', 'higher refresh rates', 'digital editing', 'UX.', 'firmware', 'programming software', 'custom code', 'circuitry', 'machine learning algorithms', 'better images', 'input data', 'apps', 'CPU power', 'wide aperture', 'low ISO', 'porting', 'OneNote', 'other devices', 'supercomputing', 'digital painting', 'modern computer', '1020', 'timecode', 'API', 'good screen', 'shitty camera', 'Pro Tools', '24-bit', 'manual camera', 'blurry mess', 'floating point math', 'Sketchbook Pro', 'QuickTime', 'professional video', 'neural networks', 'QHD screen', 'UX design', 'deinterlacing', 'file format', 'existing software', 'Inkscape', 'SoC', 'AOSP roms', 'colour balance', 'instruction set', 'Blender', 'Image quality', 'AMOLED', 'BMPCC', 'depth buffer', 'Hololens', 'good user experience', 'interpolating', 'firmware updates', 'IPS panels', 'PSD files', 'rendering power', 'mobile development', 'controller input', 'high dynamic range', 'multitouch gestures', 'same interface', 'Matlab', 'display brightness', 'dual screens', 'DK2', 'lens quality', 'ARM chips', 'dynamic contrast', 'faster performance', 'applications', 'full frame sensor', 'third party plugins', 'wireless chip', 'Retina', 'deferred rendering', 'algorithm design', 'UI.', 'Hololens', 'correct settings', 'Pi', 'surface tablet', 'older tech', 'tweening', 'newer processor', 'photoshop/illustrator', 'AMOLED screens', 'ARM.', 'HDR.', 'improved performance', '3D sound', 'more software', 'quantum dots', 'productivity apps', 'webgl', 'camera', 'Android OS', 'network connectivity', 'capacitive touch screen', 'I2C', 'computers', 'arduino', 'extra memory', 'resampling', 'native format', 'ChromeOS', 'better screen', 'hardware resources', 'included software', 'graphics acceleration', 'multi-monitor support', 'high frame rate', 'lower res', 'GIMP.', 'Nikons', 'less hardware', 'software solutions', 'other architectures', 'smaller screen', 'computer technology', 'CMYK', 'battery size', 'particular feature', 'FV-5', 'most TVs', 'fine details', 'D5300', 'workflow', 'screenspace', 'low-light situations', 'data transfer', 'useability', 'system architecture', 'video editor', 'physical hardware', 'extra screen real estate', 'scene graph', 'Pro tools', 'sensors', 'UI components', 'mirrorless', 'auto settings', 'capture device', 'Flat design', 'Android', 'lens selection', 'fast processor', 'outputted', 'Kinect', 'narrow depth', 'swivel screen', '10-bit', 'data acquisition', 'tinkering', 'decent hardware', 'lower resolution screen', 'multiple sensors', 'more powerful hardware', 'XCode', 'same hardware', 'Megapixels', 'SP3', 'board memory', 'database work', 'pixel ratio', 'larger lenses', 'iOS apps', 'Interpolation', 'new sensor', 'intervalometer', 'most DSLRs', 'better picture quality', 'same sensor', 'laptops/desktops', 'base ISO', 'Vray', 'specific drivers', 'storage size', 'HMI', 'specific monitor', 'wireless power', 'data logging', 'chromatic aberrations', 'wireless communication', 'complex functions', 'wavelets', 'browser support', 'CATIA', 'desktop ones', 'low-end hardware', 'DSLR', 'Cryengine', 'debugging', 'extra functionality', 'hardware setup', 'decent camera', 'More pixels', 'Photoshop CS6', 'good VR experience', 'instruction sets', 'higher pixel density', 'general workflow', 'high res', 'Sony Vegas', 'technical limitations', 'digital art', 'real device', 'API.', 'Downsampling', 'PCB design', 'poor image quality', 'prototype', '4K', 'spot metering', '3d', 'color image', 'available hardware', 'shutter speed', 'mbed', 'program', 'point-and-shoot', 'Cardboard', 'digital equipment', 'Adobe Illustrator', 'screen space', 'ART.', 'game streaming', 'higher resolution display', 'screen colors', 'CPU overhead', 'Most software', 'different architectures', 'most monitors', 'Kinect', 'coding side', 'worse camera', 'user side', 'video-editing', 'codecs', 'overscan', 'mirrorless', 'Zbrush', 'visual feedback', 'native development', 'electronic shutter', 'exact hardware', 'connectivity', 'touchscreen', 'more calculations', 'more screen', 'most computers', 'hardware buttons', 'manual lens', 'Web browsing', '3D screen', 'fixed lens', '3rd party apps', 'cintiq', 'multi-threading', 'stylus support', 'Rendering', 'best user experience', 'many different devices', 'HoloLens', 'transcoding', '3D game', 'software application', 'little programming', 'computer processors', 'VR app', 'ARM processor', 'focus speed', 'several filters', 'Mac software', 'basic trigonometry', 'programming knowledge', 'large file sizes', 'UX.', 'information architecture', 'embedded hardware', 'system resources', 'good interface', 'existing solutions', 'video output', 'gpu acceleration', 'programming environment', 'Wacom tablet', 'excellent camera', 'Intel hardware', 'current computers', 'graceful degradation', 'render farm', 'Unity editor', 'simulation software', 'digital signals', 'software packages', 'adobe suite', 'particle systems', 'visual designer', 'runtime', 'hardware capabilities', 'high resolution images', 'SVMs', 'video players', 'RAM management', 'antialiasing', 'LTSpice', 'bump maps', 'real world usage', 'iMovie', 'multi-core CPUs', 'specific use case', 'brightness', 'CV1', 'memory performance', 'LUTs', 'Android 5.0', 'OS', 'Wacom pen', 'VR.', 'smoothing', 'SLRs', 'unity engine', 'interchangeable lenses', 'scaling problems', 'SDE', 'graphics', 'current devices', 'PowerPoint', 'pocket camera', '4:4:4', 'performant', 'external camera', 'Android Wear', 'wide apertures', 'parallax effect', 'maximum brightness', 'Xsplit', 'phone manufacturers', 'given device', 'scanline', 'RGB', 'Chrome OS', 'prepress', 'Chromebook', 'parallax mapping', 'strobing', 'dynamic lighting', 'I/O', 'image change', 'android phones', 'programming', 'Virtual Desktop', 'digital circuits', 'implementations', 'specific settings', 'very good camera', 'Transcoding', 'technical know-how', 'emulation', 'GearVR', 'headtracking', 'conferencing', '4K screen', '16mp', 'AVRs', 'much lower latency', 'programming tools', 'computationally', 'real camera', 'SDK.', 'too much processing power', 'hardware upgrades', '4k display', 'newer technology', 'normal computer', 'database design', 'high performance computing', 'advanced functions', 'retouching', 'multimonitor', 'RNNs', 'custom drivers', 'web video', 'audio', 'Manga Studio', 'shadow maps', 'autolayout', 'video performance', 'few devices', 'haptic feedback', 'audio inputs', 'Pixelmator', 'power savings', 'graphics API', 'H264', 'input lag', 'video quality', 'simple graphics', '3D content', 'long exposures', 'bitrates', 'computer experience', 'PC monitors', 'professional applications', 'just graphics', 'MFT', 'system level', 'lower level languages', 'actual devices', 'Note4', 'native resolution', 'complex math', 'extensibility', 'LED screens', 'based tracking', 'graphics work', 'manual setting', 'improved battery life', 'slow shutter', 'desktop versions', 'decent screen', 'parallelized', 'DOF', 'camera lenses', 'spatial data', 'LVDS', 'proper interface', 'Autofocus', 'UI scaling', '70D', 'colour correction', 'Lightboost', 'faster hardware', 'control system', 'autodesk', 'base station', 'color distortion', 'platform agnostic', 'A7s', 'real environment', 'additional configuration', 'opengl', 'input/output', 'BLE', \"Raspberry Pi's\", 'fast CPU', 'pixelation', 'support libraries', 'data representation', 'magic lantern', 'additional features', 'low light conditions', 'decoder', 'Windows APIs', 'color noise', 'hardware stuff', 'mobile applications', 'focusing', 'mobile platform', 'based applications', 'bitmap', 'SP3', 'pen tablet', 'resolution change', 'multi tasking', 'high resolutions', 'optics', 'ULMB', 'mirrorless camera', 'Windows drivers', 'tessellation', 'Illustrator', 'general performance', 'D810', 'desktop', 'Wacom digitizer', 'technical limitation', 'Vive', 'development environment', 'native resolutions', 'web surfing', 'MEMS', 'artificial neural networks', 'resolution issue', 'audio data', 'same sensors', 'standard resolution', 'different interface', 'SDE', 'stock software', 'mirrorless cameras', 'texturing', 'more screen real estate', 'single screen', 'even phones', 'modular design', 'quantization', 'own firmware', 'AirPlay', 'bigger screens', 'RPI2', 'Google Glass', 'certain phones', 'virtualization', 'Lumia Camera', 'different camera', 'post process', 'processors', 'software end', 'FreeSync', 'Xcode', 'tonal range', 'second camera', 'extra functions', 'HDR image', 'better sensors', 'hardware improvements', 'chromebooks', 'high precision', 'VBA', 'firmware', 'most mobile devices', 'camera model', 'devkit', 'DK1', 'external monitor', 'digital system', 'architecture', 'post production', 'default camera app', 'rear camera', 'IPhone', 'different technology', 'CPU performance', 'custom kernels', 'various software', 'consumer hardware', 'high-res', 'movie mode', 'older TVs', '7D2', 'Touch screen', 'images/video', 'graphic design', 'higher refresh rate', 'camera flash', 'design side', 'Unity3D', 'blur reduction', 'programming side', 'simple software', 'consumer device', 'hardware engineers', 'related settings', 'based rendering', 'e-ink display', 'digitization', 'DAWs', 'graphical effects', 'mobile products', 'D7100', 'desktop programs', 'moving camera', 'best screen', 'computer power', '3D', 'mecanim', 'iOS']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ITnSohmGpPe3","executionInfo":{"status":"ok","timestamp":1615218303686,"user_tz":-60,"elapsed":1083,"user":{"displayName":"MD Salem Messoud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBYoVQGAImGxJfhgMMK1daBThGNpiaASkBXr4EYpE=s64","userId":"15534871031717137879"}},"outputId":"a2d9aa61-4a7a-4df1-c2b6-bad8439b1e21"},"source":["print(len(set(most_similar + most_similar2)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2065\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cugeiVgfysGi"},"source":["# Sense2vec vocabulary with patent terms"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TnV-tyKNj7l1","executionInfo":{"status":"ok","timestamp":1614892230100,"user_tz":-60,"elapsed":127873,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"a74338de-428b-4acc-e4a7-fc0cdd2c25ec"},"source":["termsins2v = []\n","for t in tqdm(patentdf.index):\n","    if nlp(t)[:]._.in_s2v:\n","        termsins2v+=[t]\n","print(len(termsins2v))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 13220/13220 [01:54<00:00, 115.67it/s]"],"name":"stderr"},{"output_type":"stream","text":["3458\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":125},"id":"9oRgR96LZDcH","executionInfo":{"status":"ok","timestamp":1614719506371,"user_tz":-60,"elapsed":149196,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"bb19f520-8d52-4b47-aeb8-b02d70685f5d"},"source":["', '.join(termsins2v[:222])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'image processing, electronic device, control unit, information processing, display device, image sensor, light source, coordinate system, computing device, augmented reality, computer program, storage medium, data processing, virtual reality, user input, road surface, virtual object, optical system, threshold value, computer system, storage device, control device, virtual space, input device, image analysis, mobile terminal, digital image, visible light, dynamic range, straight line, display screen, image quality, location information, mobile device, signal processing, image recognition, horizontal direction, image processor, reference point, data storage, still image, random access, wireless communication, vertical direction, deep learning, power source, light sources, first position, blood flow, raw image, user information, real space, composite image, real object, focal length, second position, real time, exposure time, touch panel, input data, visual field, hard disk, first light, operating system, magnetic field, output device, color space, color image, measurement device, depth map, color value, camera system, central processing unit, transfer function, video stream, virtual image, physical object, monitoring system, high resolution, graphics processor, information storage, high dynamic range, frame rate, data structure, tracking system, video frame, white balance, light sensor, communication network, probability distribution, bright spot, image plane, automated driving, external device, mobile phone, light beam, local area network, water content, eye tracking, edge detection, storage media, flash memory, power consumption, electrical signal, prior art, normal vector, color difference, motor vehicle, upper limit, main memory, key frame, high quality, current time, motion detection, color temperature, high frequency, neural networks, image editing, video content, image resolution, mobile computing, parallel processing, object model, personal computer, cost function, virtual world, wide angle, rear camera, flow rate, user interfaces, management system, vanishing point, autonomous vehicle, scanned image, computer programs, location data, virtual environment, wearable device, traffic light, reference frame, multiple images, heart rate, video display, measurement system, data value, surface area, color calibration, low frequency, solid state, focal plane, biological data, electronic devices, visual line, color chart, volatile memory, skin color, skin condition, computer vision, identity verification, front end, processing information, focal distance, home server, camera model, specific area, white blood, projection screen, digital video, hierarchical structure, artificial intelligence, disk drive, image compression, probability density, steering angle, angular velocity, focal point, parking lot, compact disc, external storage, display size, device management, external memory, image size, color saturation, video processing, electrical wire, lane change, aspect ratio, electromagnetic wave, spinal column, face detection, data structures, control panel, software program, optical disc, wireless network, high speed, flow meter, social network, information system, first state, second stage, color grading, color filter, physical environment, privacy protection, block size, optical sensor, wireless transmission, first stage, hard disk drive, third position, pixel density, storage system, probability density function, oxygen saturation, horizontal plane, blind spot, floor plan, color correction, optic nerve, relative speed'"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7JXGrPpGmEKV","executionInfo":{"status":"ok","timestamp":1614892230108,"user_tz":-60,"elapsed":127872,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"be8c834a-5375-45c4-a3fa-38c6da353ed1"},"source":["seedterms = set.intersection(set(most_similar + most_similar2), set(termsins2v))\n","print(', '.join(seedterms))\n","print(len(seedterms))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["deep learning, human-computer interaction, embedded devices, raw format, small screen, web browsing, graphics engine, augmented reality, neural networks, software implementation, digital video, pixel density, software level, development environment, single screen, digital processing, portable devices, rear camera, motion detection, optical system, white balance, sensor size, software tools, user experience, dynamic range, data storage, digital system, computer programs, computer graphics, edge detection, image analysis, digital systems, rendering engines, signal processing, screen size, spatial data, color temperature, manual focus, high resolution, wireless technology, memory management, computer power, deferred rendering, system performance, video conferencing, color display, color grading, high precision, processing power, display device, positional tracking, picture quality, mobile platform, image processor, body tracking, machine vision, texture mapping, wireless networking, live view, haptic feedback, raw image, modular design, software program, light meter, curved screen, external device, parallel computing, computer software, color space, software suite, cloud processing, rendering pipeline, graphical design, open standard, mobile processor, face detection, software tool, image editing, batch processing, embedded device, software stack, electronic viewfinder, specialized software, operating systems, simulation software, depth map, analog circuits, image quality, video production, color correction, vector graphics, vector images, video stabilization, physics simulation, video streaming, eye tracking, game engine, computer vision, video processing, instruction sets, video quality, word processing, graphics pipeline, digital circuits, interface design, computer architecture, file management, graphic design, camera model, display size, pure data, media players, render target, application software, color profile, wireless communication, computer technology, industry standard, color calibration, artificial neural networks, embedded hardware, sound processing, computing power, video encoding, design software, data processing, human visual system, real camera, user interfaces, base station, color management, wireless communications, video recording, camera sensors, handheld devices, high dynamic range, dynamic contrast, photo manipulation, software design, processor speed, colour correction, digital signal processing, image recognition, natural language processing, error correction, global shutter, file format, mobile computing, data manipulation, light sensor, input device, optical viewfinder, color depth, user interface design, camera lenses, high frame rate, machine learning algorithms, image sensor, parallel processing, face tracking, image stabilization, high performance computing, chromatic aberration, processing speed, computational power, user testing, main processor, human vision, color image, contrast ratio, physical design, wireless power, input data, embedded system, rapid prototyping, optical image stabilization, image compression, circuit design, programming environment, camera tracking, embedded software, distributed computing, computer hardware, camera system, visual feedback, image manipulation, audio processing, mouse input, control theory\n","189\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3mPRaeR8s4fe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615220636864,"user_tz":-60,"elapsed":5053,"user":{"displayName":"MD Salem Messoud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBYoVQGAImGxJfhgMMK1daBThGNpiaASkBXr4EYpE=s64","userId":"15534871031717137879"}},"outputId":"960ed652-2129-4a94-d97d-8a7945d93e87"},"source":["!pip install pyngrok --quiet\n","from pyngrok import ngrok"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[?25l\r\u001b[K     |▍                               | 10kB 15.9MB/s eta 0:00:01\r\u001b[K     |▉                               | 20kB 12.8MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30kB 8.0MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40kB 6.6MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51kB 4.1MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61kB 4.3MB/s eta 0:00:01\r\u001b[K     |███                             | 71kB 4.8MB/s eta 0:00:01\r\u001b[K     |███▌                            | 81kB 5.0MB/s eta 0:00:01\r\u001b[K     |████                            | 92kB 5.3MB/s eta 0:00:01\r\u001b[K     |████▍                           | 102kB 4.2MB/s eta 0:00:01\r\u001b[K     |████▉                           | 112kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 122kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 133kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 143kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 153kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████                         | 163kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 174kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████                        | 184kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 194kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 204kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 215kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 225kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 235kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 245kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████                     | 256kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 266kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████                    | 276kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 286kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 296kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 307kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 317kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 327kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 337kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 348kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 358kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 368kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 378kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 389kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 399kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 409kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 419kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 430kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 440kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 450kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 460kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 471kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 481kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 491kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 501kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 512kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 522kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 532kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 542kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 552kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 563kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 573kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 583kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 593kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 604kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 614kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 624kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 634kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 645kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 655kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 665kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 675kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 686kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 696kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 706kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 716kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 727kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 737kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 747kB 4.2MB/s \n","\u001b[?25h  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6UVXOkbFwRNC"},"source":["## Build terminology list\n","https://github.com/explosion/sense2vec#recipe-sense2vecteach \\\n","https://prodi.gy/docs/recipes#terms"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZaaHjTKNlgdB","executionInfo":{"status":"ok","timestamp":1614097302307,"user_tz":-60,"elapsed":406535,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"54f74e3e-c472-4076-a896-45efa27aee85"},"source":["# !prodigy sense2vec.teach termsg06t s2v_old --seeds \"image processing, electronic device, control unit, information processing, point cloud, display device, block diagram, user interface, image sensor, light source, coordinate system, computing device, augmented reality, neural network, computer program, storage medium, data processing, virtual reality, user input, road surface, virtual object, optical system, threshold value, computer system, storage device, machine learning, control device, virtual space, input device, image analysis, integrated circuit, mobile terminal, digital image, visible light, dynamic range, straight line, display screen, image quality, location information, mobile device, signal processing, video signal, training data, image recognition, horizontal direction, medical imaging, image processor, control system, reference point, data storage, random access, wireless communication, vertical direction, camera module, deep learning, program code, power source, low resolution, light sources, first position, blood flow, raw image, user information, real space, composite image, real object, focal length, object recognition, second position, control module, graphical representation, real time, voice recognition, exposure time, touch panel, input data, visual field, hard disk, first light, operating system, data acquisition, magnetic field, distance measurement, output device, travel lane, display mode, circuit board, color space, color image, measurement device, laser light, depth map, ray tracing, color value, camera system, infrared light, time series, digital signal, central processing unit, transfer function, video stream, virtual image, network interface, physical object, monitoring system, high resolution, noise reduction, graphics processor, display controller, information storage, high dynamic range, face recognition, frame rate, data structure, tracking system, light field, first data, video frame, white balance, light sensor, communication network, probability distribution, bright spot, physical space, image plane, automated driving, motion sensor, external device, mobile phone, power supply, light beam, local area network, start point, water content, eye tracking, edge detection, navigation system, storage media, flash memory, power consumption, electrical signal, prior art, master control, normal vector, color difference, motor vehicle, upper limit, main memory, vertical axis, rotation matrix, key frame, high quality, digital camera, vertex shader, wire harness, current time, motion detection, time interval, color temperature, high frequency, neural networks, video content, image resolution, mobile computing, parallel processing, object model, personal computer, cost function, virtual world, wide angle, character recognition, cross section, rear camera, flow rate, video camera, management system, vanishing point, data transmission, objective lens, autonomous vehicle, computer programs, sensor array, audio signal, audio data, location data, virtual environment, wearable device, texture map, traffic light, side wall, power management, printed circuit board, reference frame, multiple images, heart rate, video display, measurement system, data value, ground truth, surface area, color calibration, low frequency, solid state, data stream, focal plane, biological data, data store, electronic devices, visual line, color chart, volatile memory, skin color, skin condition, computer vision, store data, view angle, white light, identity verification, front end, radio frequency, processing information, focal distance\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n","  \"\"\")\n","Starting with seed keys: ['image_processing|NOUN', 'electronic_device|NOUN', 'control_unit|NOUN', 'information_processing|NOUN', 'point_cloud|NOUN', 'display_device|NOUN', 'block_diagram|NOUN', 'user_interface|NOUN', 'image_sensor|NOUN', 'light_source|NOUN', 'coordinate_system|NOUN', 'computing_device|NOUN', 'augmented_reality|NOUN', 'neural_network|NOUN', 'computer_program|NOUN', 'storage_medium|NOUN', 'data_processing|NOUN', 'virtual_reality|NOUN', 'user_input|NOUN', 'road_surface|NOUN', 'virtual_object|NOUN', 'optical_system|NOUN', 'threshold_value|NOUN', 'computer_system|NOUN', 'storage_device|NOUN', 'machine_learning|NOUN', 'control_device|NOUN', 'virtual_space|NOUN', 'input_device|NOUN', 'image_analysis|NOUN', 'integrated_circuit|NOUN', 'mobile_terminal|NOUN', 'digital_image|NOUN', 'visible_light|NOUN', 'dynamic_range|NOUN', 'straight_line|NOUN', 'display_screen|NOUN', 'image_quality|NOUN', 'location_information|NOUN', 'mobile_device|NOUN', 'signal_processing|NOUN', 'video_signal|NOUN', 'training_data|NOUN', 'image_recognition|NOUN', 'horizontal_direction|NOUN', 'medical_imaging|NOUN', 'image_processor|NOUN', 'control_system|NOUN', 'reference_point|NOUN', 'data_storage|NOUN', 'random_access|NOUN', 'wireless_communication|NOUN', 'vertical_direction|NOUN', 'camera_module|NOUN', 'deep_learning|NOUN', 'program_code|NOUN', 'power_source|NOUN', 'low_resolution|NOUN', 'light_sources|NOUN', 'first_position|NOUN', 'blood_flow|NOUN', 'raw_image|NOUN', 'user_information|NOUN', 'real_space|NOUN', 'composite_image|NOUN', 'real_object|NOUN', 'focal_length|NOUN', 'object_recognition|NOUN', 'second_position|NOUN', 'control_module|NOUN', 'graphical_representation|NOUN', 'real_time|NOUN', 'voice_recognition|NOUN', 'exposure_time|NOUN', 'touch_panel|NOUN', 'input_data|NOUN', 'visual_field|NOUN', 'hard_disk|NOUN', 'First_Light|ORG', 'operating_system|NOUN', 'data_acquisition|NOUN', 'magnetic_field|NOUN', 'distance_measurement|NOUN', 'output_device|NOUN', 'travel_lane|NOUN', 'display_mode|NOUN', 'circuit_board|NOUN', 'color_space|NOUN', 'color_image|NOUN', 'measurement_device|NOUN', 'laser_light|NOUN', 'depth_map|NOUN', 'ray_tracing|NOUN', 'color_value|NOUN', 'camera_system|NOUN', 'infrared_light|NOUN', 'time_series|NOUN', 'digital_signal|NOUN', 'central_processing_unit|NOUN', 'transfer_function|NOUN', 'video_stream|NOUN', 'virtual_image|NOUN', 'network_interface|NOUN', 'physical_object|NOUN', 'monitoring_system|NOUN', 'high_resolution|NOUN', 'noise_reduction|NOUN', 'graphics_processor|NOUN', 'display_controller|NOUN', 'information_storage|NOUN', 'high_dynamic_range|NOUN', 'face_recognition|NOUN', 'frame_rate|NOUN', 'data_structure|NOUN', 'tracking_system|NOUN', 'light_field|NOUN', 'first_data|NOUN', 'video_frame|NOUN', 'white_balance|NOUN', 'light_sensor|NOUN', 'communication_network|NOUN', 'probability_distribution|NOUN', 'bright_spot|NOUN', 'physical_space|NOUN', 'image_plane|NOUN', 'automated_driving|NOUN', 'motion_sensor|NOUN', 'external_device|NOUN', 'mobile_phone|NOUN', 'power_supply|NOUN', 'light_beam|NOUN', 'local_area_network|NOUN', 'start_point|NOUN', 'water_content|NOUN', 'eye_tracking|NOUN', 'edge_detection|NOUN', 'navigation_system|NOUN', 'storage_media|NOUN', 'flash_memory|NOUN', 'power_consumption|NOUN', 'electrical_signal|NOUN', 'prior_art|NOUN', 'master_control|NOUN', 'normal_vector|NOUN', 'color_difference|NOUN', 'motor_vehicle|NOUN', 'upper_limit|NOUN', 'main_memory|NOUN', 'vertical_axis|NOUN', 'rotation_matrix|NOUN', 'key_frame|NOUN', 'high_quality|NOUN', 'digital_camera|NOUN', 'vertex_shader|NOUN', 'wire_harness|NOUN', 'current_time|NOUN', 'motion_detection|NOUN', 'time_interval|NOUN', 'color_temperature|NOUN', 'high_frequency|NOUN', 'neural_networks|NOUN', 'video_content|NOUN', 'image_resolution|NOUN', 'mobile_computing|NOUN', 'parallel_processing|NOUN', 'object_model|NOUN', 'personal_computer|NOUN', 'cost_function|NOUN', 'virtual_world|NOUN', 'wide_angle|NOUN', 'character_recognition|NOUN', 'cross_section|NOUN', 'rear_camera|NOUN', 'flow_rate|NOUN', 'video_camera|NOUN', 'management_system|NOUN', 'vanishing_point|NOUN', 'data_transmission|NOUN', 'objective_lens|NOUN', 'autonomous_vehicle|NOUN', 'computer_programs|NOUN', 'sensor_array|NOUN', 'audio_signal|NOUN', 'audio_data|NOUN', 'location_data|NOUN', 'virtual_environment|NOUN', 'wearable_device|NOUN', 'texture_map|NOUN', 'traffic_light|NOUN', 'side_wall|NOUN', 'power_management|NOUN', 'printed_circuit_board|NOUN', 'reference_frame|NOUN', 'multiple_images|NOUN', 'heart_rate|NOUN', 'video_display|NOUN', 'measurement_system|NOUN', 'data_value|NOUN', 'ground_truth|NOUN', 'surface_area|NOUN', 'color_calibration|NOUN', 'low_frequency|NOUN', 'solid_state|NOUN', 'data_stream|NOUN', 'focal_plane|NOUN', 'biological_data|NOUN', 'data_store|NOUN', 'electronic_devices|NOUN', 'visual_line|NOUN', 'color_chart|NOUN', 'volatile_memory|NOUN', 'skin_color|NOUN', 'skin_condition|NOUN', 'computer_vision|NOUN', 'store_data|NOUN', 'view_angle|NOUN', 'white_light|NOUN', 'identity_verification|NOUN', 'front_end|NOUN', 'radio_frequency|NOUN', 'processing_information|NOUN', 'focal_distance|NOUN']\n","\n","✨  Starting the web server at http://localhost:8080 ...\n","Open the app in your browser and start annotating!\n","\n","\u001b[31mERROR\u001b[0m:    [Errno 98] error while attempting to bind on address ('127.0.0.1', 8080): address already in use\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_mTJUUK4sRK9","executionInfo":{"status":"ok","timestamp":1614331630591,"user_tz":-60,"elapsed":5399,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"b719508b-7f6b-42d4-94eb-6d74faae6674"},"source":["\n","# Terminate open tunnels if exist\n","ngrok.kill()\n","\n","# Setting the authtoken (optional)\n","# Get your authtoken from https://dashboard.ngrok.com/auth\n","# NGROK_AUTH_TOKEN = \"\"\n","# ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n","\n","# Open an HTTPs tunnel on port 5000 for http://localhost:5000\n","public_url = ngrok.connect(addr='8000', proto=\"http\", options={\"bind_tls\": True})\n","print(\"Tracking URL:\", public_url)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tracking URL: NgrokTunnel: \"http://341150591164.ngrok.io\" -> \"http://localhost:8000\"\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"knQ8E5o-rj2_","executionInfo":{"status":"ok","timestamp":1614332612674,"user_tz":-60,"elapsed":984496,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"bd46363b-3660-49d0-edd5-c920c3ac0a54"},"source":["# ! PRODIGY_PORT=8000 PRODIGY_HOST=127.0.0.1 prodigy sense2vec.teach termsg06t s2v_old --seeds \"image processing, electronic device, control unit, information processing, point cloud, display device, block diagram, user interface, image sensor, light source, coordinate system, computing device, augmented reality, neural network, computer program, storage medium, data processing, virtual reality, user input, road surface, virtual object, optical system, threshold value, computer system, storage device, machine learning, control device, virtual space, input device, image analysis, integrated circuit, mobile terminal, digital image, visible light, dynamic range, straight line, display screen, image quality, location information, mobile device, signal processing, video signal, training data, image recognition, horizontal direction, medical imaging, image processor, control system, reference point, data storage, random access, wireless communication, vertical direction, camera module, deep learning, program code, power source, low resolution, light sources, first position, blood flow, raw image, user information, real space, composite image, real object, focal length, object recognition, second position, control module, graphical representation, real time, voice recognition, exposure time, touch panel, input data, visual field, hard disk, first light, operating system, data acquisition, magnetic field, distance measurement, output device, travel lane, display mode, circuit board, color space, color image, measurement device, laser light, depth map, ray tracing, color value, camera system, infrared light, time series, digital signal, central processing unit, transfer function, video stream, virtual image, network interface, physical object, monitoring system, high resolution, noise reduction, graphics processor, display controller, information storage, high dynamic range, face recognition, frame rate, data structure, tracking system, light field, first data, video frame, white balance, light sensor, communication network, probability distribution, bright spot, physical space, image plane, automated driving, motion sensor, external device, mobile phone, power supply, light beam, local area network, start point, water content, eye tracking, edge detection, navigation system, storage media, flash memory, power consumption, electrical signal, prior art, master control, normal vector, color difference, motor vehicle, upper limit, main memory, vertical axis, rotation matrix, key frame, high quality, digital camera, vertex shader, wire harness, current time, motion detection, time interval, color temperature, high frequency, neural networks, video content, image resolution, mobile computing, parallel processing, object model, personal computer, cost function, virtual world, wide angle, character recognition, cross section, rear camera, flow rate, video camera, management system, vanishing point, data transmission, objective lens, autonomous vehicle, computer programs, sensor array, audio signal, audio data, location data, virtual environment, wearable device, texture map, traffic light, side wall, power management, printed circuit board, reference frame, multiple images, heart rate, video display, measurement system, data value, ground truth, surface area, color calibration, low frequency, solid state, data stream, focal plane, biological data, data store, electronic devices, visual line, color chart, volatile memory, skin color, skin condition, computer vision, store data, view angle, white light, identity verification, front end, radio frequency, processing information, focal distance\"\n","! PRODIGY_PORT=8000 PRODIGY_HOST=127.0.0.1 prodigy sense2vec.teach termsg06t s2v_old --seeds \"linear programming, operating systems, curved screen, light sensor, software programming, product design, video quality, embedded system, physics simulation, readable code, graphic design, graphics engine, digital computers, procedural programming, linear algebra, human visual system, embedded device, electrical circuits, design software, embedded software, graph theory, single screen, specialized software, digital logic, application software, additive manufacturing, science fiction, user experience, industry standard, rendering engines, software program, software tools, white balance, remote sensing, data storage, mechanical systems, mobile computing, interactive media, wireless communications, input data, color correction, artificial neural networks, stochastic processes, programming languages, rear camera, lighting design, camera model, computer animation, circuit design, visual artist, digital technology, electric circuits, spatial data, cloud processing, distributed systems, graphics pipeline, pure data, computing technology, numerical methods, mobile platform, image processing, development environment, photo manipulation, computer graphics, machine vision, screen size, instruction sets, computer programming, file format, main processor, raw image, texture mapping, simulation software, wireless power, augmented reality, computer vision, motion detection, electronic viewfinder, computer language, digital signal processing, reference material, video streaming, handheld devices, visual communication, information technology, video production, product designer, genetic algorithms, user testing, high frame rate, color grading, graphical design, color profile, memory management, modular design, logic programming, computer program, dynamic range, visual perception, computer simulations, classical mechanics, wireless technology, video conferencing, software project, machine learning algorithms, wireless networking, color space, user interfaces, small screen, word processing, computer power, image sensor, sound processing, digital media, visual media, software tool, camera tracking, optical system, functional analysis, camera sensors, high resolution, eye tracking, programming environment, color depth, mechanical design, computer modeling, complex analysis, trigonometric functions, software implementation, projection mapping, inverse kinematics, video encoding, online learning, audio processing, digital video, computer architecture, data processing, processing speed, analog electronics, computing power, finite element analysis, color display, computer networking, computational power, live view, mechanical engineering, software development, programming language, signal processing, computer technology, image quality, image stabilization, software architecture, high dynamic range, raw format, behavior analysis, digital processing, error correction, computer code, virtual environments, digital circuits, high performance computing, external device, software stack, big data analysis, base station, color management, 3d models, real camera, software level, optical image stabilization, control theory, file management, computer programs, image editing, unsupervised learning, video processing, computer science, camera lenses, computer hardware, haptic feedback, movie making, vector images, visual effects, video recording, rapid prototyping, picture quality, system performance, mobile processor, computer scientists, computer languages, positional tracking, distributed computing, image processor, sensor size, spatial reasoning, image manipulation, deep learning, image compression, natural language processing, render target, parallel processing, optical viewfinder, differential equations, interface design, data structures, chromatic aberration, visual feedback, body tracking, mouse input, human-computer interaction, rendering pipeline, display size, dynamical systems, computational complexity, texture maps, vector graphics, light meter, reinforcement learning, edge detection, high precision, processing power, face detection, design process, digital system, analog circuits, software engineering, color temperature, software design, display device, user interface design, manual focus, input device, contrast ratio, artificial intelligence, embedded devices, civil engineering, color calibration, portable devices, software suite, dynamic contrast, global shutter, computational methods, neural networks, camera system, batch processing, visual design, wireless communication, image recognition, computer software, face tracking, game engine, process control, state machines, color image, image analysis, depth map, interior design, embedded hardware, colour correction, virtual reality, deferred rendering, parallel computing, assembly language, physical design, digital systems, audio engineering, video stabilization, numerical analysis, fluid dynamics, open standard, mathematical modeling, web browsing, motion picture, still life, processor speed, motion pictures, pixel density, data manipulation, human vision, media players\"\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n","  \"\"\")\n","Starting with seed keys: ['linear_programming|NOUN', 'operating_systems|NOUN', 'curved_screen|NOUN', 'light_sensor|NOUN', 'software_programming|NOUN', 'product_design|NOUN', 'video_quality|NOUN', 'embedded_system|NOUN', 'physics_simulation|NOUN', 'readable_code|NOUN', 'graphic_design|NOUN', 'graphics_engine|NOUN', 'digital_computers|NOUN', 'procedural_programming|NOUN', 'linear_algebra|NOUN', 'human_visual_system|NOUN', 'embedded_device|NOUN', 'electrical_circuits|NOUN', 'design_software|NOUN', 'embedded_software|NOUN', 'graph_theory|NOUN', 'single_screen|NOUN', 'specialized_software|NOUN', 'digital_logic|NOUN', 'application_software|NOUN', 'additive_manufacturing|NOUN', 'science_fiction|NOUN', 'user_experience|NOUN', 'industry_standard|NOUN', 'rendering_engines|NOUN', 'software_program|NOUN', 'software_tools|NOUN', 'white_balance|NOUN', 'remote_sensing|NOUN', 'data_storage|NOUN', 'mechanical_systems|NOUN', 'mobile_computing|NOUN', 'interactive_media|NOUN', 'wireless_communications|NOUN', 'input_data|NOUN', 'color_correction|NOUN', 'artificial_neural_networks|NOUN', 'stochastic_processes|NOUN', 'programming_languages|NOUN', 'rear_camera|NOUN', 'lighting_design|NOUN', 'camera_model|NOUN', 'computer_animation|NOUN', 'circuit_design|NOUN', 'visual_artist|NOUN', 'digital_technology|NOUN', 'electric_circuits|NOUN', 'spatial_data|NOUN', 'cloud_processing|NOUN', 'distributed_systems|NOUN', 'graphics_pipeline|NOUN', 'pure_data|NOUN', 'computing_technology|NOUN', 'numerical_methods|NOUN', 'mobile_platform|NOUN', 'image_processing|NOUN', 'development_environment|NOUN', 'photo_manipulation|NOUN', 'computer_graphics|NOUN', 'machine_vision|NOUN', 'screen_size|NOUN', 'instruction_sets|NOUN', 'computer_programming|NOUN', 'file_format|NOUN', 'main_processor|NOUN', 'raw_image|NOUN', 'texture_mapping|NOUN', 'simulation_software|NOUN', 'wireless_power|NOUN', 'augmented_reality|NOUN', 'computer_vision|NOUN', 'motion_detection|NOUN', 'electronic_viewfinder|NOUN', 'computer_language|NOUN', 'digital_signal_processing|NOUN', 'reference_material|NOUN', 'video_streaming|NOUN', 'handheld_devices|NOUN', 'visual_communication|NOUN', 'information_technology|NOUN', 'video_production|NOUN', 'product_designer|NOUN', 'genetic_algorithms|NOUN', 'user_testing|NOUN', 'high_frame_rate|NOUN', 'color_grading|NOUN', 'graphical_design|NOUN', 'color_profile|NOUN', 'memory_management|NOUN', 'modular_design|NOUN', 'logic_programming|NOUN', 'computer_program|NOUN', 'dynamic_range|NOUN', 'visual_perception|NOUN', 'computer_simulations|NOUN', 'classical_mechanics|NOUN', 'wireless_technology|NOUN', 'video_conferencing|NOUN', 'software_project|NOUN', 'machine_learning_algorithms|NOUN', 'wireless_networking|NOUN', 'color_space|NOUN', 'user_interfaces|NOUN', 'small_screen|NOUN', 'word_processing|NOUN', 'computer_power|NOUN', 'image_sensor|NOUN', 'sound_processing|NOUN', 'digital_media|NOUN', 'visual_media|NOUN', 'software_tool|NOUN', 'camera_tracking|NOUN', 'optical_system|NOUN', 'functional_analysis|NOUN', 'camera_sensors|NOUN', 'high_resolution|NOUN', 'eye_tracking|NOUN', 'programming_environment|NOUN', 'color_depth|NOUN', 'mechanical_design|NOUN', 'computer_modeling|NOUN', 'complex_analysis|NOUN', 'trigonometric_functions|NOUN', 'software_implementation|NOUN', 'projection_mapping|NOUN', 'inverse_kinematics|NOUN', 'video_encoding|NOUN', 'online_learning|NOUN', 'audio_processing|NOUN', 'digital_video|NOUN', 'computer_architecture|NOUN', 'data_processing|NOUN', 'processing_speed|NOUN', 'analog_electronics|NOUN', 'computing_power|NOUN', 'finite_element_analysis|NOUN', 'color_display|NOUN', 'computer_networking|NOUN', 'computational_power|NOUN', 'live_view|NOUN', 'mechanical_engineering|NOUN', 'software_development|NOUN', 'programming_language|NOUN', 'signal_processing|NOUN', 'computer_technology|NOUN', 'image_quality|NOUN', 'image_stabilization|NOUN', 'software_architecture|NOUN', 'high_dynamic_range|NOUN', 'raw_format|NOUN', 'behavior_analysis|NOUN', 'digital_processing|NOUN', 'error_correction|NOUN', 'computer_code|NOUN', 'virtual_environments|NOUN', 'digital_circuits|NOUN', 'high_performance_computing|NOUN', 'external_device|NOUN', 'software_stack|NOUN', 'big_data_analysis|NOUN', 'base_station|NOUN', 'color_management|NOUN', '3d_models|NOUN', 'real_camera|NOUN', 'software_level|NOUN', 'optical_image_stabilization|NOUN', 'control_theory|NOUN', 'file_management|NOUN', 'computer_programs|NOUN', 'image_editing|NOUN', 'unsupervised_learning|NOUN', 'video_processing|NOUN', 'computer_science|NOUN', 'camera_lenses|NOUN', 'computer_hardware|NOUN', 'haptic_feedback|NOUN', 'movie_making|NOUN', 'vector_images|NOUN', 'visual_effects|NOUN', 'video_recording|NOUN', 'rapid_prototyping|NOUN', 'picture_quality|NOUN', 'system_performance|NOUN', 'mobile_processor|NOUN', 'computer_scientists|NOUN', 'computer_languages|NOUN', 'positional_tracking|NOUN', 'distributed_computing|NOUN', 'image_processor|NOUN', 'sensor_size|NOUN', 'spatial_reasoning|NOUN', 'image_manipulation|NOUN', 'deep_learning|NOUN', 'image_compression|NOUN', 'natural_language_processing|NOUN', 'render_target|NOUN', 'parallel_processing|NOUN', 'optical_viewfinder|NOUN', 'differential_equations|NOUN', 'interface_design|NOUN', 'data_structures|NOUN', 'chromatic_aberration|NOUN', 'visual_feedback|NOUN', 'body_tracking|NOUN', 'mouse_input|NOUN', 'human-computer_interaction|NOUN', 'rendering_pipeline|NOUN', 'display_size|NOUN', 'dynamical_systems|NOUN', 'computational_complexity|NOUN', 'texture_maps|NOUN', 'vector_graphics|NOUN', 'light_meter|NOUN', 'reinforcement_learning|NOUN', 'edge_detection|NOUN', 'high_precision|NOUN', 'processing_power|NOUN', 'face_detection|NOUN', 'design_process|NOUN', 'digital_system|NOUN', 'analog_circuits|NOUN', 'software_engineering|NOUN', 'color_temperature|NOUN', 'software_design|NOUN', 'display_device|NOUN', 'user_interface_design|NOUN', 'manual_focus|NOUN', 'input_device|NOUN', 'contrast_ratio|NOUN', 'artificial_intelligence|NOUN', 'embedded_devices|NOUN', 'civil_engineering|NOUN', 'color_calibration|NOUN', 'portable_devices|NOUN', 'software_suite|NOUN', 'dynamic_contrast|NOUN', 'global_shutter|NOUN', 'computational_methods|NOUN', 'neural_networks|NOUN', 'camera_system|NOUN', 'batch_processing|NOUN', 'visual_design|NOUN', 'wireless_communication|NOUN', 'image_recognition|NOUN', 'computer_software|NOUN', 'face_tracking|NOUN', 'game_engine|NOUN', 'process_control|NOUN', 'state_machines|NOUN', 'color_image|NOUN', 'image_analysis|NOUN', 'depth_map|NOUN', 'interior_design|NOUN', 'embedded_hardware|NOUN', 'colour_correction|NOUN', 'virtual_reality|NOUN', 'deferred_rendering|NOUN', 'parallel_computing|NOUN', 'assembly_language|NOUN', 'physical_design|NOUN', 'digital_systems|NOUN', 'audio_engineering|NOUN', 'video_stabilization|NOUN', 'numerical_analysis|NOUN', 'fluid_dynamics|NOUN', 'open_standard|NOUN', 'mathematical_modeling|NOUN', 'web_browsing|NOUN', 'motion_picture|NOUN', 'still_life|NOUN', 'processor_speed|NOUN', 'motion_pictures|NOUN', 'pixel_density|NOUN', 'data_manipulation|NOUN', 'human_vision|NOUN', 'media_players|NOUN']\n","\n","✨  Starting the web server at http://127.0.0.1:8000 ...\n","Open the app in your browser and start annotating!\n","\n","\n","\u001b[38;5;2m✔ Saved 324 annotations to database SQLite\u001b[0m\n","Dataset: termsg06t\n","Session ID: 2021-02-26_09-27-30\n","\n","^C\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G3dq4uRQxFaQ","executionInfo":{"status":"ok","timestamp":1614332625795,"user_tz":-60,"elapsed":4901,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"e9132336-93fc-44a3-c965-329a99b16130"},"source":["!prodigy sense2vec.to-patterns termsg06t en_core_web_sm TECH --output-file termsg06tpatterns.jsonl"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n","  \"\"\")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"xr8fnMrfcejQ","executionInfo":{"status":"ok","timestamp":1614332625797,"user_tz":-60,"elapsed":3553,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"9bb649d4-f0d7-4ae9-982c-07feff5f5cba"},"source":["# shutil.copy2('termsg06tpatterns.jsonl', basedir)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/MyDrive/Education/Master Informatique IA Computer Science AI Paris Saclay/AI/TC3 Information Retrieval/inforet7/termsg06tpatterns.jsonl'"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"markdown","metadata":{"id":"VsozamCIypCv"},"source":["#Annotate\n","https://prodi.gy/docs/named-entity-recognition"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YRLG7DT1yk_S","executionInfo":{"status":"ok","timestamp":1615218352986,"user_tz":-60,"elapsed":4660,"user":{"displayName":"MD Salem Messoud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBYoVQGAImGxJfhgMMK1daBThGNpiaASkBXr4EYpE=s64","userId":"15534871031717137879"}},"outputId":"92b39e54-3f41-48c4-d024-74ca354bc968"},"source":["import re\n","fig = re.compile(r'(figs?)\\.',re.I)\n","g06tpatents = [fig.sub(r'\\1',pat) for pat in gzip.open('G06T.txt.gz', 'r').read().decode().split('\\n\\n\\n')]\n","\n","# txt=''\n","# with gzip.open('G06T.txt.gz', 'r') as file:\n","#   txt = file.read().decode()).split('\\n\\n\\n')\n","  \n","# g06tpatents = [fig.sub(r'\\1',pat) for pat in txt]\n","print(len(g06tpatents))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uNuErJQRzIu0","executionInfo":{"status":"ok","timestamp":1615218367158,"user_tz":-60,"elapsed":712,"user":{"displayName":"MD Salem Messoud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBYoVQGAImGxJfhgMMK1daBThGNpiaASkBXr4EYpE=s64","userId":"15534871031717137879"}},"outputId":"e1d4235c-2da7-492d-c66c-ae304285289e"},"source":["# import random\n","sentsplit = re.compile('[\\n.;]')\n","\n","# Select only first 12 patents\n","g06tsents = [li.strip() for pat in g06tpatents[:12] for li in sentsplit.split(pat) if len(li)>25 and '____' not in li]\n","random.shuffle(g06tsents)\n","print(len(g06tsents))\n","open('g06tsents-3000.txt','w').write('\\n'.join(g06tsents))\n","g06tsents[:5]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["3923\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['determining, from the calculated colour variances, one or more voxels of the shape volume that are not colour consistent',\n"," 'For example, the intersection identification program 428 might use the last available timestamped location from the navigation system (not shown) of the computing device 400',\n"," 'Step (e) of estimating the three-dimensional shape volume may further comprise:',\n"," 'The local contrast value determinator 114 is configured to determine a local contrast value for the digital image',\n"," 'The method comprises identifying S1 regions of a first color 105 and regions of a second color 106 in the image content 101, determining S2 the luminance values of the pixels in the regions of the first color 105 and the regions of the second color 106, and darkening S3 the regions of the first color 105 and brightening S4 the regions of the second color 106, or brightening the regions of the first color 105 and darkening the regions of the second color 106 based on a difference of the luminance values of the regions of the first color 105 and the luminance values of the regions of the second color 106']"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rq5wZyq53bo-","executionInfo":{"status":"ok","timestamp":1614332689419,"user_tz":-60,"elapsed":2008,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"ccbf072c-04ab-48ed-d0a3-e782d8e2b1f5"},"source":["ngrok.kill()\n","public_url = ngrok.connect(addr='8000', proto=\"http\", options={\"bind_tls\": True})\n","print(\"Tracking URL:\", public_url)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tracking URL: NgrokTunnel: \"http://455aee81eb8b.ngrok.io\" -> \"http://localhost:8000\"\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TbBvIzTPw_pu"},"source":["Annotate manually, with help from terminology list, i.e. pattern"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u4XADnUU3Ivz","executionInfo":{"status":"ok","timestamp":1614333954907,"user_tz":-60,"elapsed":1264187,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"ec048b65-cbd1-4f55-8758-9d6fa9c53179"},"source":["! PRODIGY_PORT=8000 PRODIGY_HOST=127.0.0.1  prodigy ner.manual annotatedg06t en_core_web_sm g06tsents-3000.txt --loader txt --patterns termsg06tpatterns.jsonl --label TECH\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n","  \"\"\")\n","Using 1 label(s): TECH\n","Added dataset annotatedg06t to database SQLite.\n","\n","✨  Starting the web server at http://127.0.0.1:8000 ...\n","Open the app in your browser and start annotating!\n","\n","\n","\u001b[38;5;2m✔ Saved 109 annotations to database SQLite\u001b[0m\n","Dataset: annotatedg06t\n","Session ID: 2021-02-26_09-44-52\n","\n","^C\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VFAA-Hb43g26","executionInfo":{"status":"ok","timestamp":1614333958138,"user_tz":-60,"elapsed":1771,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"c3b4ad4c-141f-454b-ae7a-7f61f65456ce"},"source":["! prodigy db-out annotatedg06t > annotatedg06t.jsonl"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n","  \"\"\")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"v27_d6SYcm-E","executionInfo":{"status":"ok","timestamp":1614333959123,"user_tz":-60,"elapsed":2352,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"9af1ff38-43c9-4425-ae37-2f98fc0d16bb"},"source":["# shutil.copy2('annotatedg06t.jsonl', basedir)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/MyDrive/Education/Master Informatique IA Computer Science AI Paris Saclay/AI/TC3 Information Retrieval/inforet7/annotatedg06t.jsonl'"]},"metadata":{"tags":[]},"execution_count":70}]},{"cell_type":"markdown","metadata":{"id":"Czdbz4hfxc4f"},"source":["Train NER component with newly anotated terminology list \\\n","https://prodi.gy/docs/recipes#training"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WwiuhYaih5u-","executionInfo":{"status":"ok","timestamp":1614334081261,"user_tz":-60,"elapsed":116035,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"a86faf09-0f5b-4004-f7b2-71204c4e7cba"},"source":["!prodigy train ner annotatedg06t en_vectors_web_lg --init-tok2vec ./tok2vec_cd8_model289.bin --output ./tmp_model --eval-split 0.2\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n","  \"\"\")\n","⠴ Loading 'en_vectors_web_lg'...tcmalloc: large alloc 1285169152 bytes == 0x558450002000 @  0x7f6c382c01e7 0x7f6c34d7546e 0x7f6c34dc9e7c 0x7f6c34dcaaaf 0x7f6c34e6c470 0x55843d4290e4 0x55843d428de0 0x55843d49d6f5 0x55843d497b0e 0x55843d42a77a 0x55843d49986a 0x55843d497b0e 0x55843d4a09fc 0x7f6c241ab7da 0x7f6c241b480a 0x55843d4297f2 0x55843d49cd75 0x7f6c241a94c8 0x7f6c241ab89a 0x7f6c241bb57d 0x55843d36a337 0x7f6c244bfbe0 0x55843d429050 0x55843d51a99d 0x55843d49cfe9 0x55843d497e0d 0x55843d42a77a 0x55843d498a45 0x55843d42a69a 0x55843d49ce50 0x55843d497e0d\n","\u001b[2K\u001b[38;5;2m✔ Loaded model 'en_vectors_web_lg'\u001b[0m\n","Created and merged data for 106 total examples\n","Using 85 train / 21 eval (split 20%)\n","Component: ner | Batch size: compounding | Dropout: 0.2 | Iterations: 10\n","\u001b[38;5;2m✔ Initializing with tok2vec weights ./tok2vec_cd8_model289.bin\u001b[0m\n","embed_rows: 10000 | require_vectors: True | cnn_maxout_pieces: 3 |\n","token_vector_width: 128 | conv_depth: 8 | nr_feature_tokens: 3 |\n","pretrained_vectors: en_vectors_web_lg.vectors | pretrained_dims: 300\n","\u001b[38;5;4mℹ Baseline accuracy: 0.000\u001b[0m\n","\u001b[1m\n","=========================== ✨  Training the model ===========================\u001b[0m\n","\n","#    Loss       Precision   Recall     F-Score \n","--   --------   ---------   --------   --------\n"," 1     290.08       0.000      0.000      0.000\n"," 2     171.34      20.000     10.526     13.793\n"," 3      93.36      50.000     31.579     38.710\n"," 4      77.86      53.333     42.105     47.059\n"," 5      40.56      66.667     42.105     51.613\n"," 6      34.41      76.923     52.632     62.500\n"," 7      15.18      57.143     42.105     48.485\n"," 8       8.35      56.250     47.368     51.429\n"," 9       4.21      58.824     52.632     55.556\n","10       0.63      62.500     52.632     57.143\n","\u001b[1m\n","============================= ✨  Results summary =============================\u001b[0m\n","\n","Label   Precision   Recall   F-Score\n","-----   ---------   ------   -------\n","TECH       76.923   52.632    62.500\n","\n","\n","Best F-Score   \u001b[38;5;2m62.500\u001b[0m\n","Baseline       0.000              \n","\n","\u001b[38;5;2m✔ Saved model: /content/tmp_model\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TdfNIAnHx2RX"},"source":["How useful it is to collect more data ? \\\n","https://prodi.gy/docs/recipes#train-curve"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t0EENaGuiaez","executionInfo":{"status":"ok","timestamp":1614334294703,"user_tz":-60,"elapsed":325458,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"215625a3-f374-43ed-aab8-a16c27f09ede"},"source":["!prodigy train-curve ner annotatedg06t en_vectors_web_lg --init-tok2vec ./tok2vec_cd8_model289.bin  --eval-split 0.2\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n","  \"\"\")\n","\u001b[38;5;2m✔ Starting with model 'en_vectors_web_lg'\u001b[0m\n","Training 4 times with 25%, 50%, 75%, 100% of the data\n","\u001b[1m\n","=============================== ✨  Train curve ===============================\u001b[0m\n","%      Accuracy   Difference\n","----   --------   ----------\n","tcmalloc: large alloc 1285169152 bytes == 0x558283b4e000 @  0x7f3b7fc481e7 0x7f3b7c6fd46e 0x7f3b7c751e7c 0x7f3b7c752aaf 0x7f3b7c7f4470 0x558270bd90e4 0x558270bd8de0 0x558270c4d6f5 0x558270c47b0e 0x558270bda77a 0x558270c4986a 0x558270c47b0e 0x558270c509fc 0x7f3b6bb337da 0x7f3b6bb3c80a 0x558270bd97f2 0x558270c4cd75 0x7f3b6bb314c8 0x7f3b6bb3389a 0x7f3b6bb4357d 0x558270b1a337 0x7f3b6be47be0 0x558270bd9050 0x558270cca99d 0x558270c4cfe9 0x558270c47e0d 0x558270bda77a 0x558270c48a45 0x558270bda69a 0x558270c4ce50 0x558270c47e0d\n","  0%       0.00   \u001b[38;5;250mbaseline\u001b[0m\n"," 25%      35.71   \u001b[38;5;2m+35.71\u001b[0m\n","tcmalloc: large alloc 1285169152 bytes == 0x5582d9c28000 @  0x7f3b7fc481e7 0x7f3b7c6fd46e 0x7f3b7c751e7c 0x7f3b7c752aaf 0x7f3b7c7f4470 0x558270bd90e4 0x558270bd8de0 0x558270c4d6f5 0x558270c47b0e 0x558270bda77a 0x558270c4986a 0x558270c47b0e 0x558270c509fc 0x7f3b6bb337da 0x7f3b6bb3c80a 0x558270bd97f2 0x558270c4cd75 0x7f3b6bb314c8 0x7f3b6bb3389a 0x7f3b6bb4357d 0x558270b1a337 0x7f3b6be47be0 0x558270bd9050 0x558270cca99d 0x558270c4cfe9 0x558270c47e0d 0x558270bda77a 0x558270c48a45 0x558270bda69a 0x558270c4ce50 0x558270c47e0d\n"," 50%      37.50   \u001b[38;5;2m+1.79\u001b[0m\n"," 75%      58.82   \u001b[38;5;2m+21.32\u001b[0m\n","100%      62.50   \u001b[38;5;2m+3.68\u001b[0m\n","\n","\u001b[38;5;2m✔ Accuracy improved in the last sample\u001b[0m\n","As a rule of thumb, if accuracy increases in the last segment, this could\n","indicate that collecting more annotations of the same type will improve the\n","model further.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"o1RhNXyRyNOv"},"source":["Correct the model \\\n","https://prodi.gy/docs/recipes#ner-correct"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5gihRNvf02En","executionInfo":{"status":"ok","timestamp":1614334362392,"user_tz":-60,"elapsed":2322,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"c00afec4-ad97-4f3c-f8aa-dd128106b104"},"source":["ngrok.kill()\n","public_url = ngrok.connect(addr='8000', proto=\"http\", options={\"bind_tls\": True})\n","print(\"Tracking URL:\", public_url)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tracking URL: NgrokTunnel: \"http://6d0dac50f11f.ngrok.io\" -> \"http://localhost:8000\"\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"If2lAhVHixax","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614334711773,"user_tz":-60,"elapsed":293714,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"0c2369ef-4fb2-4244-f5b9-b9cd21b94c59"},"source":["! PRODIGY_PORT=8000 PRODIGY_HOST=127.0.0.1 prodigy ner.correct annotatedg06t_correct ./tmp_model g06tsents-3000.txt --loader txt --label TECH --exclude annotatedg06t"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n","  \"\"\")\n","Using 1 label(s): TECH\n"],"name":"stdout"},{"output_type":"stream","text":["t=2021-02-26T10:13:41+0000 lvl=warn msg=\"failed to open private leg\" id=926ad1722a1b privaddr=localhost:8000 err=\"dial tcp 127.0.0.1:8000: connect: connection refused\"\n","t=2021-02-26T10:13:42+0000 lvl=warn msg=\"failed to open private leg\" id=ed2b23c38e96 privaddr=localhost:8000 err=\"dial tcp 127.0.0.1:8000: connect: connection refused\"\n"],"name":"stderr"},{"output_type":"stream","text":["tcmalloc: large alloc 1285169152 bytes == 0x565399c86000 @  0x7f13536251e7 0x7f13500da46e 0x7f135012ee7c 0x7f135012faaf 0x7f13501d1470 0x5653886770e4 0x565388676de0 0x5653886eb6f5 0x5653886e5b0e 0x56538867877a 0x5653886e786a 0x5653886e5b0e 0x5653886ee9fc 0x7f133f5107da 0x7f133f51980a 0x5653886777f2 0x5653886ead75 0x7f133f50e4c8 0x7f133f51089a 0x7f133f52057d 0x5653885b8337 0x7f133f824be0 0x565388677050 0x56538876899d 0x5653886eafe9 0x5653886e5e0d 0x56538867877a 0x5653886e6a45 0x56538867869a 0x5653886eae50 0x5653886e5e0d\n","\u001b[38;5;3m⚠ The model you're using isn't setting sentence boundaries (e.g. via\n","the parser or sentencizer). This means that incoming examples won't be split\n","into sentences.\u001b[0m\n","\n","✨  Starting the web server at http://127.0.0.1:8000 ...\n","Open the app in your browser and start annotating!\n","\n","\n","\u001b[38;5;2m✔ Saved 26 annotations to database SQLite\u001b[0m\n","Dataset: annotatedg06t_correct\n","Session ID: 2021-02-26_10-13-45\n","\n","^C\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"54Z-3g8E0VEV","executionInfo":{"status":"ok","timestamp":1614334715947,"user_tz":-60,"elapsed":1800,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"ac3b9b99-f098-4c6c-b048-eca0c6e289db"},"source":["! prodigy db-out annotatedg06t_correct > annotatedg06t_correct.jsonl"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n","  \"\"\")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"CzkF1b5E0aXa","executionInfo":{"status":"ok","timestamp":1614334719717,"user_tz":-60,"elapsed":920,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"b35cb30f-4c07-4793-e899-fc4eceff7bfe"},"source":["# shutil.copy2('annotatedg06t_correct.jsonl', basedir)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/MyDrive/Education/Master Informatique IA Computer Science AI Paris Saclay/AI/TC3 Information Retrieval/inforet7/annotatedg06t_correct.jsonl'"]},"metadata":{"tags":[]},"execution_count":78}]},{"cell_type":"markdown","metadata":{"id":"cTsN4IsGyeo6"},"source":["Retrain with correction and original terminology list"]},{"cell_type":"code","metadata":{"id":"CS339-xEjC0M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614334829126,"user_tz":-60,"elapsed":104830,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"1b6fb3e8-f98b-4a3f-c893-0a3f4b5c4f85"},"source":["!prodigy train ner annotatedg06t,annotatedg06t_correct en_vectors_web_lg --init-tok2vec ./tok2vec_cd8_model289.bin --output ./tech_model --eval-split 0.2 --n-iter 10\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n","  \"\"\")\n","⠙ Loading 'en_vectors_web_lg'...tcmalloc: large alloc 1285169152 bytes == 0x5585d8290000 @  0x7fc5bc3dd1e7 0x7fc5b8e9246e 0x7fc5b8ee6e7c 0x7fc5b8ee7aaf 0x7fc5b8f89470 0x5585c71260e4 0x5585c7125de0 0x5585c719a6f5 0x5585c7194b0e 0x5585c712777a 0x5585c719686a 0x5585c7194b0e 0x5585c719d9fc 0x7fc5a82c87da 0x7fc5a82d180a 0x5585c71267f2 0x5585c7199d75 0x7fc5a82c64c8 0x7fc5a82c889a 0x7fc5a82d857d 0x5585c7067337 0x7fc5a85dcbe0 0x5585c7126050 0x5585c721799d 0x5585c7199fe9 0x5585c7194e0d 0x5585c712777a 0x5585c7195a45 0x5585c712769a 0x5585c7199e50 0x5585c7194e0d\n","\u001b[2K\u001b[38;5;2m✔ Loaded model 'en_vectors_web_lg'\u001b[0m\n","Created and merged data for 132 total examples\n","Using 106 train / 26 eval (split 20%)\n","Component: ner | Batch size: compounding | Dropout: 0.2 | Iterations: 10\n","\u001b[38;5;2m✔ Initializing with tok2vec weights ./tok2vec_cd8_model289.bin\u001b[0m\n","embed_rows: 10000 | require_vectors: True | cnn_maxout_pieces: 3 |\n","token_vector_width: 128 | conv_depth: 8 | nr_feature_tokens: 3 |\n","pretrained_vectors: en_vectors_web_lg.vectors | pretrained_dims: 300\n","\u001b[38;5;4mℹ Baseline accuracy: 0.000\u001b[0m\n","\u001b[1m\n","=========================== ✨  Training the model ===========================\u001b[0m\n","\n","#    Loss       Precision   Recall     F-Score \n","--   --------   ---------   --------   --------\n"," 1     375.43       0.000      0.000      0.000\n"," 2     137.12      39.130     36.000     37.500\n"," 3      94.70      68.182     60.000     63.830\n"," 4      67.49      63.636     56.000     59.574\n"," 5      41.43      68.182     60.000     63.830\n"," 6      35.02      73.684     56.000     63.636\n"," 7      17.70      73.684     56.000     63.636\n"," 8      15.91      73.684     56.000     63.636\n"," 9      15.76      60.870     56.000     58.333\n","10       4.45      58.333     56.000     57.143\n","\u001b[1m\n","============================= ✨  Results summary =============================\u001b[0m\n","\n","Label   Precision   Recall   F-Score\n","-----   ---------   ------   -------\n","TECH       68.182   60.000    63.830\n","\n","\n","Best F-Score   \u001b[38;5;2m63.830\u001b[0m\n","Baseline       0.000              \n","\n","\u001b[38;5;2m✔ Saved model: /content/tech_model\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"TumFogLA2fu_","executionInfo":{"status":"ok","timestamp":1614335587080,"user_tz":-60,"elapsed":52101,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"eac92732-9009-4953-b38e-66ff2cd2e8e9"},"source":["modelzipfile = shutil.make_archive('tech_model', 'zip', 'tech_model')\n","# shutil.copy2(modelzipfile, basedir)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/MyDrive/Education/Master Informatique IA Computer Science AI Paris Saclay/AI/TC3 Information Retrieval/inforet7/tech_model.zip'"]},"metadata":{"tags":[]},"execution_count":82}]},{"cell_type":"code","metadata":{"id":"zEzh9wFwjGlK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614335917991,"user_tz":-60,"elapsed":254067,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"843f45e4-417f-4b60-a581-71d58948921b"},"source":["!prodigy train-curve ner annotatedg06t,annotatedg06t_correct en_vectors_web_lg --init-tok2vec ./tok2vec_cd8_model289.bin  --eval-split 0.2 \n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n","  \"\"\")\n","\u001b[38;5;2m✔ Starting with model 'en_vectors_web_lg'\u001b[0m\n","Training 4 times with 25%, 50%, 75%, 100% of the data\n","\u001b[1m\n","=============================== ✨  Train curve ===============================\u001b[0m\n","%      Accuracy   Difference\n","----   --------   ----------\n","tcmalloc: large alloc 1285169152 bytes == 0x559ccc800000 @  0x7f6994b9d1e7 0x7f699165246e 0x7f69916a6e7c 0x7f69916a7aaf 0x7f6991749470 0x559cbb3050e4 0x559cbb304de0 0x559cbb3796f5 0x559cbb373b0e 0x559cbb30677a 0x559cbb37586a 0x559cbb373b0e 0x559cbb37c9fc 0x7f6980a887da 0x7f6980a9180a 0x559cbb3057f2 0x559cbb378d75 0x7f6980a864c8 0x7f6980a8889a 0x7f6980a9857d 0x559cbb246337 0x7f6980d9cbe0 0x559cbb305050 0x559cbb3f699d 0x559cbb378fe9 0x559cbb373e0d 0x559cbb30677a 0x559cbb374a45 0x559cbb30669a 0x559cbb378e50 0x559cbb373e0d\n","  0%       0.00   \u001b[38;5;250mbaseline\u001b[0m\n"," 25%      41.03   \u001b[38;5;2m+41.03\u001b[0m\n","tcmalloc: large alloc 1285169152 bytes == 0x559d22618000 @  0x7f6994b9d1e7 0x7f699165246e 0x7f69916a6e7c 0x7f69916a7aaf 0x7f6991749470 0x559cbb3050e4 0x559cbb304de0 0x559cbb3796f5 0x559cbb373b0e 0x559cbb30677a 0x559cbb37586a 0x559cbb373b0e 0x559cbb37c9fc 0x7f6980a887da 0x7f6980a9180a 0x559cbb3057f2 0x559cbb378d75 0x7f6980a864c8 0x7f6980a8889a 0x7f6980a9857d 0x559cbb246337 0x7f6980d9cbe0 0x559cbb305050 0x559cbb3f699d 0x559cbb378fe9 0x559cbb373e0d 0x559cbb30677a 0x559cbb374a45 0x559cbb30669a 0x559cbb378e50 0x559cbb373e0d\n"," 50%      72.34   \u001b[38;5;2m+31.31\u001b[0m\n"," 75%      68.09   \u001b[38;5;1m-4.26\u001b[0m\n","100%      63.83   \u001b[38;5;1m-4.26\u001b[0m\n","\n","\u001b[38;5;1m✘ Accuracy decreased in the last sample\u001b[0m\n","As a rule of thumb, if accuracy increases in the last segment, this could\n","indicate that collecting more annotations of the same type will improve the\n","model further.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"07cjq16qxnRT"},"source":["It seems like annotating more examples would bring little benefit."]},{"cell_type":"markdown","metadata":{"id":"BDOx0thbwwCv"},"source":["## Display example of NER on a patent"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f9pgE1EA2UZM","executionInfo":{"status":"ok","timestamp":1614336042286,"user_tz":-60,"elapsed":941,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"711bf318-892b-48ca-a3be-c7b6c26086dd"},"source":["print(g06tpatents[11][:1111])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["METHOD AND APPARATUS FOR GENERATING A THREE-DIMENSIONAL MODEL\n","_____2019_____3503030_____488163391_____EP3500000.txt_____G06T_____G06T2207/20044:G06T2207/30196:G06T7/564:G06T7/593\n","The method comprising providing a plurality of images of a scene captured by a plurality of image capturing devices (101); providing silhouette information of at least one object in the scene (102); generating a point cloud for the scene in 3D space using the plurality of images (103); extracting an object point cloud from the generated point cloud, the object point cloud being a point cloud associated with the at least one object in the scene (104); estimating a 3D shape volume of the at least one object from the silhouette information (105); and combining the object point cloud and the shape volume of the at least one object to generate a three-dimensional model (106).\n","An apparatus for generating a 3D model, and a computer readable medium for generating the 3D model.\n","_____d:\n","The present invention relates to a method and apparatus for generating a three-dimensional model. In particular the present invention is concern\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ct5Q5j6r7UAb"},"source":["SPACY_MODEL = \"./tech_model\"      # path to spaCy model with entity recognizer\n","\n","nlp = spacy.load(SPACY_MODEL)\n","doc = nlp(g06tpatents[11])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"VvxkcFqa7b0c","executionInfo":{"status":"ok","timestamp":1614336133814,"user_tz":-60,"elapsed":1116,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"171d8ea2-35b9-42b8-e5c0-6335692a313a"},"source":["# colors = {\"TECH\": \"linear-gradient(90deg, #fc9ce7, blue)\"}\n","# options = {\"ents\": [\"TECH\"], \"colors\": colors}\n","options = {\"ents\": [\"TECH\"]}\n","displacy.render(doc, style=\"ent\", options=options, jupyter=True)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">METHOD AND APPARATUS FOR GENERATING A \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    THREE-DIMENSIONAL MODEL\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n","</br>_____2019_____3503030_____488163391_____EP3500000.txt_____G06T_____G06T2207/20044:G06T2207/30196:G06T7/564:G06T7/593</br>The method comprising providing a plurality of images of a scene captured by a plurality of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices (101); providing \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of at least one object in the scene (102); generating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," for the scene in \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D space\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," using the plurality of images (103); extracting an object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," from the generated \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," being a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," associated with the at least one object in the scene (104); estimating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D shape\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," volume of the at least one object from the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," (105); and combining the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," and the shape volume of the at least one object to generate a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," (106).</br>An apparatus for generating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", and a computer readable medium for generating the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",".</br>_____d:</br>The present invention relates to a method and apparatus for generating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",". In particular the present invention is concerned with generating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of an object in a scene captured by a plurality of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices.</br>Background</br>Emerging types of media support the ability to individually control the viewer's perspective, which can provide a more immersive and more personalized viewing experience. Furthermore, applications of virtual, augmented, and mixed reality (VR/AR/MR) are becoming widely available and used. There is an ever increasing demand for compelling content.</br>Most available \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    VR/\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n","\n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    AR/\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n","\n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    MR content\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," is synthetic (graphics), created by artists and designers. Available real-world content (live action) is mostly limited to \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    360-degree video\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," captured using omnidirectional camera rigs. However, these rigs only provide a 3 degrees of freedom immersive experience, as only rotation is supported for the viewer.</br>To provide an \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    immersive video\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," viewing experience and/or \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    VR/\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n","\n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    AR/\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n","\n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    MR content\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," it is desirable to generate Free-Viewpoint Video (FVV). FVV allows a user to freely, or with some restrictions, navigate a recorded scene and select a certain viewpoint at any moment, within a given range. FVV therefore provides a 6 degrees of freedom immersive experience.</br>Current FVV techniques for generating \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    FVV visual information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," from real-world content include image-based techniques, where the intermediate views between cameras are generated using interpolation or warping of the available images; and geometry-based techniques, where \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D geometry\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of both the dynamic foreground and \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    static background\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," is acquired, allowing the rendering from any other viewpoint.</br>An existing \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image-\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n","based FVV technique is to generate multiple depth maps using a multi-camera setup, allowing the user to experience new rendered views in between cameras and in a limited area around them.</br>Another \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image-\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n","based approach uses an array of standard consumer cameras to generate a FVV scene. The \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    2D matrix\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of cameras is extended to a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D one\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," by adding a temporal component, creating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D grid\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," that is triangulated, and which defines different paths the virtual camera can take across the FVV sequence. The rendered view is a weighted warp of these possibilities. The resulting FVV is of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    high visual quality\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," but if the number of cameras is limited, the possible navigation range is also very limited.</br>Another \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image based\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    FFV technique\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," is used in sports broadcasting. In this approach, a scene is captured using professional high-end cameras. Foreground masks and \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    camera pose\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," information are recovered at every frame, and a simple model of the background is reconstructed to improve the visual experience in camera transitions. This approach requires the transitions to be pre-computed offline, and does not allow freeze frames during the camera transitions (static FVV).</br>Geometry-based techniques focus on acquiring the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D geometry\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of a scene. This can be seen as a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D reconstruction\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," problem that is extended to the temporal dimension.</br>Space carving, otherwise known as shape-from-silhouette (SfS) is an existing approach for generating the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D geometry\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of objects in a scene. Multi-view stereo (MVS) techniques have also been used for the reconstruction of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D models\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," and especially for dynamic \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D scenes\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",". Another existing approach uses, in a studio setup, both \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    RGB cameras\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," and infrared structured light to generate dense \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point clouds\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," using MVS. These \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point clouds\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," are meshed using a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette constrained\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," Poisson surface reconstruction.</br>In between the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image-\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n","based and geometry-based techniques for FVV content creation, there are existing hybrid techniques that make use of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D geometry\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," estimation to improve the synthesis of new views. One such existing approach uses MVS to generate a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the scene, helping their \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image warping\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," system to perform better.</br>While FVV is an established technique, there are limited available options for generating real-world content using this approach. Many of these existing approaches as outlined above require complicated and expensive camera setups, cameras with \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    IR sensing functionalities\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", and professional studios. These existing approaches also require significant computing resources, and may suffer from accuracy issues in certain situations.</br>It is an objective of the present invention to improve on, or at least provide an alternative to, existing methods and apparatuses for generating content based on captured images of a scene, and a particular objective is to improve on, or at least provide an alternative to existing methods and apparatuses that use geometry-based techniques to generate a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of an object in the scene.</br>Summary</br>According to the present invention there is provided a method and apparatus as set forth in the appended claims. Other features of the invention will be apparent from the dependent claims, and the description which follows.</br>According to a first aspect of the invention, there is provided a method for generating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", the method comprising the following steps:</br>(a) providing a plurality of images of a scene captured by a plurality of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices;(b) providing \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of at least one object in the scene, the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," being extracted from the plurality of images;(c) generating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," for the scene in three-dimensional space using the plurality of images;(d) extracting an object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," from the generated \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," being a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," associated with the at least one object in the scene;(e) estimating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional shape volume\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the at least one object from the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n","; and(f) combining the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," and the shape volume of the at least one object to generate the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",".</br>The method according to the present invention is able to use a plurality of images of a scene captured by a plurality of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices to generate a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",". In other words, the method is able to generate a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of an object in the scene, or all or part of the scene including the object using the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    captured image\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," data. The object may be considered as the foreground of the scene.</br>The \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," may be for use in \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    video content\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," creation, and in particular Free-Viewpoint Video (FVV) content creation. The \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," may additionally or separately be used for the creation of Augmented Reality (AR) and/or Virtual Reality (VR) and/or other Mixed Reality (MR) content.</br>The method obtains an object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," associated with at least one object in the scene, and a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional shape volume\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the at least one object. The object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," and the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional shape volume\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," are obtained from the captured images. Significantly, the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," and the shape volume are combined to generate the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",".</br>The combination of the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," and the shape volume is beneficial. This is particularly the case where the plurality of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices are arranged in a sparse setup. A sparse setup may mean that only a small number of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices are provided to capture the scene.</br>The small number of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices may be closely arranged together, meaning that there is overlapping \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image data\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," for a small section of the scene but patchy data for the rest of the scene. In such a sparse setup, if the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," were used, by itself, to generate the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," then the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," would not be able to accurately recreate the object in the scene. There would likely be significant occlusions, which would likely result in significantly inflated volumes for the three-dimensional model meaning that the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," does not accurately reflect the object in the scene.</br>The small number of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices may be spread out, meaning that while the overall scene is captured, there is little overlapping \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image data\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",". In such a sparse setup, an object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," technique will likely be unable to compute a dense \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," covering the object's surface sufficiently. This means that an object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," technique by itself will be unlikely able to generate a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",".</br>Significantly, by combing the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," and the shape volume, the method of the present invention is able generate three-dimensional models that accurately reflect the object in the scene even in sparse \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," device setups. The method of the present invention therefore enables the benefits of the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," and the shape volume techniques to be achieved without the associated disadvantages. The generated \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," is able to preserve the detail of the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," and the completeness of the estimated \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional shape volume\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," while avoiding inflation of the three-dimensional model.</br>The method is able to generate accurate \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional models\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the object using \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    low cost image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices. This includes RBG cameras without using \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    IR sensing functionality\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", and especially handheld cameras such as mobile phones. The method is also able to be used with informal networks of cameras, such as multiple people catching a scene on their mobile phones.</br>The generated \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," may be static. In preferred applications, such as free-viewpoint video, the generated \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," is preferably dynamic.</br>The \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices may each \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    capture video\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," data comprising a plurality of temporally separated images, here each image as captured by an \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," device represents a frame of the video. The method may comprise generating \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional models\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the object for the different frames of video, such that a dynamic three-dimensional model of the object is generated.</br>Step (e) of estimating the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional shape volume\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," may further comprise:</br>performing a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel colour\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," consistency check on the shape volume estimated from the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," so as to determine one or more voxels of the shape volume that are not colour consistent.</br>Estimating the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional shape volume\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," from the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," itself may result in an occluded \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional shape\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," which does not accurately represent the object. This is because the use of the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", by itself, may not be able to handle concavities in the object, and such effects may be magnified in sparse \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," device setups.</br>Significantly, performing a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel colour\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," consistency check on the shape volume may be used to determine voxels of the shape volume that are not colour consistent. Any voxels determined to not be colour consistent may be voxels representing \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image data\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," that does not belong to the object. These \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour inconsistent\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," voxels may be removed from the shape volume. This has the benefit of removing voxels that do not represent the object, and means that the resultant \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," may be a more accurate reflection of the object in the scene.</br>This approach to carving out the shape volume using the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel colour\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," consistency check is ideally suited for smoothing sharp edges that might appear in sparse \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," device setups. The \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel colour consistency\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check also enables the method to be more robust to noisy object/foreground masks that might include part of the background in the shape volume.</br>Step (e) may further comprise removing voxels from the shape volume that are determined not to be colour consistent.</br>Performing the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel colour\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," consistency check on the shape volume may comprise:</br>projecting voxels of the shape volume estimated from the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," onto the plurality of images to generate a plurality of projections for each voxel projected onto the plurality of images; and optionallycalculating a difference in colour across the plurality of projections for each voxel projected onto the plurality of images; and optionallydetermining, from the calculated colour differences, one or more voxels of the shape volume that are not colour consistent.</br> EPO &lt;DP n=&quot;6&quot;&gt; </br>Calculating the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour difference\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," may comprise calculating the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour variance\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," across the plurality of projections.</br>The shape volume comprises a set of voxels. The present method may project some or all of these voxels onto the plurality of images. If the voxel is part of the object, then the resultant projections should be colour consistent across the plurality of images because they all represent the same point of the object across the plurality of images. If, however, the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel is\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," not part of the object, then the resultant projections will be expected to be colour inconsistent because they may represent different parts of the scene across the plurality of images.</br>The calculated \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour variance\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," may be the variance in the colour hue across the plurality of projections for each voxel projected onto the plurality of images.</br>In existing \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image-\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n","based \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    photo hulls\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," approaches, \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    RGB colour\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," channels or the CIELAB colour space have been used to determine whether a voxel is colour consistent. By contrast, the method of the present invention may use the variance in the colour hue, e.g. the variance measured on the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    hue channel\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    HSV colour\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," space. The use of the colour hue is beneficial as it enables the present method to accept less relevant differences in saturation and value that may remain in images, e.g. after colour correction. Such differences may normally be due to differences in the sensor or white balance of the plurality of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices. The method of the present invention is therefore able to use the difference in colour \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    hue value\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," which is actually the differentiating quantity rather than other, less relevant, colour measures.</br>Step (e) of estimating the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional shape volume\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," may further comprise:</br>providing an estimated three-dimensional skeleton of the at least one object, the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," being estimated from the plurality of images; and optionallycalculating, for voxels of the shape volume, the distance between the voxel and a portion of the three-dimensional skeleton; and optionallyremoving voxels from the shape volume based on the calculated distance.</br>The portion of the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," may be the portion of the three-dimensional skeleton nearest to the voxel.</br>Here, a &quot;three-dimensional skeleton&quot; can be considered as a thin version of the overall shape volume of the object that may only contain necessary structural features. The \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," may a thin version of the overall shape volume that is equidistant or approximately equidistant to the boundaries of the shape volume. The skeleton may emphasize the geometrical and topological properties of the shape volume, such as its connectivity, topology, and dimensions.</br>The method may thus determine voxels of the shape volume that are far away from an estimated \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the object. Such voxels may unlikely be part of the object, and thus may be removed resulting in a shape volume that better reflects the object in the scene. This approach to carving out the shape volume using the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," is ideally suited to the presence of large occlusions, which the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour consistency\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check may not accurately detect.</br>Providing the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the at least one object may comprise: detecting two-dimensional skeletons of the at least one object in the plurality of images; and may further comprise generating the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," from the detected two-dimensional skeletons.</br>Here, a &quot;two-dimensional skeleton&quot; can be considered as a thin version of the overall object in the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image data\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," that may only contain necessary structural features. The two-dimensional skeleton may a thin version of the object in the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image data\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," that is equidistant or approximately equidistant to the boundaries of the object in the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image data\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",".</br>The \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," may thus be estimated using two-dimensional skeletons estimated from the input images. This may involve triangulating the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    two-dimensional skeletons\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," estimated from the input images.</br>The method may carve out the shape volume using the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel colour\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," consistency check and the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," or may use only one of these shape carving techniques. The method may first perform the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel colour consistency\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check for the voxels, and may then perform the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check for the voxels, or vice versa.</br>In an example implementation, the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel colour consistency\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check may be performed on a voxel and the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel may\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," be given a score. A low score may indicate that the voxel is a candidate for removal. A \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check, e.g. by calculating for the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel the\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," distance between the voxel and the portion of the three-dimensional skeleton that is nearest to the voxel, may then be performed and the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel may\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," be given a score based on this check.</br>The \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel may\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," be removed based on the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel colour consistency\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check score or the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check score. In addition, the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel may\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," be removed based on a combination of both these scores. That is, if both the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel colour consistency\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check score and the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check score are low, the voxel is likely to be removed.</br>The \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel colour consistency\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check and/or the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check may be performed for the voxels on the surface of the shape volume. It will be appreciated that as voxels are removed, new voxels are exposed and form part of the surface of the shape volume. The \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel colour consistency\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check and/or the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check may be repeated until a convergence condition is reached, e.g. no more voxels satisfy a condition for removal.</br>Step (f) of combining the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," and the shape volume of the at least one object to generate the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," may comprise:</br>estimating the surface of the shape volume; and optionallyestimating the surface of the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n","; and optionallyusing the surface of the shape volume and the surface of the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," to generate the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",".</br>Using the surface of the shape volume and the surface of the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," may comprise deforming the surface of the shape volume using the surface of the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",". Here, &quot;deforming&quot; means altering the surface of the shape volume, e.g. by shrinking or expanding the surface. In this way, if the estimated shape volume is inflated, it may be deformed down in size using the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",". In addition, the surface of the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," may be used to add detail to the surface of the shape volume. Further, if the estimated shape volume misses a part of the object, the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," may be used to add the missing part into the three-dimensional model.</br>Step (a) of providing the plurality of images may further comprise correcting the colour of the plurality of images. This may comprise applying a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour transformation\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," function to the plurality of images.</br>The plurality of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices may have different \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    camera sensors\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," with different resolutions and even different white balances. This may be because the plurality of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices are part of an informal, rather than a professional setup for capturing the scene. The plurality of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices may be for example mobile phones.</br>This may mean that the resultant images have differences in colour tone and these differences in \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour tone\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," may be caused by variation in the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices rather than variation in \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour tone\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," in the scene being captured. This variation in \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour tone\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," may introduce errors in the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel colour consistency\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check or artifacts in a resultant stage of generating appearance information for the three-dimensional model.</br>Significantly, by correcting the colour of the plurality of images the present invention is able to mitigate or reduce the effect of these differences in colour tone.</br>Step (a) of providing the plurality of images may further comprises estimating the pose of the plurality of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices in the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional coordinate system\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the scene when capturing the plurality of images. The pose may be estimated using Structure from Motion (SfM) techniques.</br>The plurality of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices may have different poses. These different poses may vary and be unpredictable. This may especially be the case if the plurality of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices are handheld cameras, such as mobile phones. By estimating the pose of the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices, the method of the present invention is able to take this variation in pose into account when generating the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",".</br>Step (b) of providing the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the at least one object in the scene may comprise segmenting the plurality of images so as to determine the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",".</br>The method may further comprise rendering the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," to generate visual information.</br>The method may further comprise:</br>(g) generating appearance information for the three-dimensional model; and(h) generating visual information for the scene using the appearance information.</br>Here, &quot;visual information&quot; may be used to generate a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional reconstruction\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the scene which may be viewed by a user. The visual information may be used to generate \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    video data\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", such as dynamic \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image data\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",". The \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    video data\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," may be for free-viewpoint video, meaning that different image or \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    video scenes\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," may be constructed by moving the viewpoint within the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    video data\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," space. The visual information may be \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," data providing the visual appearance of a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",". The visual information may be used for free-viewpoint video. The visual information may refer to augmented reality, virtual reality, or other mixed reality image data.</br>Here, &quot;appearance information&quot; may mean information regarding the appearance of the at least one object. The appearance information may be derived from the plurality of images.</br>Step (g) of generating the appearance information may comprise using image-based rendering methods to texture the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," using the plurality of images.</br>The method may further comprise:</br>extracting a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    background point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," from the generated \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    background point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," being the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," remaining after the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," has been extracted; and optionally generating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    background three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," from the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    background point\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," cloud.</br>The background \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," may be a static \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",".</br>The method may further comprise generating \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    background appearance\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," information using the background three-dimensional model. Generating the visual information for the scene may comprise generating the visual information using the appearance information and the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    background appearance\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," information.</br>Accordingly, there is provided a computer readable medium having instructions recorded thereon which, when expected by a processing device, cause the processing device to perform the method of the first aspect of the invention.</br>Accordingly, there is provided an apparatus for generating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", the apparatus comprising:</br>an \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image providing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," module for providing a plurality of images of a scene captured by a plurality of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices;a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," providing module for providing \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of at least one object in the scene, the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," being extracted from the plurality of images;a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud generation\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," module for generating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," for the scene in three-dimensional space using the plurality of images;an object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," extraction module for extracting an object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," from the generated \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," being a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," associated with the at least one object in the scene;a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional shape volume estimation\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," module for estimating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional shape volume\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the at least one object from the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n","; anda combining module for combining the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," and the shape volume of the at least one object to generate a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",".</br>The apparatus may further comprise:</br>an appearance information generation module for generating appearance information for the at least one object using the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n","; anda \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    video generation\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," module for generating the visual information for the scene using the appearance information.</br>The apparatus may be operable to perform the method of the first aspect of the invention.</br>According to a second aspect of the invention, there is provided a method for generating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", the method comprising:</br>(a) providing \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of at least one object in the scene, the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," being extracted from a plurality of images captured by a plurality of imaging devices;(b) providing a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the at least one object;(c) estimating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional shape volume\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the at least one object from the silhouette information;(d) calculating, for voxels of the shape volume, the distance between the voxel and a portion of the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional skeleton;(e\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",") removing one or more voxels from the shape volume based on the calculated distance to generate a modified shape volume; and(f) generating the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," using the modified shape volume.</br>While the use of an object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," and shape volume and combining these to generate the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," achieves numerous benefits as outlined above, this approach is not necessary in all aspects of the invention. In particular, the additional step of performing the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check to remove voxels from the shape volume improves, by itself, on existing SfS approaches by providing improved shape carving of the shape volume. This means that a shape volume estimated using the features of the second aspect of the invention improves on the existing SfS approach and results in the more accurate generation of the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," even in sparse camera setups.</br>Accordingly, there is provided a computer readable medium having instructions recorded thereon which, when expected by a processing device, cause the processing device to perform the method of the second aspect of the invention.</br>Accordingly, there is provided an apparatus for generating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", the apparatus comprising:</br>a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," providing module for providing \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of at least one object in the scene, the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," being extracted from the plurality of images;a three-dimensional skeleton providing module for providing a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the at least one object;a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional shape volume estimation\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," module for estimating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional shape volume\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the at least one object from the silhouette information;a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel calculation\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," model for calculating, for voxels of the shape volume, the distance between the voxel and a portion of the three-dimensional skeleton; anda removing \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel module\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," for removing one or more voxels from the shape volume based on the calculated distance to generate a modified shape volume; anda \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model generation\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," module for generating the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," using the modified shape volume.</br>According to a third aspect of the invention, there is provided a method for generating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", the method comprising:</br>(a) providing \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of at least one object in the scene, the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," being extracted from a plurality of images captured by a plurality of imaging devices;(b) estimating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional shape volume\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the at least one object from the silhouette information;(c) performing a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel colour consistency\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check on the shape volume estimated from the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel colour consistency\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check comprising:projecting voxels of the shape volume onto the plurality of images to generate a plurality of projections for each voxel projected onto the plurality of images;calculating the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour variance\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," across the plurality of projections for each voxel projected onto the plurality of images, wherein the calculated \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour variance\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," is the variance in the colour hue across the plurality of projections for each voxel projected onto the plurality of images;determining, from the calculated colour variances, voxels of the shape volume that are not colour consistent; and(d) removing one or more voxels from the shape volume that are determined not to be colour consistent so as to generate a modified \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional shape volume\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n","; and(e) generating the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," using the modified \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional shape\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," volume.</br>While the use of an object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," and shape volume, and combining these to generate the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," achieves numerous benefits as outlined above, this approach is not necessary in all aspects of the invention. In particular, the additional step of performing the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour consistency\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check using the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour hue\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," information to remove voxels from the shape volume improves, by itself, on existing SfS approaches by providing improved shape carving of the shape volume. This means that a shape volume estimated using the features of the third aspect of the invention improves on the existing SfS approach and results in the more accurate generation of the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," even in sparse camera setups.</br>Accordingly, there is provided a computer readable medium having instructions recorded thereon which, when expected by a processing device, cause the processing device to perform the method of the third aspect of the invention.</br>Accordingly, there is provided an apparatus for generating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", the apparatus comprising:</br>a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," providing module for providing \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of at least one object in the scene, the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," being extracted from a plurality of images captured by a plurality of imaging devices;a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional shape volume estimation\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," module for estimating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional shape volume\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the at least one object from the silhouette information;a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel colour\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," consistency check performing module for performing a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel colour consistency\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check on the shape volume estimated from the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel colour consistency\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check comprising:projecting voxels of the shape volume onto the plurality of images to generate a plurality of projections for each voxel projected onto the plurality of images;calculating the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour variance\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," across the plurality of projections for each voxel projected onto the plurality of images, wherein the calculated \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour variance\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," is the variance in the colour hue across the plurality of projections for each voxel projected onto the plurality of images;determining, from the calculated colour variances, voxels of the shape volume that are not colour consistent; anda \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel removing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," module for removing one or more voxels from the shape volume that are determined not to be colour consistent so as to generate a modified \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional shape\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," volume; anda \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model generation\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," module for generating the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," using the modified \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional shape\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," volume.</br>Brief Description of the Drawings</br>Examples of the present disclosure will now be described with reference to the accompanying drawings, in which:</br>Figure 1 shows a method according to a first aspect of the invention;Figures 2(a)-(d) show example \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point clouds\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," generated for an object;Figure 3(a)-(d) show example two-dimensional and three-dimensional skeletons generated for an object;Figures 4(a)-(b) show example \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional shape\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," volumes generated for an object;Figures 5(a)-5(c) show example \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional shape\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," volumes generated for an object using a first example setup of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices;Figures 6(a)-6(c) show example \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional shape\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," volumes generated for an object using a second example setup of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices;Figures 7(a)-7(c) show example \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional shape\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," volumes generated for an object using a third example setup of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices; andFigures 8(a)-8(b) show a detailed example implementation of the method according to the first aspect of the invention.</br>Detailed Description</br>Referring to Figure 1 there is shown an example method for generating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional (\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n","3D) model according to the first aspect of the invention.</br>In step 101 a plurality of images are provided. The plurality of images are of a scene captured by a plurality of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices. The scene in this context can be understood as a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D region\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," with an object, such as a person, located therein. The object may be dynamic, meaning that it is moving between frames of images captured by the plurality of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices, and the background of the scene may be relatively stationary.</br>The \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices may be any device capable of capturing \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    static images\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," or dynamic images, e.g. video. The \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices could, for example, be professional \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    video cameras\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," as used in the television and film industries. Such \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices may be dispersed around the scene in a dense, regular, arrangement such that a large amount of information for the scene is captured in the images, and that there is a large amount of overlap between the images captured by the different \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices. The \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices may be simple hand-held cameras, video recorders, or even mobile phones with the capability to capture images and/or record videos. The \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices may be sparsely arranged around the scene such that some aspects of the scene may not be captured in detail, and/or such that there may not be much overlap between the images captured by the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices. It will be appreciated that not all of the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices need be the same, and that they could comprise a number of different types of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices.</br>In some examples, in step 101, one or more pre-processing operations are performed on the plurality of images. Such pre-processing operations are particularly useful where the plurality of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices are of low quality, of different types, and/or handheld.</br>One example pre-processing operation is to colour correct the images captured by the plurality of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices. The plurality of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices may have different \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    camera sensors\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," with different resolutions and even different white balances, which may affect the quality and/or accuracy of the generated \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," unless corrected for.</br>An example \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour correction\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," operation requires the designation of a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    palette image\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," and a target image, and involves determining a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour transformation\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," function to transform the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour distribution\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the target image to match that of the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    palette image\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",". In this example, the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    palette image\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," is an image captured by one of the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices while a target image is an image captured by a different one of the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices. A target images is selected for each of the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices such that \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour transformation\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," functions are determined for each of the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices.</br>In this example \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour correction\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," operation, \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Gaussian Mixture\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," Models are used to represent the colour distribution of the target and palette image. The target and \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    palette images\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," are robustly registered to estimate a non-linear parametric \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour transformation\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," function. This \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour transformation\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," function is then applied for all of the images captured by the respective \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices. Because the transformation function is parametric, it can be applied to all of the images captured by the respective \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," device without creating temporal artifacts. Further, this approach is ideally suited to parallel processing architectures so as to enable the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour correction\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the images to be completed within minimal processing time.</br>It will be appreciated that other techniques for correcting for colour across the images captured by the plurality of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices are within the scope of the present invention. It will further be appreciated that the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour correction\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," operation is not required for the method of the first aspect of the invention. This is especially the case where all of the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices are of the same type / model or have the same \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image sensor\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",".</br>Another example pre-processing operation is to estimate the pose of the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices. The plurality of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices may have different poses, and these poses may vary in an unpredictable manner. This is especially the case if the plurality of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices are handheld cameras, such as mobile phones.</br>An example pose estimation operation involves estimating the pose of the plurality of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices in \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D space\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", and in particular in the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional coordinate system\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the scene when capturing the plurality of images.</br>One approach for estimating the pose of the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices is to use Structure from Motion (SfM) techniques on a frame-by-frame basis. But such an approach could be intractable and computationally expensive.</br>Another approach for estimating the pose of the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices is to use \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    monocular Simultaneous Localization\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," and Mapping (SLAM) algorithms. But such approaches may not be successfully due to their dependency on good initialisation and instability for very small motions, which is typically the case with handheld devices.</br>A beneficial approach is to estimate accurate \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    calibration using\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," SfM at only at a subset of the time intervals. These small subset of frames are denoted as keyframes, and in one example implementation, there is one keyframe for every second of video. In between these time intervals, an algorithm is applied to interpolate calibration parameters for each \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," device individually.</br>In more detail, for this beneficial approach, we can represent the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    n image\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," capturing devices \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    video sequences\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," as S = {s1, ...,sn}, where si(j), j ∈ {1, ...,N} denotes the jth frame of a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    video sequence\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," si ∈ S, with N number of frames. Here, a frame of a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    video sequence\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," can be considered as an image provided by the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," device.</br>A feature in an image sj (j) is defined as: fjk=xjk,djk,xjk∈R2,djk∈Rd</br>Here, xj(k) corresponds to a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    2D position\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of a feature k in frame si(j) and dj(k) represents the description of feature fj(k) in space d. Because, in this example approach, Scale Invariant Feature Transform (SIFT) features are used, the space size of the descriptor is set to d = 128.</br>As mentioned above, the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D poses\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices are estimated by applying SfM on a small subset of frames (key frames) n for every \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    video sequence\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",". The key \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    frame poses\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," are then used as a reference for performing an \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    interpolation process\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," for each \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," device.</br>The key frames have accurate 2D↔3D correspondences which are computed during the triangulation and bundle adjustment process of the SfM pipeline. This correspondence is exploited during the interpolation stage.</br>In particular, if s'(j) and si(j + 1) represent a keyframe and the following frame in the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    video sequence\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," si ∈ S, the first step towards finding the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    camera pose\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of si(j + 1) is to compute successive \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    2D matches\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," between the two frames. The successive \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    2D matches\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," may be computed using a Nearest Neighbour Search (NNS) matching approach.</br>In the second step, when all successful matches have been found for frame si(j + 1), every feature fj+1(k) will have a valid match fj′k in frame si′j, which is known to correspond to a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D point\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," in the reconstruction. The updated correspondences are then used as an input to a PnP algorithm for computing the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," device pose for the new frame. Different \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    PnP algorithms\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," may be used depending on the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D geometry\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the scene.</br>It has been found that the distribution of the 2D↔3D correspondence may affect the accuracy of the estimated pose. In particular, it has been found that the position of the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," device is more sensitive with respect to rotation movements. This effect may be compensated for by running a two way pass for every frame in the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    video sequence\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," and computing the final position by a linear interpolation between the two values.</br>It will be appreciated that other techniques for estimating the pose of the plurality of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices are within the scope of the present invention. It will further be appreciated that the pose estimation operation is not required for the method of the first aspect of the invention. This is especially the case where the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices are not handheld devices.</br>Another example pre-processing operation is to segment the at least one object in the plurality of images. There are many segmentation approaches available, and the choice of segmentation approach may depend on factors such as the quality of the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image data\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", the type of object to be segmented, and the computational resources available. The present invention is not limited to any particular \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image segmentation\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," technique. It will further be appreciated that the operation of segmentation may not be required if the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image data\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," is provided pre-segmented, for example.</br>Referring to Figure 1, in step 102, \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," is provided of at least one object in the scene. The \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," is extracted from the plurality of images, and may be obtained by performing segmentation operations on the plurality of images as described above.</br>In step 103, a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," for the scene in \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D space\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," is generated using the plurality of images.</br>In one example approach, the generation of the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," follows a two-stage approach.</br>In a first stage, a sparse \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," is calculated through an SfM approach, typically using SIFT features. In a second stage, a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    patch-based point cloud densification\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," algorithm, such as PMVS, generates the final dense cloud. The density of the resulting \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D point\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," cloud depends on the number of cameras and the amount of overlap in the images.</br>In a preferred approach, KAZE features are used to generate the sparse \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D point\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," cloud as the computation can be easily parallelised. Further, the resulting sparse \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point clouds\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," are slightly denser compared to the SIFT approach. The \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    KAZE approach\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," may not accurately detect feature points in dark areas so it is further preferred to enhance the images prior to using the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    KAZE features\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",".</br>Referring to Figure 1, in step 104, the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," is then extracted from the generated \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D point\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," cloud. The object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," may be extracted using the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", otherwise known as object masks, obtained from a segmentation operation performed on the images. This results in segmenting the generated \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D point\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," cloud into the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", i.e. the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    foreground point\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," cloud, and a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    background point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",".</br>Referring to Figures 2(a)-(d) there is shown resultant object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point clouds\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 201-204 generated for a first frame (Figures 2(a)-(b)) and a second frame (Figures 2(c)-2(d)) of a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    video sequence\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," captured by a plurality of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices in a sparse setup. The object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point clouds\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 201, 203 in Figures 2(a) and 2(c) show the SIFT+PMVS approach, while the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point clouds\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 202, 204 in Figures 2(b) and 2(d) show the preferred \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    KAZE approach\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," with colour enhancement. Figures 2(a)-2(d) highlight that the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    KAZE approach\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," generates denser \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point clouds\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 202, 204, and thus enables a more accurate \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," to be generated.</br>Referring to Figure 1, in step 105 a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional shape volume\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the at least one object is estimated using the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",".</br>The \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional shape volume\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," is estimated using a shape-from-silhouette (SfS) approach. Existing SfS approaches may not be able to handle concavities well, and may suffer from occlusions when sparse setups of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices are used.</br>Significantly, the present invention is able to mitigate this effect by using shape carving techniques for carving out the shape volume to remove voxels which have been incorrectly determined to be part of the shape volume using the SfS approach.</br>In an example implementation, the present invention performs a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel colour\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," consistency check on the shape volume estimated from the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," so as to determine voxels of the shape volume that are not colour consistent.</br>In this example implementation, voxels of the shape volume are projected onto the plurality of images. This results in the generation of a plurality of projections for each voxel projected onto the plurality of images. If the voxel is part of the object, then the resultant projections should be colour consistent across the plurality of images because they all represent the same point of the object across the plurality of images. If, however, the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel is\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," not part of the object, then the resultant projections will be expected to be colour inconsistent because they may represent different parts of the scene across the plurality of images.</br>To determine this, the present method calculates a difference in colour across the plurality of projections for each voxel projected onto the plurality of images; and determines, from the calculated colour differences, voxels of the shape volume that are not colour consistent. The voxels that are determined to not be colour consistent may then be removed from the shape volume.</br>In this implementation, the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour difference\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," is a measure of the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour variance\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," across the plurality of projections and is, in particular, a measure of the variance in the colour hue across the plurality of projections. Measuring the variance in the colour hue provides benefits as compared to measuring RBG or CIELAB variance as it enables the present method to accept less relevant differences in saturation and value that may remain in images, e.g. after colour correction. Such differences may normally be due to differences in the sensor or white balance of the plurality of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices.</br>In an example implementation, the present invention uses an estimated three-dimensional skeleton for the at least one object to determine voxels of the shape volume to be removed.</br>In this example implementation, 2D skeletons of the at least one object are detected in the plurality of images. Example 2D skeletons 301-303 of an object are shown in Figures 3(a)-(c). The \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    2D skeletons\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 301-303 are used to generate the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 304 as shown in Figure 3(d).</br>In more detail, the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 304 of the object is estimated by triangulating a set of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    2D skeletons\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 301-303 detected in the plurality of images. The \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    2D skeletons\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 301-303 may be determined using Part Affinity Fields (PAFs). This results in, for each image, a set of detected skeletons 301-303 having a set of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    2D joints\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," and a set of confidence values. Unwanted skeletons (e.g. objects in the background, such as people walking by or audience members to the scene) may be filtered out by using the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", otherwise known as object masks obtained during the object segmentation. If there is more than one intended object in the scene, epipolar constraints may be applied to the scene so that the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    2D skeletons\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 301-303 can be correctly matched to the different objects. The \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D joint\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," coordinates for each skeleton 301-303 are estimated by minimizing a set of overdetermined linear \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    triangulation problems\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," to generate the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 304.</br>In step 105, for voxels of the shape volume, the distance between the voxel and the portion of the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," that is nearest to the voxel is calculated. This can be performed by calculating the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Euclidean distance\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of each voxel to its closest bone in the skeleton. Voxels from the shape volume may then be removed based on the calculated distance. In particular, voxels of the shape volume that are far away from an estimated three-dimensional skeleton of the object may be removed. Such voxels may unlikely be part of the object, and thus may be removed resulting in a shape volume that better reflects the object in the scene.</br>In an example implementation, both the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel colour consistency\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check and the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check are used to carve out the shape volume estimated using the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",". The \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel may\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," be removed based on a score calculated as a result of the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel colour consistency\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check score and a score calculated as a result of the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check. That is, if both the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel colour consistency\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check score and the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check score are low for a voxel, the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel is\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," likely to be removed.</br>The \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel colour consistency\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check and the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check are performed for the voxels on the surface of the shape volume. As voxels are removed, new voxels are exposed and form part of the surface of the shape volume. The \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel colour consistency\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check and/or the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check is repeated until a convergence condition is reached, e.g. no more voxels satisfy a condition for removal. In other words, the consistency check need to be performed on all of the voxels, but rather on just the surface voxels until a convergence condition is reached.</br>Referring to Figure 4(a), there is shown an example shape volume 401 estimated using the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," only. The estimated shape volume 401 is extremely inflated due to severe occlusions as a result of a sparse \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," device setup.</br>Referring to Figure 4(b), there is shown an example shape volume 402 estimated using the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," and then carved out using the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour consistency\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check and \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check operations described above. The \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour consistency\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check is effective in softening the edges of the shape volume, while the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check is effective in removing a large number of incorrect voxels.</br>Referring again to Figure 1, in step 106 of the method, the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," is generated by combining the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," and the shape volume.</br>In an example implementation, the surface Mv of the shape volume is estimated using a Marching Cubes algorithm or other similar approach. Further, the surface Mf of the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," is estimated using a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Poisson Surface Reconstruction\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," (PSR) algorithm or other similar approach. The surface Mf is then used to guide a controlled deformation of the surface Mν, such that the resultant model Mv′ has both the details captured by the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," and full volume completeness.</br>In this example implementation, a ray for every \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    vertex vi\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of Mv is cast following its normal. If the ray intersects with Mf, vi will move to the point of intersection. The set of vertices displaced after this \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    ray casting\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," stage define a set of handle points H for the deformation problem. The deformation region R grows from the handle region by iteratively searching the neighbours of each vi ∈ H. Each \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    vj ∈\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," R is assigned a level that increases with the number of steps needed to take to approach a vertex of the handle. The closest handle vertex and its corresponding \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    displacement vector\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," dih are also stored. The displacement function for each vir is defined as follows: dvjr=vjr+njr⋅dihl−ljl</br>Here, njr is the normal of vertex vjr, l is the total number of levels in the deformation region, and lj is the current level.</br>In this example implementation, possible artifacts in the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," are reduced by applying \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Laplacian smoothing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," to the resulting \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", and by identifying and removing isolated triangle isles and non-manifold edges and vertices.</br>The above example implementation is only one example approach for combining the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," and the shape volume. Other methods for combining the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," and the shape volume to generate the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," are within the scope of the present invention.</br>By combining the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," and the shape volume, the present invention is able generate three-dimensional models that accurately reflect the object in the scene even in sparse \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," device setups. The present invention therefore enables the benefits of the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," and the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," techniques to be achieved without the associated disadvantages. The generated \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," is able to preserve the detail of the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," and the completeness of the estimated \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional shape volume\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," while avoiding inflation of the three-dimensional model.</br>Referring to Figures 5 to 7 there are shown \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D models\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," generated based on \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image data\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of a scene captured by a plurality of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices.</br>Referring to Figures 5(a) - (c) there are shown three \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D models\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 501, 502, 503 of an object captured using 53 \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices. This is a dense camera setup meaning that there is good scene coverage and a significant amount of overlap between the images captured by the plurality of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices. Figure 5(a) there is shown a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 501 of an object generated using an existing SfS approach, Figure 5(b) shows a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 502 of an object generated using an existing \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," MVS approach, and Figure 5(c) shows a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 503 of an object generated using the approach of the present invention where the shape volume and the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," are combined to generate the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 503.</br>In this dense camera setup, both the SfS approach and the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    MVS approach\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," work well, but the approach of the present invention results in a more accurate \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," by combining these existing approaches.</br>Referring to Figures 6(a)-6(c) there are shown three \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D models\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 601, 602, 603 of an object captured using 18 \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices. The 18 \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices have been arranged in way which attempts to preserve as much scene coverage as possible, and as result the overlap between the images captured by the plurality of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices suffers. Figure 6(a) shows a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 601 of the object generated using the SfS approach, Figure 6(b) shows the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 602 of the object generated using the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," MVS approach, and Figure 6(c) shows the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 603 of the object generated using the approach of the present invention where the shape volume and the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," are combined to generate the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 603.</br>As the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image overlap\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of this sparse camera setup has been reduced with the intention of maximising the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    scene coverage\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", it can be seen that the SfS approach outperforms the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    MVS approach\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", as expected. Significantly, however, the approach of the claimed invention which combines the shape volume and the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," outperforms both these approaches. In particular, the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 603 more accurately reflects the object captured in the scene.</br>Referring to Figures 7(a)-7(c) there are shown three \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D models\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 701, 702, 703 of an object captured using 18 \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices. In this example, the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices have been arranged in a way which prioritises overlap of the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices rather than scene coverage. Figure 7(a) shows a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 701 of the object generated using the SfS approach, Figure 7(b) shows the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 702 of the object generated using the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," MVS approach, and Figure 7(c) shows the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    7D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 603 of the object generated using the approach of the present invention where the shape volume and the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," are combined to generate the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 703.</br>As the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image overlap\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of this sparse camera setup has been maximised at the expense of scene coverage, it can be seen that the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    MVS approach\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," outperforms the SfS approach, as expected. Significantly, however, the approach of the claimed invention which combines the shape volume and the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," outperforms both these approaches. In particular, the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 703 more accurately reflects the object captured in the scene.</br>Accurate \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D reconstruction\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of objects in the scene, and in particular dynamic objects, allows users to fully immerse in related \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    VR/\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n","\n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    AR/\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n","\n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    MR visualizations\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",". The method according to the first aspect of the invention is able to achieve this while supporting affordable \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capture\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," and processing. Therefore, it can be seen that the approach of the present invention has benefits in terms of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," accuracy in both dense camera setups and sparse camera setups. Significantly, the present invention provides these benefits in sparse camera setups regardless of whether \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," device overlap or scene coverage is maximised.</br>In an example implementation, the method further comprises generating appearance information for the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",". This involves using image-based rendering methods to texture the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," using the plurality of images, and in particular involves colouring the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," using an \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image blending\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," technique. This technique acts to merge the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," provided by the different \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices, even though there might be over or underexposed images and differences in colour balance. The \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    background 3D\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," model may be rendered as a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    textured 3D\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," mesh.</br>In this example implementation, the blending function is firstly defined in the topology space by back-projecting every facet to each image to obtain a per-facet per-image rating. In this way, the higher the area of the back-projection of the facet onto the image, the higher the rating. The final rating is smoothed using the angle between the normal of the facet and the image, which penalizes wider angles even when the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," device is very close to the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",". In addition, an \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    occlusion test\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," for every facet and image is performed which avoids the inclusion of wrongly projected areas onto the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    textured 3D\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," model. Moreover, to further improve the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    visual quality\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    textured 3D\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," models, particularly of human faces, the method in this example implementation searches for faces in the images and determine the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," device with the largest facial region. The rating of the facets belonging to that detected area are significantly increased, giving a much larger contribution to that particular \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," device. Further, to ensure smooth transitions across the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", each vertex also gets a per-image rating by averaging the ratings of the faces that contain it. The final colour for each point of the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," is a weighted average of the camera contributions, bilinearly interpolating the ratings across the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",".</br>The generated appearance information may then be used to generate visual information for the scene. There are several methods for generating the visual information for display on different devices.</br>The \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the object is rendered either as a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", or as a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    textured 3D\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," mesh. These approaches for rendering the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the object may be used for both view synthesis and for visualization in \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    VR/\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n","\n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    AR/\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n","MR.</br>In \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    free view point video\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    video may\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," be viewed from new angles either in a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    free navigation\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," mode and a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    view synthesis\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," mode. During the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    free navigation\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," mode, a user can freely change the viewpoint and only the reconstructed objects are rendered. In the view synthesis mode, the path is restricted to a circular trajectory between \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices.</br>In example implementations, the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the scene and the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the background are used to improve view synthesis, such that the user may sweep between different \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    camera views\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," smoothly.</br>In the view synthesis mode, the virtual camera is using extrinsic and intrinsic parameters from the selected real \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," device, and the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    captured image\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," is projected onto the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," and a background plane. During the transition phase, when a user changes the viewpoint, the new parameters of the virtual camera are computed as an interpolation between the current camera and the destination camera. In the case of different \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image resolutions\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," or focal lengths for the current camera and the destination camera, linear interpolation may be used to compute \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    virtual camera intrinsic\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," parameters. To compute the virtual camera's position and rotation, a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    spherical linear interpolation approach\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," may be used</br>When a change in the viewpoint is requested by the user, a virtual camera begins to move from the current viewpoint of the camera towards the destination camera. During the transition, the pose of the virtual camera is unknown and is determined according to the known poses of the current and destination cameras respectively.</br>To synthesize a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    virtual view\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," in real-time, the method only considers the current frame and the two nearest \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices. The scene from the two nearest \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices is rendered into depth textures in two passes. In the third pass, the scene from the virtual camera is rendered, a per-\n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    pixel depth\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," test in a fragment shader is performed, and colours from the two cameras and the rendered mesh (or point cloud) are blended. In the last rendering pass post-processing effects such as \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    motion blur\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," may also be applied.</br>Referring to Figures 8(a)-8(b), there is shown a detailed implementation of the method according to the first aspect of the invention.</br>In step 801, the plurality of images are provided.</br>In step 802, the images are colour corrected using the techniques described above so as to provide \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour corrected\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," images 803.</br>In step 804, the poses of the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices that provided the plurality of images are estimated using the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour corrected\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," images 803 and the techniques described above. This results in pose information 805.</br>In step 806, object segmentation is performed on the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour corrected\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," images 803 to segment the object from the background in the plurality of images. This results in \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", otherwise known as object masks 807.</br>In step 808, the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour corrected\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," images 803, pose information 805, and \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 807 are used to generate a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," using the techniques described above. This results in a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    background point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 812 and an object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 813.</br>In step 809, the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour corrected\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," images 803, pose information 805, and \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 807 are used to estimate a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," using the techniques described above. This results in a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 810.</br>In step 811, the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour corrected\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," images 803, pose information 805, \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 807 and \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 810 are used to estimate a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D shape\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," volume for the object using the technique described above. This results in a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D shape\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," volume 814.</br>In step 815, the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    background point\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," cloud 812 is used to generate a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    background model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 815. This results in a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 817 for the background.</br>In step 816, the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 813 and the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D shape\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," volume 814 are combined using the techniques described above. This results in a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 818 for the object.</br>In step 819, the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 817 for the background and the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," 818 for the object are used to generate visual information, such as by using rendering techniques.</br>Although the above described embodiments use the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," and shape volume, and combine these to generate the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", this approach is not necessary in all aspects of the invention.</br>In particular, the additional step of performing the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check to remove voxels from the shape volume improves, by itself, on SfS approaches by providing improved shape carving of the shape volume. This means that a shape volume estimated using the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check described above improves on the SfS approach and results in the more accurate generation of the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," even in sparse camera setups.</br>In particular, the additional step of performing the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour consistency\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check using the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour hue\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," information to remove voxels from the shape volume improves, by itself, on SfS approaches by providing improved shape carving of the shape volume. This means that a shape volume estimated using \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel colour consistency\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check described above improves on the SfS approach and results in the more accurate generation of the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," even in sparse camera setups.</br>The \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour consistency\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check and the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check may be performed together without necessarily required the combining with the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," as described in the above embodiments.</br>The present invention further provides an apparatus and a computer readable medium for implementing the methods described above. The apparatus may be a computing apparatus or a collection of computing apparatuses communicated with one another over a distributed network.</br>At least some of the example embodiments described herein may be constructed, partially or wholly, using dedicated special-purpose hardware. Terms such as 'component', 'module' or 'unit' used herein may include, but are not limited to, a hardware device, such as circuitry in the form of discrete or integrated components, a Field Programmable Gate Array (FPGA) or Application Specific Integrated Circuit (ASIC), which performs certain tasks or provides the associated functionality. In some embodiments, the described elements may be configured to reside on a tangible, persistent, addressable storage medium and may be configured to execute on one or more processors. These functional elements may in some embodiments include, by way of example, components, such as software components, object-oriented software components, class components and task components, processes, functions, attributes, procedures, subroutines, segments of program code, drivers, firmware, microcode, circuitry, data, databases, data structures, tables, arrays, and variables. Although the example embodiments have been described with reference to the components, modules and units discussed herein, such functional elements may be combined into fewer elements or separated into additional elements.</br>Various combinations of optional features have been described herein, and it will be appreciated that described features may be combined in any suitable combination. In particular, the features of any one example embodiment may be combined with features of any other embodiment, as appropriate, except where such combinations are mutually exclusive. Throughout this specification, the term &quot;comprising&quot; or &quot;comprises&quot; means including the component(s) specified but not to the exclusion of the presence of others.</br>The described and illustrated embodiments are to be considered as illustrative and not restrictive in character, it being understood that only the preferred embodiments have been shown and described and that all changes and modifications that come within the scope of the inventions as defined in the claims are desired to be protected. It should be understood that while the use of words such as &quot;preferable&quot;, &quot;preferably&quot;, &quot;preferred&quot; or &quot;more preferred&quot; in the description suggest that a feature so described may be desirable, it may nevertheless not be necessary and embodiments lacking such a feature may be contemplated as within the scope of the invention as defined in the appended claims. In relation to the claims, it is intended that when words such as &quot;a,&quot; &quot;an,&quot; &quot;at least one,&quot; or &quot;at least one portion&quot; are used to preface a feature there is no intention to limit the claim to only one such feature unless specifically stated to the contrary in the claim. When the language &quot;at least a portion&quot; and/or &quot;a portion&quot; is used the item can include a portion and/or the entire item unless specifically stated to the contrary.</br>In summary, there is provided a method and apparatus for generating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",". The method comprising providing a plurality of images of a scene captured by a plurality of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices (101); providing \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of at least one object in the scene (102); generating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," for the scene in \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D space\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," using the plurality of images (103); extracting an object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," from the generated \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," being a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," associated with the at least one object in the scene (104); estimating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D shape\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," volume of the at least one object from the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," (105); and combining the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," and the shape volume of the at least one object to generate a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," (106).</br>An apparatus for generating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", and a computer readable medium for generating the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," are also provided.</br>Attention is directed to all papers and documents which are filed concurrently with or previous to this specification in connection with this application and which are open to public inspection with this specification, and the contents of all such papers and documents are incorporated herein by reference.</br>All of the features disclosed in this specification (including any accompanying claims and drawings), and/or all of the steps of any method or process so disclosed, may be combined in any combination, except combinations where at least some of such features and/or steps are mutually exclusive.</br>Each feature disclosed in this specification (including any accompanying claims and drawings) may be replaced by alternative features serving the same, equivalent or similar purpose, unless expressly stated otherwise. Thus, unless expressly stated otherwise, each feature disclosed is one example only of a generic series of equivalent or similar features.</br>The invention is not restricted to the details of the foregoing embodiment(s). The invention extends to any novel one, or any novel combination, of the features disclosed in this specification (including any accompanying claims, abstract and drawings), or to any novel one, or any novel combination, of the steps of any method or process so disclosed.</br>_____c:</br>1. A method for generating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", the method comprising the following steps: (a) providing a plurality of images of a scene captured by a plurality of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices; (b) providing \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of at least one object in the scene, the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," being extracted from the plurality of images; (c) generating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," for the scene in three-dimensional space using the plurality of images; (d) extracting an object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," from the generated \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," being a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," associated with the at least one object in the scene; (e) estimating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional shape volume\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the at least one object from the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n","; and (f) combining the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," and the shape volume of the at least one object to generate the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",".</br>2. A method as claimed in claim 1, wherein step (e) of estimating the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional shape volume\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," further comprises: performing a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel colour consistency\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check on the shape volume estimated from the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," so as to determine one or more voxels of the shape volume that are not colour consistent; and removing one or more voxels from the shape volume that are determined not to be colour consistent.</br>3. A method as claimed in claim 2, wherein performing the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel colour\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," consistency check on the shape volume comprises: projecting voxels of the shape volume estimated from the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," onto the plurality of images to generate a plurality of projections for each voxel projected onto the plurality of images; calculating the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour variance\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," across the plurality of projections for each voxel projected onto the plurality of images; and determining, from the calculated colour variances, one or more voxels of the shape volume that are not colour consistent.</br>4. A method as claimed in claim 3, wherein the calculated \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour variance\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," is the variance in the colour hue across the plurality of projections for each voxel projected onto the plurality of images.</br>5. A method as claimed in any preceding claim, wherein step (e) of estimating the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional shape volume\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," further comprises: providing an estimated three-dimensional skeleton of the at least one object, the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," being estimated from the plurality of images; calculating, for voxels of the shape volume, the distance between the voxel and a portion of the three-dimensional skeleton; and removing one or more voxels from the shape volume based on the calculated distance.</br>6. A method as claimed in claim 5, wherein providing the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the at least one object comprises: detecting two-dimensional skeletons of the at least one object in the plurality of images; and generating the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," from the detected two-dimensional skeletons.</br>7. A method as claimed in any preceding claim, wherein step (f) of combining the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," and the shape volume of the at least one object to generate the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," comprises: estimating the surface of the shape volume; estimating the surface of the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n","; and using the surface of the shape volume and the surface of the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," to generate the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    3D model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",".</br>8. A method as claimed in any preceding claim, wherein step (a) of providing the plurality of images further comprises correcting the colour of the plurality of images by applying a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour transformation\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," function to the plurality of images, and/or wherein step (a) of providing the plurality of images further comprises estimating the pose of the plurality of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices in three-dimensional space when capturing the plurality of images.</br>9. A method as claimed in any preceding claim, wherein step (b) of providing the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the at least one object in the scene comprises segmenting the plurality of images so as to determine the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",".</br>10. A method as claimed in any preceding claim, further comprising: (g) generating appearance information for the three-dimensional model; and (h) generating visual information for the scene using the appearance information.</br>11. A method as claimed in claim 10, wherein step (g) of generating the appearance information comprises using image-based rendering methods to texture the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," using the plurality of images.</br>12. A method as claimed in claim 10 or 11, further comprising: extracting a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    background point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," from the generated \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    background point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," being the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," remaining after the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," has been extracted; generating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    background three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," from the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    background point\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," cloud.</br>13. An apparatus for generating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", the apparatus comprising: an \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image providing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," module for providing a plurality of images of a scene captured by a plurality of \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    image capturing\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," devices; a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," providing module for providing \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of at least one object in the scene, the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," being extracted from the plurality of images; a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud generation\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," module for generating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," for the scene in three-dimensional space using the plurality of images; a object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," extraction module for extracting a object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," from the generated \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," being a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," associated with the at least one object in the scene; a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional shape volume estimation\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," module for estimating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional shape volume\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the at least one object from the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n","; and a combining module for combining the object \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    point cloud\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," and the shape volume of the at least one object to generate the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",".</br>14. A method for generating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", the method comprising: (a) providing \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of at least one object in the scene, the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," being extracted from a plurality of images captured by a plurality of imaging devices; (b) providing a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional skeleton\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the at least one object; (c) estimating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional shape volume\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the at least one object from the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n","; (d) calculating, for voxels of the shape volume, the distance between the voxel and a portion of the three-dimensional skeleton; (e) removing one or more voxels from the shape volume based on the calculated distance to generate a modified shape volume; and (f) generating the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," using the modified shape volume.</br>15. A method for generating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", the method comprising: (a) providing \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of at least one object in the scene, the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," being extracted from a plurality of images captured by a plurality of imaging devices; (b) estimating a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional shape volume\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the at least one object from the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n","; (c) performing a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel colour\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," consistency check on the shape volume estimated from the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    silhouette information\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    voxel colour consistency\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," check comprising: projecting voxels of the shape volume onto the plurality of images to generate a plurality of projections for each voxel projected onto the plurality of images; calculating the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour variance\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," across the plurality of projections for each voxel projected onto the plurality of images, wherein the calculated \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    colour variance\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," is the variance in the colour hue across the plurality of projections for each voxel projected onto the plurality of images; determining, from the calculated colour variances, one or more voxels of the shape volume that are not colour consistent; and (d) removing one or more voxels from the shape volume that are determined not to be colour consistent so as to generate a modified \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional shape volume\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n","; and (e) generating the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional model\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," using the modified \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    three-dimensional shape\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," volume.</div></span>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"0cqDy87l0mxI"},"source":["**Missing:**\\\n","virtual, augmented, and mixed reality (VR/AR/MR)\\\n","Many spelling: abbreviated and full word, often combined with comma ',' and '/' \\\n","Free-Viewpoint Video (FVV)\\\n","\n","**Incorrect:** \\\n","image- TECH\\\n","3D one (The 2D matrix TECH of cameras is extended to a 3D one TECH )\\\n","image based TECH FFV technique TECH : should be 1 term\\\n","camera sensors TECH: G06T appears to include only software, not hardware"]},{"cell_type":"markdown","metadata":{"id":"H6WNrF7U9iil"},"source":["# Attempt to train with other models.\n","Fail due to version incompatibility\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6nwn9RHjwu_6","executionInfo":{"status":"ok","timestamp":1614722297490,"user_tz":-60,"elapsed":2105,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"845aa6a3-c17f-491d-853b-72049a91eda7"},"source":["# ! python -m spacy download en_core_web_md\n","# ! python -m spacy download en_core_web_trf"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","\u001b[38;5;1m✘ No compatible model found for 'en_core_web_trf' (spaCy v2.3.5).\u001b[0m\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"2xwFEw_I16_G","executionInfo":{"status":"ok","timestamp":1614720528826,"user_tz":-60,"elapsed":1966,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"0a7f0910-bfed-4730-b164-650c8a0f135c"},"source":["shutil.copy2(basedir/'annotatedg06t.jsonl', os.getcwd())\n","shutil.copy2(basedir/'annotatedg06t_correct.jsonl', os.getcwd())"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/annotatedg06t_correct.jsonl'"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8qYhhK1r11Rl","executionInfo":{"status":"ok","timestamp":1614720763131,"user_tz":-60,"elapsed":4227,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"bdcccf44-ee3f-4f70-d1fc-0c5ab120ddf5"},"source":["!prodigy db-in annotatedg06t annotatedg06t.jsonl\n","!prodigy db-in annotatedg06t_correct annotatedg06t_correct.jsonl"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n","  \"\"\")\n","\u001b[38;5;2m✔ Created dataset 'annotatedg06t' in database SQLite\u001b[0m\n","\u001b[38;5;2m✔ Imported 109 annotations to 'annotatedg06t' (session\n","2021-03-02_21-32-40) in database SQLite\u001b[0m\n","Found and keeping existing \"answer\" in 109 examples\n","/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n","  \"\"\")\n","\u001b[38;5;2m✔ Created dataset 'annotatedg06t_correct' in database SQLite\u001b[0m\n","\u001b[38;5;2m✔ Imported 26 annotations to 'annotatedg06t_correct' (session\n","2021-03-02_21-32-42) in database SQLite\u001b[0m\n","Found and keeping existing \"answer\" in 26 examples\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"imwfSE458yCN","executionInfo":{"status":"ok","timestamp":1614722317446,"user_tz":-60,"elapsed":1880,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"9feb188a-09d2-43c0-b8dc-f45d3b3a08ae"},"source":["!prodigy stats"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n","  \"\"\")\n","\u001b[1m\n","============================== ✨  Prodigy Stats ==============================\u001b[0m\n","\n","Version          1.10.6                        \n","Location         /usr/local/lib/python3.7/dist-packages/prodigy\n","Prodigy Home     /root/.prodigy                \n","Platform         Linux-4.19.112+-x86_64-with-Ubuntu-18.04-bionic\n","Python Version   3.7.10                        \n","Database Name    SQLite                        \n","Database Id      sqlite                        \n","Total Datasets   2                             \n","Total Sessions   2                             \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8IuNeKpm2G-4"},"source":["# New terms outside Wikipedia list"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4axlz3pMlCPX","executionInfo":{"status":"ok","timestamp":1614615466052,"user_tz":-60,"elapsed":771,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"66222262-b9ba-4354-97ed-db9e38c6ef54"},"source":["print(os.listdir('./'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['.config', 'manyterms.lower.txt', 'drive', 's2v_old', 'G06T.txt.gz', 'tok2vec_cd8_model289.bin', 's2v_reddit_2015_md.tar.gz', '._s2v_old', 'termsg06tpatterns.jsonl', 'prodigy-1.10.6-cp36.cp37.cp38-cp36m.cp37m.cp38-linux_x86_64.whl', 'sample_data']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"vmPYnSbykyox","executionInfo":{"status":"ok","timestamp":1614892588342,"user_tz":-60,"elapsed":800,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"9d253bd0-addc-4133-8db2-d05933ab848c"},"source":["shutil.copy2(basedir/'termsg06tpatterns.jsonl', os.getcwd())"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/termsg06tpatterns.jsonl'"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"code","metadata":{"id":"z29kZ5Rg2LFB"},"source":["import json\n","\n","# techterms = [jline for jline in open('termsg06tpatterns.jsonl').read().split('\\n')]\n","# techterms[-2:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c_fCGiTWlcSH","executionInfo":{"status":"ok","timestamp":1614892591333,"user_tz":-60,"elapsed":480,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"df0fdbf6-953a-4996-e44e-b6a5c3840591"},"source":["def get_term_from_jsonl(jsonline):\n","  if not jsonline: return\n","  line = json.loads(jsonline)\n","  words = [pattern['lower'] for pattern in line['pattern']]\n","  return words[0] if len(words) == 1 else ' '.join(words)\n","\n","terms = list(map(get_term_from_jsonl, open('termsg06tpatterns.jsonl', 'r').read().split('\\n')))\n","print(len(terms))\n","print(terms[:11])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["391\n","['stochastic processes', 'dynamic range', 'vr', 'numerical analysis', 'blender', 'vr applications', 'video processing', 'white balance', 'online learning', 'numerical methods', 'board design']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_jD2II6ZpdJm","executionInfo":{"status":"ok","timestamp":1614892635550,"user_tz":-60,"elapsed":646,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"699596dc-caa4-426e-a0ed-6c9bfe489ef2"},"source":["newterms = list(set(terms) - set(mwes))\n","print(len(newterms))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["81\n","3d raytracer graphics apis vr applications webgl . 3d video adobe suite wysiwyg design tools gimp autocad . 3d design 3d objects game programming gui stuff guis 2d game 3d work blender opengl webgl vr\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LrSWYF5UGinm","executionInfo":{"status":"ok","timestamp":1614892653880,"user_tz":-60,"elapsed":532,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"4b87ed0e-a2ff-4cac-f19b-ea0e91f15128"},"source":["print(', '.join(newterms[:22]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["3d, raytracer, graphics apis, vr applications, webgl ., 3d video, adobe suite, wysiwyg, design tools, gimp, autocad ., 3d design, 3d objects, game programming, gui stuff, guis, 2d game, 3d work, blender, opengl, webgl, vr\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LejrJ6XQEn9J"},"source":["# Add Wikipedia links"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F4ZgTCobHumi","executionInfo":{"status":"ok","timestamp":1615137276013,"user_tz":-60,"elapsed":118231,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"15cffc05-e743-4234-e30f-d90c648845e0"},"source":["# https://stackabuse.com/getting-started-with-pythons-wikipedia-api/\n","!pip install wikipedia\n","import wikipedia"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting wikipedia\n","  Downloading https://files.pythonhosted.org/packages/67/35/25e68fbc99e672127cc6fbb14b8ec1ba3dfef035bf1e4c90f78f24a80b7d/wikipedia-1.4.0.tar.gz\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (4.6.3)\n","Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (2.23.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n","Building wheels for collected packages: wikipedia\n","  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wikipedia: filename=wikipedia-1.4.0-cp37-none-any.whl size=11686 sha256=fef33bd5781bc878cd3147c15c54c516e2b33bb4714bbe99e648b9af817ade95\n","  Stored in directory: /root/.cache/pip/wheels/87/2a/18/4e471fd96d12114d16fe4a446d00c3b38fb9efcb744bd31f4a\n","Successfully built wikipedia\n","Installing collected packages: wikipedia\n","Successfully installed wikipedia-1.4.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LtOE8hebHzXR","executionInfo":{"status":"ok","timestamp":1615136320615,"user_tz":-60,"elapsed":2345,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"0086eaf2-d225-418e-af88-7e33178c3df0"},"source":["print(wikipedia.page(\"wysiwyg\").url)\n","print(wikipedia.page(\"raytracer\").url)\n","print(wikipedia.page(\"3D computer\").url)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["https://en.wikipedia.org/wiki/WYSIWYG\n","https://en.wikipedia.org/wiki/Ray_tracing_(graphics)\n","https://en.wikipedia.org/wiki/3D_computer_graphics\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"V3_NhqFbIDDq"},"source":["shutil.unpack_archive(basedir/'tech_model.zip', 'tech_model')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VyssFZC5IA14"},"source":["import spacy\n","SPACY_MODEL = \"./tech_model\"      # path to spaCy model with entity recognizer\n","nlp = spacy.load(SPACY_MODEL)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZjflcDzxKwNT"},"source":["from IPython.display import display, HTML"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":140},"id":"xXABHOcnF7sP","executionInfo":{"status":"ok","timestamp":1615137316423,"user_tz":-60,"elapsed":620,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"451f67f2-f677-4117-da77-e7e344fd92d3"},"source":["sentence = 'In 3D computer graphics, ray tracing is a rendering technique for generating an image by tracing the path of light as pixels in an image plane and simulating the effects of its encounters with virtual objects'\n","print([ent for ent in nlp(sentence).ents])\n","# displacy.render(nlp(sentence), style=\"ent\", jupyter=True)\n","displacy.render(nlp(sentence), style=\"ent\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[3D computer, ray tracing]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">In \\n<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\\n    3D computer\\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\\n</mark>\\n graphics, \\n<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\\n    ray tracing\\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\\n</mark>\\n is a rendering technique for generating an image by tracing the path of light as pixels in an image plane and simulating the effects of its encounters with virtual objects</div>'"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"XBe54srYN2FW"},"source":["## Display terms with Wikipedia link and summary tooltip"]},{"cell_type":"code","metadata":{"id":"MQNn3zkpIab2"},"source":["import random \n","\n","def get_term_link(term):\n","  try:\n","    page = wikipedia.page(term)\n","  # except wikipedia.DisambiguationError as e:\n","  #   page = wikipedia.page(random.choice(e.options))\n","  except:\n","    return None\n","  # print(page)\n","  anchor = f\"<a href='{page.url}' title='{page.summary.split('.')[0]}'>{term}</a>\"\n","  # print(anchor)\n","  return anchor\n","\n","def get_linked_text(nlp, text):\n","  doc = nlp(text)\n","  html = displacy.render(doc, style='ent')\n","  for ent in doc.ents:\n","    # print(ent)\n","    link = get_term_link(ent.text)\n","    if link: html = html.replace(f'{ent.text}\\n', f'{link}\\n')\n","  return html"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"JhP_FQWjP7Bq","executionInfo":{"status":"ok","timestamp":1615137323134,"user_tz":-60,"elapsed":1655,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"2725bb87-7c53-4759-a9f3-1d4aa7201817"},"source":["wikipedia.page('rendering engine').summary.split('.')[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Rendering or image synthesis is the process of generating a photorealistic or non-photorealistic image from a 2D or 3D model by means of a computer program'"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"id":"w7M28-U5NNSA","executionInfo":{"status":"ok","timestamp":1615137330936,"user_tz":-60,"elapsed":2396,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"6fc94d27-06ad-4bdb-9ea0-ac33b5f7fdfb"},"source":["HTML(get_linked_text(nlp, sentence))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">In \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/3D_computer_graphics' title='3D computer graphics, or three-dimensional computer graphics (in contrast to 2D computer graphics), are graphics that use a three-dimensional representation of geometric data (often Cartesian) that is stored in the computer for the purposes of performing calculations and rendering 2D images'>3D computer</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," graphics, \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/Ray_tracing_(graphics)' title='In 3D computer graphics, ray tracing is a rendering technique for generating an image by tracing the path of light as pixels in an image plane and simulating the effects of its encounters with virtual objects'>ray tracing</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," is a rendering technique for generating an image by tracing the path of light as pixels in an image plane and simulating the effects of its encounters with virtual objects</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"id":"O9aS43pQV9RV","executionInfo":{"status":"ok","timestamp":1615137335766,"user_tz":-60,"elapsed":1900,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"109fd9c5-d29e-4927-cfb3-1366c287bc62"},"source":["sentence1 = 'performing image processing TECH on at least one of the standard scanned image corresponding to the registration information or the standard scanned image of the suspicious object in response to an image processing TECH instruction'\n","HTML(get_linked_text(nlp, sentence1))\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">performing \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/Digital_image_processing' title='Digital image processing is the use of a digital computer to process digital images through an algorithm'>image processing</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," TECH on at least one of the standard scanned image corresponding to the registration information or the standard scanned image of the suspicious object in response to an \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/Digital_image_processing' title='Digital image processing is the use of a digital computer to process digital images through an algorithm'>image processing</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," TECH instruction</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"Cy77cFrVNfw-"},"source":["# print(displacy.render(nlp(g06tpatents[99][:888]), style=\"ent\")) \n","# displacy.render(nlp(g06tpatents[99][999:9999]), style=\"ent\", jupyter=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"xSSjLOlDUp44","executionInfo":{"status":"ok","timestamp":1615137476468,"user_tz":-60,"elapsed":21512,"user":{"displayName":"Phan Ent","photoUrl":"","userId":"16751912121320332077"}},"outputId":"2c0f3d5a-be05-43eb-ca40-839debe52425"},"source":["HTML(get_linked_text(nlp, g06tpatents[99][999:9999]))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">as a whole into the present disclosure.</br>TECHNICAL FIELD</br>The present disclosure relates to the field of security inspection, and in particular to a method, apparatus and system for assisting security inspection.</br>BACKGROUND</br>The use of containers or vehicles for smuggling, for example, smuggling of contraband such as drugs, explosives and even weapons of mass destruction and radioactive dispersion devices, has become an international public nuisance that disturbs governments and interferes with the normal order of international cargo transportation. Cargo/vehicle security inspection is a topic of common concern all over the world. X-\n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/Fluoroscopy' title='Fluoroscopy () is an imaging technique that uses X-rays to obtain real-time moving images of the interior of an object'>ray fluoroscopy</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," imaging technology is the most basic and earliest widely used technology in the field of contraband inspection, and it is still the most widely used inspection technology for containers, cargos and vehicles in the world.</br>X-\n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/Fluoroscopy' title='Fluoroscopy () is an imaging technique that uses X-rays to obtain real-time moving images of the interior of an object'>ray fluoroscopic</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," images differ greatly from natural light images, and X-\n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/Fluoroscopy' title='Fluoroscopy () is an imaging technique that uses X-rays to obtain real-time moving images of the interior of an object'>ray fluoroscopic</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," images of various cargos taken from different angles are not well known to most people. In order to effectively carry out \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/Journal_of_Cell_Biology' title='Journal of Cell Biology is an international, peer-reviewed journal owned by The Rockefeller University and published by Rockefeller University Press'>image screening</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", an \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/Journal_of_Cell_Biology' title='Journal of Cell Biology is an international, peer-reviewed journal owned by The Rockefeller University and published by Rockefeller University Press'>image screening</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," inspector must acquire a large amount of commodity knowledge through experience accumulation or training to have a prejudgment on the X-\n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/Fluoroscopy' title='Fluoroscopy () is an imaging technique that uses X-rays to obtain real-time moving images of the interior of an object'>ray fluoroscopic</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," images of various cargos.</br>SUMMARY</br>According to an aspect of the present disclosure, a method for assisting security inspection is provided, comprising: acquiring registration information of an inspected object; acquiring a standard scanned image corresponding to the registration information; displaying the standard scanned image in an \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/Augmented_reality' title='Augmented reality (AR) is an interactive experience of a real-world environment where the objects that reside in the real world are enhanced by computer-generated perceptual information, sometimes across multiple sensory modalities, including visual, auditory, haptic, somatosensory and olfactory'>Augmented Reality</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," (AR) manner to determine whether the inspected object is a suspicious object through comparing the standard scanned image with an actual scanned image, the actual scanned image comprising an image of the inspected object.</br>In some embodiments, the method further comprises: acquiring at least one of a three-dimensional model or descriptive information corresponding to the registration information; displaying at least one of the three-dimensional model or the descriptive information corresponding to the registration information in an AR manner.</br>In some embodiments, the method further comprises: acquiring an actual scanned image marked with a suspicious area, wherein if the inspected object is a suspicious object, a location at which the image of the inspected object is displayed in the actual \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/Image_scanner' title='An image scanner—often abbreviated to just scanner, is a device that optically scans images, printed text, handwriting or an object and converts it to a digital image'>scanned image</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," is marked as the suspicious area; displaying the actual \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/Image_scanner' title='An image scanner—often abbreviated to just scanner, is a device that optically scans images, printed text, handwriting or an object and converts it to a digital image'>scanned image</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," marked with the suspicious area on a container in which the inspected object is placed in an AR manner.</br>In some embodiments, the method further comprises: displaying at least one of the standard scanned image, a three-dimensional model, or descriptive information corresponding to the registration information on the container in which the inspected object is placed in an AR manner.</br>In some embodiments, the method further comprises: acquiring at least one of a standard scanned image, a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/3D_modeling' title='In 3D computer graphics, 3D modeling is the process of developing a mathematical representation of any surface of an object (inanimate or living) in three dimensions via specialized software'>three-dimensional model</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," or descriptive information of the suspicious object; displaying at least one of the standard scanned image, the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/3D_modeling' title='In 3D computer graphics, 3D modeling is the process of developing a mathematical representation of any surface of an object (inanimate or living) in three dimensions via specialized software'>three-dimensional model</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," or the descriptive information of the suspicious object in an AR manner.</br>In some embodiments, the method further comprises: performing \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/Digital_image_processing' title='Digital image processing is the use of a digital computer to process digital images through an algorithm'>image processing</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," on at least one of the standard scanned image corresponding to the registration information or the standard scanned image of the suspicious object in response to an \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/Digital_image_processing' title='Digital image processing is the use of a digital computer to process digital images through an algorithm'>image processing</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," instruction, and displaying the processed standard scanned image in an AR manner; or performing at least one of a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/Zoom_Video_Communications' title='Zoom Video Communications, Inc'>zooming operation</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," or a rotating operation on at least one of the three-dimensional model corresponding to the registration information or the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/3D_modeling' title='In 3D computer graphics, 3D modeling is the process of developing a mathematical representation of any surface of an object (inanimate or living) in three dimensions via specialized software'>three-dimensional model</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the suspicious object in response to a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/Process_modeling' title='The term process model is used in various contexts'>model processing</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," instruction, and displaying the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/3D_modeling' title='In 3D computer graphics, 3D modeling is the process of developing a mathematical representation of any surface of an object (inanimate or living) in three dimensions via specialized software'>three-dimensional model</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," subjected to the model operation in an AR manner.</br>According to another aspect of the present disclosure, an apparatus for assisting security inspection is provided, comprising: an input interface configured to acquire registration information of an inspected object; a processor configured to acquire a standard scanned image corresponding to the registration information; and a display configured to display the standard scanned image in an AR manner to determine whether the inspected object is a suspicious object through comparing the standard scanned image with an actual scanned image, the actual scanned image comprising an image of the inspected object.</br>In some embodiments, the processor is further configured to acquire at least one of a three-dimensional model or descriptive information corresponding to the registration information; the display is further configured to display at least one of the three-dimensional model or the descriptive information corresponding to the registration information in an AR manner.</br>In some embodiments, the input interface is further configured to acquire the actual \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/Image_scanner' title='An image scanner—often abbreviated to just scanner, is a device that optically scans images, printed text, handwriting or an object and converts it to a digital image'>scanned image</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," marked with a suspicious area, wherein if the inspected object is a suspicious object, a location at which the image of the inspected object is displayed in the actual \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/Image_scanner' title='An image scanner—often abbreviated to just scanner, is a device that optically scans images, printed text, handwriting or an object and converts it to a digital image'>scanned image</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," is marked as the suspicious area; the display is further configured to display the actual \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/Image_scanner' title='An image scanner—often abbreviated to just scanner, is a device that optically scans images, printed text, handwriting or an object and converts it to a digital image'>scanned image</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," marked with the suspicious area on a container in which the inspected object is placed in an AR manner.</br>In some embodiments, the display is further configured to display at least one of the standard scanned image, the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/3D_modeling' title='In 3D computer graphics, 3D modeling is the process of developing a mathematical representation of any surface of an object (inanimate or living) in three dimensions via specialized software'>three-dimensional model</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n",", or the descriptive information corresponding to the registration information on the container in which the inspected object is placed in an AR manner.</br>In some embodiments, the processor is further configured to acquire at least one of a standard scanned image, a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/3D_modeling' title='In 3D computer graphics, 3D modeling is the process of developing a mathematical representation of any surface of an object (inanimate or living) in three dimensions via specialized software'>three-dimensional model</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," or descriptive information of a suspicious object; the display is further configured to display at least one of the standard scanned image, the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/3D_modeling' title='In 3D computer graphics, 3D modeling is the process of developing a mathematical representation of any surface of an object (inanimate or living) in three dimensions via specialized software'>three-dimensional model</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," or the descriptive information of the suspicious object in an AR manner.</br>In some embodiments, the processor is further configured to perform \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/Digital_image_processing' title='Digital image processing is the use of a digital computer to process digital images through an algorithm'>image processing</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," on at least one of the standard scanned image corresponding to the registration information or the standard scanned image of the suspicious object in response to an \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/Digital_image_processing' title='Digital image processing is the use of a digital computer to process digital images through an algorithm'>image processing</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," instruction; the display is further configured to display the processed standard scanned image in an AR manner; or the processor is further configured to performing at least one of a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/Zoom_Video_Communications' title='Zoom Video Communications, Inc'>zooming operation</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," or a rotating operation on at least one of a three-dimensional model corresponding to the registration information or the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/3D_modeling' title='In 3D computer graphics, 3D modeling is the process of developing a mathematical representation of any surface of an object (inanimate or living) in three dimensions via specialized software'>three-dimensional model</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of the suspicious object in response to a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/Process_modeling' title='The term process model is used in various contexts'>model processing</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," instruction; the display is further configured to display the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/3D_modeling' title='In 3D computer graphics, 3D modeling is the process of developing a mathematical representation of any surface of an object (inanimate or living) in three dimensions via specialized software'>three-dimensional model</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," subjected to the model operation in an AR manner.</br>According to still another aspect of the present disclosure, a system for assisting security inspection is provided, comprising: a server, a database and the apparatus for assisting security inspection described above; wherein, the server is configured to store at least one of standard scanned images or \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/3D_modeling' title='In 3D computer graphics, 3D modeling is the process of developing a mathematical representation of any surface of an object (inanimate or living) in three dimensions via specialized software'>three-dimensional models</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n","; the database is configured to store addresses of the standard scanned images and the \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/3D_modeling' title='In 3D computer graphics, 3D modeling is the process of developing a mathematical representation of any surface of an object (inanimate or living) in three dimensions via specialized software'>three-dimensional models</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," in the server, as well as descriptive information corresponding to the registration information and descriptive information of the suspicious objects.</br>According to still another aspect of the present disclosure, a device for assisting security inspection is provided, comprising: a memory; and a processor coupled to the memory, the processor configured to carry out the method for assisting security inspection described above based on instructions stored in the memory.</br>According to still another aspect of the present disclosure, a computer-readable storage medium is provided on which computer program instructions are stored, which when executed by a processor implement the steps of the method for assisting security inspection described above.</br>Other features and advantages of the present disclosure will become clear through detailed descriptions of the illustrative embodiments of the present disclosure with reference to the following drawings.</br>BRIEF DESCRIPTION OF THE DRAWINGS</br>The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate embodiments of the present disclosure and, together with the description, serve to explain the principles of the present disclosure.</br>The present disclosure will be more clearly understood from the following detailed description with reference to the accompanying drawings, in which:</br>FIG 1 is a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/Schematic' title='A schematic, or schematic diagram, is a representation of the elements of a system using abstract, graphic symbols rather than realistic pictures'>schematic flow chart</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of a method for assisting security inspection some embodiments of the present disclosure.FIG 2 is a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/Schematic' title='A schematic, or schematic diagram, is a representation of the elements of a system using abstract, graphic symbols rather than realistic pictures'>schematic flow chart</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of a method for assisting security inspection some other embodiments of the present disclosure.FIG 3 is a \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    <a href='https://en.wikipedia.org/wiki/Schematic' title='A schematic, or schematic diagram, is a representation of the elements of a system using abstract, graphic symbols rather than realistic pictures'>schematic flow chart</a>\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TECH</span>\n","</mark>\n"," of a meth</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"6Zjc1-nST7Fp"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ALjHR5s_c8xQ"},"source":["# Our improvement"]},{"cell_type":"markdown","metadata":{"id":"QBnlCgHgdJmM"},"source":["## Text pre-treatment\n","Here we are cleaning the text by removing all undesirable content and lemmatizing the text"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2vRFrDQilgTI","executionInfo":{"status":"ok","timestamp":1615220054120,"user_tz":-60,"elapsed":3023,"user":{"displayName":"MD Salem Messoud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBYoVQGAImGxJfhgMMK1daBThGNpiaASkBXr4EYpE=s64","userId":"15534871031717137879"}},"outputId":"f686723e-d75b-4e7b-b59d-fb2c606de107"},"source":["import nltk\n","import numpy as np\n","\n","from nltk import WordNetLemmatizer\n","nltk.download('brown')\n","nltk.download('names')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('universal_tagset')\n","nltk.download('stopwords')\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/brown.zip.\n","[nltk_data] Downloading package names to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/names.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n","[nltk_data]   Unzipping taggers/universal_tagset.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-soIg_W7eVlD","executionInfo":{"status":"ok","timestamp":1615220359120,"user_tz":-60,"elapsed":223892,"user":{"displayName":"MD Salem Messoud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBYoVQGAImGxJfhgMMK1daBThGNpiaASkBXr4EYpE=s64","userId":"15534871031717137879"}},"outputId":"a3528fc9-f770-4fa6-f341-c3b83b5bf013"},"source":["import string\n","import nltk\n","from nltk.corpus import stopwords\n","stop_words = set(stopwords.words('english'))\n","g06tpatents_normalized = []\n","for ind in range(len(g06tpatents)):\n","  try:\n","    parag = g06tpatents[ind]\n","    parag = parag.lower()\n","    tokens = word_tokenize(parag)\n","    tokens = [i for i in tokens if not i in stop_words]\n","    wordnet_lemmatizer = WordNetLemmatizer()\n","      # vectorizing function to able to call on list of tokens\n","    lemmatize_words = np.vectorize(wordnet_lemmatizer.lemmatize)\n","    lemmatized_text = ' '.join(lemmatize_words(tokens))\n","\n","      #final_text = normalise(nltk.word_tokenize(lemmatized_text), verbose=False)\n","      #array.append( ' '.join(lemmatize_words(final_text)) )\n","    g06tpatents_normalized.append(lemmatized_text)\n","  except :\n","      pass\n","  if ind%500 == 0:\n","    print(ind)\n","print(len(g06tpatents_normalized))\n","print(g06tpatents_normalized[:5])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0\n","500\n","1000\n","1500\n","1998\n","[\"system method optimization video bitrate _____2019_____3503561_____487586047_____ep3500000.txt_____g06t_____g06t7/194 : h04n21/23412 : h04n21/234345 : h04n21/2353 : h04n21/2365 : h04n21/23655 : h04n21/2662 : h04n21/44012 : h04n21/440245 : h04n21/4728 : h04n21/8456 method optimization video bitrate given content , method comprising step : receiving ( 401 ) background data stream ( 102 ) , lowest bitrate within set available stream given content created compressing complete source content ( 100 ) highest bitrate ; starting reception ( 402 ) foreground data stream ( 101 ) wherein video content said foreground data stream ( 101 ) cover part complete source content ( 100 ) higher bitrate said lowest bitrate ; verifying ( 403 ) whether possible receive present foreground data stream ( 101 ) within predefined threshold delay ; presenting ( 404 ) blended foreground data stream ( 101 ) top background data stream ( 102 ) . _____d : technical field present invention relates system method video content compression streaming via broadband communication channel internet . present invention address known problem reducing content bit-rate preserving best possible quality content . background invention existing solution provide method reducing bit-rate ( number bit conveyed processed per unit time , typically 1 second ) applying mainly video content , bit-rates relatively high much room improvement respect data compression . solution attempt reduction bit-rate whole video stream using various motion picture compression method e.g . utilizing mpeg-2 h.264 hevc like . solution allow one video stream , generated , complete source content ( resulting reduced bitrate , source content ) , available receiver select depending condition available communication link . stream utilize high bit-rate utilize low bit-rate . receiver may select present low bit-rate stream said condition poor high bit-rate stream said condition good . method used popular standard mpeg-dash ( dynamic adaptive streaming http ) similar http live streaming ( hl ) solution known prior art . aim present invention optimization video data compression order achieve lower bit-rates maintaining subjective content quality . summary object present invention object present invention method optimization video bitrate given content , method characterized comprises step : receiving background data stream , lowest bitrate within set available stream given content created compressing complete source content highest bitrate ; starting reception foreground data stream wherein video content said foreground data stream cover part complete source content higher bitrate said lowest bitrate ; verifying whether possible receive present foreground data stream within predefined threshold delay : blending foreground data stream top background data stream . preferably , method comprises step : starting reception complete source content ; verifying whether possible receive present complete source content within predefined threshold delay ; outputting complete source content . preferably , method comprises step monitoring connection reception parameter case possible receive present complete source content within predefined threshold delay , returning first verification step . preferably , available supplementary data stream , addition background data stream foreground data stream , tested retrieval , retrieved presentation according blending information data wherein supplementary data stream cover part complete source content . preferably , blending information retrieved request receiver available via mpeg private section wherein said blending information comprises sequence blending said stream . preferably , said blending information comprises information alpha channel determining opacity pixel group . another object present invention computer program comprising program code mean performing step computer-implemented method according present invention said program run computer . another object present invention computer readable medium storing computer-executable instruction performing step computer-implemented method according present invention executed computer . another object present invention encoder optimization video bitrate given content , encoder characterized comprises : reception mean configured receive complete source content buffer memory buffer according specified parameter ; segmentation module configured create background data stream background stream generator foreground stream foreground stream generator ; wherein background stream , lowest bitrate within set available stream given content created compressing complete source content highest bitrate foreground stream wherein content cover part complete source content higher bitrate said lowest bitrate ; multiplexer configured deliver selected data stream receiver according requirement communicated encoder via communication mean ; wherein controller configured control said reception mean , said segmentation module said multiplexer . preferably , one supplementary stream segmented complete source content supplementary stream generator wherein supplementary data stream cover part complete source content . preferably , segmentation module configured generate blending information identifying sequence available stream defined order . preferably , wherein blending information provided via mpeg private section . brief description drawing object invention presented herein , accomplished providing system method optimization video bitrate . detail feature present invention , nature various advantage become apparent following detailed description preferred embodiment shown drawing , : fig 1 present original high bit rate high quality video stream ( represented single frame ) ; fig 2 present stream fig1 transformed foreground background ( represented single frame ) ; fig 3 present foreground background blended presentation ( represented single frame ) ; fig 4 present method selection stream stream presentation ; fig 5 present time diagram showing selection stream ; andfig 6 depicts system according present invention . notation nomenclature portion detailed description follows presented term data processing procedure , step symbolic representation operation data bit performed computer memory . therefore , computer executes logical step thus requiring physical manipulation physical quantity . usually quantity take form electrical magnetic signal capable stored , transferred , combined , compared , otherwise manipulated computer system . reason common usage , signal referred bit , packet , message , value , element , symbol , character , term , number , like . additionally , similar term associated appropriate physical quantity merely convenient label applied quantity . term `` processing '' `` creating '' `` transferring '' `` executing '' `` determining '' `` detecting '' `` obtaining '' `` selecting '' `` calculating '' `` generating '' like , refer action process computer system manipulates transforms data represented physical ( electronic ) quantity within computer 's register memory data similarly represented physical quantity within memory register information storage . computer-readable ( storage ) medium , referred herein , typically may non-transitory and/or comprise non-transitory device . context , non-transitory storage medium may include device may tangible , meaning device concrete physical form , although device may change physical state . thus , example , non-transitory refers device remaining tangible despite change state . utilized herein , term `` example '' mean serving non-limiting example , instance , illustration . utilized herein , term `` example '' `` e.g . '' introduce list one non-limiting example , instance , illustration . description embodiment present invention relates mainly video content streamed via communication link i.e . sent subsequently presented . however , become apparent , one skilled art , may also applied relate bit stream sent two point via variety medium . method disclosed herein comprises following general step : division input data stream least two component : background foreground component making component available receiver ; andselection least one component transmit blend presentation selection performed receiver andtransmitting selected component recipient/receiver ; andblending ( according predefined tailored method ) selected component presentation . present invention best described exemplary embodiment follow . one exemplary embodiment present invention method optimization bit rate video stream comprising moving picture . method allows transforming given video stream least two video stream . first least two video stream called foreground hereinafter second least two video stream called background hereinafter . remaining stream may called supplementary stream . stream encoded using least one selected bit rate i.e . encoded manner target bit rate stream substantially close selected bit rate . skilled person notice different method , e.g . different codecs , may used achieve goal . skilled person also appreciate selected bit rate may result different quality resulting stream depending method e.g . codec used . nevertheless , quality may subjective bitrate objective term , make use universal . exemplary content video data stream ( 100 ) , highest bitrate within set processed data stream ( 100 , 101 , 102 ) , depicted fig 1. original , complete high bit rate stream ( 100 ) encoded using high target bit rate ( e.g . full hd video may bit rate 5mb/s ) , meaning also requires high quality connection transmitter receiver , sent presented without delay . stream transformed two data stream ( 101 ) ( 102 ) example . stream ( 102 ) stream original high bit rate stream ( 100 ) encoded using low ( e.g . 500kb/s lower ) target bit rate also meaning low quality/bandwidth connection transmitter receiver sufficient sent presented without delay . data stream ( 102 ) comprises background . present embodiment present background whole stream . caused background data stream ( 102 ) selectable purpose presentation low quality connection purpose blending data stream ( 101 ) defined . however , skilled person appreciate present method may adapted situation additional background stream available cover area presentation covered data stream ( 101 ) . aforementioned high quality connection mean connection possible transmit high bit rate stream ( 100 ) without delay presentation ( e.g . within predefined threshold delay ) . exemplary value high bit rate 5mb/s mobile lte , typically enough transmit full hd video encoded using h.264 codec . term without delay understood allowing presentation content within predefined threshold delay ( typical given device , e.g . local buffering period ) presentation begun , disturbed - meaning stopped paused deformed like due low quality connection . aforementioned low quality connection mean connection possible transmit high bit rate stream ( 100 ) without delay presentation . exemplary value 1mb/s mobile lte typically enough transmit low resolution e.g . 576i video enough e.g . aforementioned full hd video . data stream ( 101 ) sub-stream original high bit rate stream ( 100 ) , meaning data representing video covering part original video high bit rate stream ( 100 ) included stream ( 101 ) . typically , data include first plan motion picture contrasting element like . way limiting present invention foreground selected . may arbitrary shape adjusted per frame , per object present video content ( e.g . car , person etc . ) like . different segmentation technique may employed order select foreground otherwise key object given video frame sequence frame . one exemplary segmentation technique arbitrary rectangular area identified human active considered motion picture . another exemplary segmentation technique selection circular area per frame inside circular area contains portion frame highest contrast value . skilled person notice stream ( 101 ) keep original quality target bit rate lower one original high bit rate stream ( 100 ) . due fact stream ( 101 ) contains substantially le data stream ( 100 ) . therefore , typically utilize lower bit rate encoding technique used . hence , stream ( 101 ) requires connection substantially lower bandwidth send present without delay original high bit rate stream ( 100 ) . final stream may blended directly presentation way single frame first background drawn foreground drawn top background . provides benefit presenting key element motion picture high quality reducing overall bit rate needed send content stream . skilled person appreciate possible select foreground target bit rate foreground ( 101 ) background ( 102 ) data stream following statement true : bit rate background ( 102 ) preferably substantially lower bit rate original high bit rate stream ( 100 ) ; bit rate foreground ( 101 ) preferably close bit rate original high bit rate stream ( 100 ) bit rate foreground substantially higher bit rate background ( 102 ) .the combined bit rate stream ( 101 ) ( 102 ) preferably substantially lower bit rate original high bit rate stream ( 100 ) . another embodiment present invention method according first embodiment foreground data stream ( 101 ) carry additional blending information . exemplary blending information comprises alpha channel determining opacity pixel group like . information taken account blending final picture presentation . typically , alpha channel used make foreground partially opaque order smooth transition area foreground background . blending information may retrieved request sent every receiver , example mpeg private section . another extension , aforementioned alpha channel , information sequence blending stream . case , transmitter/encoder device includes blending sequence stream receiver device may use information blend subset available stream defined order ( example according bitrate ) . another embodiment present invention method ( 400 ) according fig 4. method may used adaptively select available stream sent transmitter presented , receiver , order optimally use available connection transmitter receiver . method variation widely known mpeg-dash standard ( iso/iec 23009-1:2012 ) . available stream assumed prepared according previous embodiment . method ( 400 ) begin step ( 401 ) reception subsequent presentation background data stream ( 102 ) . background data stream ( 102 ) may retrieved based information present said blending information data retrieved based information available content . background ( 102 ) stream smallest bit rate ( within set available stream given content ) therefore probable received presented without delay . next , step ( 402 ) method start reception foreground data stream ( 101 ) . stream presented step . , step ( 403 ) , verified whether possible receive present foreground data stream ( 101 ) without delay ( otherwise whether possible within predefined threshold delay ) , method move step ( 404 ) . otherwise , method move step ( 402 ) keep checking possibility present foreground data stream ( 101 ) . subsequently , step ( 404 ) , method present blended foreground data stream ( 101 ) top background ( 102 ) . next , reception original high bit rate stream ( 100 ) started . skilled person appreciate although would enough know quantitative quality connection e.g . speed determine reception high bit rate stream possible , better start reception thereof provides desired information 'in place ' i.e . mere action reception stream receiver device know whether speed reception sufficient . method move step ( 406 ) , , similarly step ( 403 ) , verified whether possible receive present original high bit rate stream ( 100 ) without delay . possible , method move step ( 407 ) present ( otherwise output presentation ) original high bit rate stream ( 100 ) . , step ( 406 ) , possible receive present original high bit rate stream ( 100 ) , i.e . base stream , without delay , method move step ( 403 ) continue monitoring possibility presentation . step ( 407 ) , method move step ( 406 ) order continue monitoring connection ( reception parameter ) possibly fall back lower bit rate respective condition step ( 406 ) ( 403 ) fail . skilled person appreciate detail described method may changed without limiting scope present invention . example , step ( 401 ) method ( 400 ) may select different configuration e.g . one step ( 404 ) method . skilled person easily adopt present method new first step . skilled person also appreciate level division transformation stream possible e.g . foreground level may created . foreground level may overlap background may created combination thereof may used adopt cumulative bit rate blended stream best use available connection quality . fig 5 present time diagram showing selection stream according present invention . figure present exemplary time diagram depicting quality reception stream . line ( 500 ) show reception quality ( particular quality may considered throughput ) change time . apparent , one skilled art , unit ax scale diagram i.e . quantitative dependency relevant qualitative dependency presented therein . exemplary data reception bitrate flow ( 500 ) diagram depicts time span , begin step ( 401 ) method ( 400 ) end step ( 405 ) method ( 400 ) . time span b depicts time presentation blended stream ( 101 ) ( 102 ) time span c depicts time reception quality good enough present original high bit rate stream ( 100 ) . typically , implementation method ( 400 ) attempt minimise time span b , allowed reception quality . time span depicts time quality reception drop extent presentation original high bit rate stream ( 100 ) possible , thus method present blended stream ( 101 ) ( 102 ) . time span e depicts time reception quality high original high bit rate stream ( 100 ) may presented . diagram fig 5 show advantage provided present invention instead switching presentation low quality stream ( 102 ) , presentation retained high quality stream ( 101 ) important area foreground . optionally , available supplementary data stream , addition background data stream ( 102 ) foreground data stream ( 101 ) , tested retrieval , retrieved presentation according blending information data . clear supplementary data stream verification retrieval may added method fig 4. similarly , foreground data stream ( 101 ) , supplementary data stream cover part complete source content ( 100 ) . preferably , distinct part overlap , overlapping allowed . case , blending information data may comprise order data stream testing retrieval , may organized according different technique sorting according bitrate increment two data stream . fig 6 depicts system according present invention , managed , transmission side controller ( 601 ) . system may comprise encoder general receiver ( 608 ) . encoder receives original high bit rate stream ( 100 ) via suitable reception mean ( 602 ) buffer memory buffer ( 603 ) according specified parameter . , controller ( 601 ) may instruct segmentation module ( 609 ) create background data stream ( 102 ) background stream generator ( 604 ) foreground stream ( 101 ) foreground stream generator ( 605 ) according aforementioned description . additionally , one supplementary stream may segmented original high bit rate stream ( 100 ) supplementary stream ( ) generator ( 606 ) . segmentation module ( 609 ) may also generate aforementioned blending information ( 610 ) since aware applied segmentation . multiplexer ( 607 ) configured deliver selected data stream receiver ( 608 ) according requirement communicated controller ( 601 ) via suitable communication mean . least part method according invention may computer implemented . accordingly , present invention may take form entirely hardware embodiment , entirely software embodiment ( including firmware , resident software , micro-code , etc . ) embodiment combining software hardware aspect may generally referred herein `` circuit '' , `` module '' `` system '' . furthermore , present invention may take form computer program product embodied tangible medium expression computer usable program code embodied medium . easily recognized , one skilled art , aforementioned method optimization video bitrate may performed and/or controlled one computer program . computer program typically executed utilizing computing resource computing device . application stored non-transitory medium . example non-transitory medium nonvolatile memory , example flash memory example volatile memory ram . computer instruction executed processor . memory exemplary recording medium storing computer program comprising computer-executable instruction performing step computer-implemented method according technical concept presented herein . invention presented herein depicted , described , defined reference particular preferred embodiment , reference example implementation foregoing specification imply limitation invention . , however , evident various modification change may made thereto without departing broader scope technical concept . presented preferred embodiment exemplary , exhaustive scope technical concept presented herein . accordingly , scope protection limited preferred embodiment described specification , limited claim follow . _____c : 1. method optimization video bitrate given content , method characterized comprises step : • receiving ( 401 ) background data stream ( 102 ) , lowest bitrate within set available stream given content created compressing complete source content ( 100 ) highest bitrate ; • starting reception ( 402 ) foreground data stream ( 101 ) wherein video content said foreground data stream ( 101 ) cover part complete source content ( 100 ) higher bitrate said lowest bitrate ; • verifying ( 403 ) whether possible receive present foreground data stream ( 101 ) within predefined threshold delay : ∘ blending ( 404 ) foreground data stream ( 101 ) top background data stream ( 102 ) . 2. method according claim 1 wherein method comprises step : • starting reception complete source content ( 100 ) ; • verifying ( 406 ) whether possible receive present complete source content ( 100 ) within predefined threshold delay ; • outputting ( 407 ) complete source content ( 100 ) . 3. method according claim 1 wherein method comprises step : • monitoring connection reception parameter case possible receive present complete source content ( 100 ) within predefined threshold delay , returning first verification step ( 403 ) . 4. method according claim 1 wherein available supplementary data stream , addition background data stream ( 102 ) foreground data stream ( 101 ) , tested retrieval , retrieved presentation according blending information data wherein supplementary data stream cover part complete source content ( 100 ) . 5. method according claim 1 wherein blending information retrieved request receiver available via mpeg private section wherein said blending information comprises sequence blending said stream . 6. method according claim 1 wherein said blending information comprises information alpha channel determining opacity pixel group . 7. computer program comprising program code mean performing step computer-implemented method according claim 1 said program run computer . 8. computer readable medium storing computer-executable instruction performing step computer-implemented method according claim 1 executed computer . 9. encoder optimization video bitrate given content , encoder characterized comprises : • reception mean ( 602 ) configured receive complete source content ( 100 ) buffer memory buffer ( 603 ) according specified parameter ; • segmentation module ( 609 ) configured create background data stream ( 102 ) background stream generator ( 604 ) foreground stream ( 101 ) foreground stream generator ( 605 ) ; ∘ wherein background stream ( 102 ) , lowest bitrate within set available stream given content created compressing complete source content ( 100 ) highest bitrate foreground stream ( 101 ) wherein content cover part complete source content ( 100 ) higher bitrate said lowest bitrate ; • multiplexer ( 607 ) configured deliver selected data stream receiver ( 608 ) according requirement communicated encoder via communication mean ; • wherein controller ( 601 ) configured control said reception mean ( 602 ) , said segmentation module ( 609 ) said multiplexer ( 607 ) . 10. encoder according claim 9 wherein one supplementary stream segmented complete source content ( 100 ) supplementary stream generator ( 606 ) wherein supplementary data stream cover part complete source content ( 100 ) . 11. encoder according claim 10 wherein segmentation module ( 609 ) configured generate blending information ( 610 ) identifying sequence available stream defined order . 12. encoder according claim 11 wherein blending information provided via mpeg private section .\", \"apparatus , method computer program processing piecewise-smooth signal _____2019_____3503016_____487586276_____ep3500000.txt_____g06t_____g06t2207/20076 : g06t2207/20192 : g06t5/002 apparatus comprising least one processor , least one memory including computer program code , wherein least one memory computer program code configured , least one processor , : receive input signal , input signal comprising piecewise-smooth signal noise signal ; determine prior probability density function representation piecewise-smooth signal , representation piecewise-smooth signal comprising combination smooth , gaussian , variable jump , cauchy , variable ; determine likelihood function based input signal ; determine posterior probability density function based likelihood function prior probability density function distribution ; estimate piecewise-smooth signal based estimate posterior probability density function . _____d : field invention present application relates apparatus , method computer program signal processing . present application specifically relates signal processing piecewise-smooth signal . background several important signal categorized piecewise-smooth signal . piecewise-smooth signal type signal include sharp jump separate smoother region . example 1d piecewise-smooth signal electro-cardiograph ( ecg ) signal may comprise 'jump ' region smoother region repeat beat . higher dimensional example piecewise-smooth signal include 2d image data . technique piecewise-smooth signal noise reduction known . example image reconstruction ( signal 2d piecewise-smooth image ) , jump signal ( edge image ) traditionally processed using process using total variation ( tv ) regularization reduce noise . approach use cauchy difference prior method carry similar edge-preserving reconstruction . summary first aspect provided : apparatus comprising least one processor , least one memory including computer program code , wherein least one memory computer program code configured , least one processor , : receive input signal , input signal comprising piecewise-smooth signal noise signal ; determine prior probability density function representation piecewise-smooth signal , representation piecewise-smooth signal comprising combination smooth , gaussian , variable jump , cauchy , variable ; determine likelihood function based input signal ; determine posterior probability density function based likelihood function prior probability density function distribution ; estimate piecewise-smooth signal based estimate posterior probability density function . processor configured determine prior probability density function representation piecewise-smooth signal , representation piecewise-smooth signal comprising combination smooth , gaussian , variable jump , cauchy , variable may configured : determine first probability density function representing sample difference smooth , gaussian , variable ; determine second probability density function representing sample difference jump , cauchy , variable ; convolve first probability density function second probability density function generate prior probability density function . processor configured convolve first probability density function second probability density function generate prior probability density function may configured determine : analytical determination using least one imaginary error function dawson function ; numerical determination using grid sample point interpolation point grid sample point . processor configured estimate piecewise-smooth signal based estimate posterior probability density function may configured estimate piecewise-smooth signal based maximum posteriori estimate posterior probability density function . processor configured estimate piecewise-smooth signal based maximum posteriori estimate posterior probability density function may configured minimise combination logarithm likelihood function logarithm prior probability density function . processor configured minimise combination logarithm likelihood function logarithm prior probability density function may configured apply one : newton 's method ; barzilai-borwein method . processor configured determine posterior probability density function based likelihood function prior probability density function may configured determine posterior probability density function based product likelihood function prior probability density function according bayes ' theorem . processor may configured window measured input signal . smooth , gaussian , variable jump , cauchy , variable may independent . according second aspect provided method comprising : receiving input signal , input signal comprising piecewise-smooth signal noise signal ; determining prior probability density function representation piecewise-smooth signal , representation piecewise-smooth signal comprising combination smooth , gaussian , variable jump , cauchy , variable ; determining likelihood function based input signal ; determining posterior probability density function based likelihood function prior probability density function distribution ; estimating piecewise-smooth signal based estimate posterior probability density function . determining prior probability density function representation piecewise-smooth signal , representation piecewise-smooth signal comprising combination smooth , gaussian , variable jump , cauchy , variable may comprise : determining first probability density function representing sample difference smooth , gaussian , variable ; determining second probability density function representing sample difference jump , cauchy , variable ; convolving first probability density function second probability density function generate prior probability density function . convolving first probability density function second probability density function generate prior probability density function may comprise : analytical determination using least one imaginary error function dawson function ; numerical determination using grid sample point interpolation point grid sample point . estimating piecewise-smooth signal based estimate posterior probability density function may comprise estimating piecewise-smooth signal based maximum posteriori estimate posterior probability density function . estimating piecewise-smooth signal based maximum posteriori estimate posterior probability density function may comprise minimising combination logarithm likelihood function logarithm prior probability density function . minimising combination logarithm likelihood function logarithm prior probability density function may comprise applying one : newton 's method ; barzilai-borwein method . determining posterior probability density function based likelihood function prior probability density function may comprise determining posterior probability density function based product likelihood function prior probability density function according bayes ' theorem . method may comprise windowing input signal . smooth , gaussian , variable jump , cauchy , variable may independent . according third aspect provided apparatus comprising : mean receiving input signal , input signal comprising piecewise-smooth signal noise signal ; mean determining prior probability density function representation piecewise-smooth signal , representation piecewise-smooth signal comprising combination smooth , gaussian , variable jump , cauchy , variable ; determining likelihood function based input signal ; mean determining posterior probability density function based likelihood function prior probability density function distribution ; estimating piecewise-smooth signal based estimate posterior probability density function . mean determining prior probability density function representation piecewise-smooth signal , representation piecewise-smooth signal comprising combination smooth , gaussian , variable jump , cauchy , variable may comprise : mean determining first probability density function representing sample difference smooth , gaussian , variable ; mean determining second probability density function representing sample difference jump , cauchy , variable ; mean convolving first probability density function second probability density function generate prior probability density function . mean convolving first probability density function second probability density function generate prior probability density function may comprise : mean performing analytical determination using least one imaginary error function dawson function ; mean numerical determination using grid sample point interpolation point grid sample point . mean estimating piecewise-smooth signal based estimate posterior probability density function may comprise mean estimating piecewise-smooth signal based maximum posteriori estimate posterior probability density function . mean estimating piecewise-smooth signal based maximum posteriori estimate posterior probability density function may comprise mean minimising combination logarithm likelihood function logarithm prior probability density function . mean minimising combination logarithm likelihood function logarithm prior probability density function may comprise mean applying one : newton 's method ; barzilai-borwein method . mean determining posterior probability density function based likelihood function prior probability density function may comprise mean determining posterior probability density function based product likelihood function prior probability density function according bayes ' theorem . apparatus may comprise mean windowing input signal . smooth , gaussian , variable jump , cauchy , variable may independent . aspect provided computer program comprising program code mean adapted perform step second aspect program run data processing apparatus . brief description drawing assist understanding present disclosure show embodiment may put effect , reference made way example accompanying drawing : figure 1a schematically show example waveform jump smoother region ; figure 1b schematically show apparatus suitable implementing embodiment ; figure 1c schematically show example system suitable implementing apparatus shown figure 1b according embodiment ; figure 2 schematically show example prior probability density function determiner shown figure 1c detail according embodiment ; figure 3 schematically show example posterior probability density function determiner implemented figure 1c according embodiment ; figure 4 schematically show example point estimator implemented figure 1c according embodiment ; figure 5 show flow chart showing operation example system according embodiment ; figure 6 show flow chart showing operation example prior probability density function determiner according embodiment ; andfigure 7 show flow chart showing operation example posterior density function determiner point estimator according embodiment . detailed description example disclosed application applicable apparatus , method processing piecewise-smooth signal , i.e . signal jump smoother region . respect figure 1a shown example signal 1 3. signal 1 , 3 piecewise-smooth signal comprise smoother region 11 , 17 jump 13 , 15. jump prominent signal feature deviate smoother region signal . peak ecg signal , r-peak , example jump . jump may step-function like shape . shape possible . jump may abrupt sudden start end point . specifically concept concern estimation unknown piecewise-smooth signal u ( ) measurement ( ) characterized using observation equation ( ) =a ( u ( ) ) +e ( ) , function e ( ) signal originating noise process . linear equation matrix ( discretized form ) . example application signal processing described herein one denoising measured ecg signal . embodiment chosen identity operator measurement form ( ) =u ( ) +e ( ) . case , problem estimate signal u ( ) noise corrupted measurement ( ) . example processing may deconvolution 1d ( 2d higher dimension ) signal ( deblurring/sharpening image ) chosen convolution operator , image restoration painting . concept discussed detail hereafter feature embodiment process input signal using signal presented sum ( gaussian ) smooth variable ( cauchy distributed ) variable representing jump considered statistically method ( apparatus ) determine combined distribution sum without estimation variable . embodiment therefore produce solution simple computationally effective ( thus processor based implementation low cpu memory requirement ) . respect figure 1b , example apparatus 100 system suitable implementing embodiment shown . apparatus 100 comprises memory 102 processor 104. apparatus 100 also comprises transceiver 106 receiving information signal analysed apparatus device . embodiment transceiver 106 configured receive user input , example input starting analysis changing statistical variable described detail herein . transceiver 106 may also configured transmit information e.g . processed signal information device remote display . transceiver 106 may configured use suitable protocol coding system transmitting and/or receiving information . furthermore , although transceiver 106 shown connected wireless antenna 107 communicating wirelessly apparatus device , embodiment transceiver 106 configured communicate apparatus device least partially wired connection . embodiment apparatus 100 also comprises , communication , display 108 enables apparatus 100 provide output visual fashion interpreted user viewer . example apparatus 100 may cause visual information provided display 108 viewed user . embodiment apparatus may furthermore comprise signal source sensor providing signal analysed directly camera imaging apparatus suitable generating image signal processed , ecg sensor providing ecg signal etc . example , apparatus 100 processor 104 memory 102 may configured execute program order enable function processing described hereafter operate . although example apparatus 100 shown single device thus operates single centralised apparatus embodiment , functionality described hereafter may enabled distributed system apparatus communicating . example functionality may enabled cloud based system wherein apparatus server physically logically separated . embodiment processor 104 configured implement posterior probability density function algorithm point estimation algorithm based input signal function discussed detail hereafter . following example concept explained respect specific example 1d signal . however understood process described hereafter may applied generally 2d ( example image data ) higher dimension signal piecewise-smooth signal would understood higher dimension signal may interpreted series statistically independent 1d signal . respect figure 1c example apparatus implementing embodiment shown . apparatus comprises input digital input signal ( ) 200. apparatus comprises window filter 201 , prior probability density function determiner 203 , posterior probability density function determiner 205 point estimator 207. window filter 201 configured receive digital input signal ( ) 200 generate sequence windowed sample period comprising windowed signal m= ( m0 , ... , mn ) 210 sample length n+1 . embodiment window filter 201 configured produce sequence separate therefore succeeding proceeding window share sample . embodiment window filter 201 configured produce least partially overlapping sample sequence . windowed signal 210 sample length suitable sample length . window filter 201 embodiment configured pas windowed signal 210 posterior probability density function determiner 205. embodiment apparatus comprises prior probability density function ( pdf ) determiner 203. prior probability density function ( pdf ) determiner 203 may implemented hardware special purpose circuit , software , logic combination thereof . furthermore aspect prior probability density function ( pdf ) determiner 203 may implemented hardware , aspect may implemented firmware software may executed controller , microprocessor computing device , although invention limited thereto . various aspect prior probability density function ( pdf ) determiner 203 may implemented , non-limiting example , hardware , software , firmware , special purpose circuit logic , general purpose hardware controller computing device , software module ( e.g . library function ) , physical circuitry ( e.g . asic ) , programmed microprocessor combination thereof . prior probability density function ( pdf ) determiner 203 may thus implemented computer software executable least one data processor , processor entity , hardware , combination software hardware . prior probability density function ( pdf ) determiner 203 configured generate prior pdf 220 based statistical determination wherein signal modelled sum two random signal . generated prior pdf 220 passed posterior probability density function determiner 205. posterior probability density function determiner 205 may also implemented hardware special purpose circuit , software , logic combination thereof . furthermore aspect posterior probability density function determiner 205 may implemented hardware , aspect may implemented firmware software may executed controller , microprocessor computing device , although invention limited thereto . various aspect posterior probability density function determiner 205 may implemented , non-limiting example , hardware , software , firmware , special purpose circuit logic , general purpose hardware controller computing device , software module ( e.g . library function ) , physical circuitry ( e.g . asic ) , programmed microprocessor combination thereof . posterior probability density function determiner 205 may thus implemented computer software executable least one data processor , processor entity , hardware , combination software hardware . posterior probability density function determiner 205 configured receive windowed signal 210 furthermore prior pdf using bayesian method determine posterior probability density function 230 , denoted p ( u|m ) , window signal 210 based prior pdf 220. determined posterior probability density function 230 passed point estimator 207. point estimator 207 also may implemented hardware special purpose circuit , software , logic combination thereof . furthermore aspect point estimator 207 may implemented hardware , aspect may implemented firmware software may executed controller , microprocessor computing device , although invention limited thereto . various aspect point estimator 207 may implemented , non-limiting example , hardware , software , firmware , special purpose circuit logic , general purpose hardware controller computing device , software module ( e.g . library function ) , physical circuitry ( e.g . asic ) , programmed microprocessor combination thereof . point estimator 207 may thus implemented computer software executable least one data processor , processor entity , hardware , combination software hardware . point estimator 207 , configured receive posterior probability density function 230 , configured distinguish piecewise-smooth signal noise component . estimated signal point may output . respect figure 2 example prior probability density function determiner 203 shown detail . prior probability density function determiner 203 embodiment configured receive lambda λ sigma σ input control determination random variable based smooth ( gaussian ) function jump ( cauchy ) function . prior probability density function determiner 203 comprise smooth ( gaussian ) function variable determiner 211 , jump ( cauchy ) function variable determiner 213 convolution determiner 215. smooth ( gaussian ) function variable determiner 211 may implemented hardware special purpose circuit , software , logic combination thereof . furthermore aspect smooth ( gaussian ) function variable determiner 211 may implemented hardware , aspect may implemented firmware software may executed controller , microprocessor computing device , although invention limited thereto . various aspect smooth ( gaussian ) function variable determiner 211 may implemented , non-limiting example , hardware , software , firmware , special purpose circuit logic , general purpose hardware controller computing device , software module ( e.g . library function ) , physical circuitry ( e.g . asic ) , programmed microprocessor combination thereof . smooth ( gaussian ) function variable determiner 211 may thus implemented computer software executable least one data processor , processor entity , hardware , combination software hardware . smooth ( gaussian ) function variable determiner 211 configured supply suitable pdf smooth function x based sigma σ input , example using following expression : px=pxx=∏k=1n12πσ2e−xk−xk−122σ2 , xk ( k = 0 , ... . , n ) value x discretization point . pdf p ( x ) may passed convolution 215. jump ( cauchy ) function variable determiner 213 may implemented hardware special purpose circuit , software , logic combination thereof . furthermore aspect jump ( cauchy ) function variable determiner 213 may implemented hardware , aspect may implemented firmware software may executed controller , microprocessor computing device , although invention limited thereto . various aspect jump ( cauchy ) function variable determiner 213 may implemented , non-limiting example , hardware , software , firmware , special purpose circuit logic , general purpose hardware controller computing device , software module ( e.g . library function ) , physical circuitry ( e.g . asic ) , programmed microprocessor combination thereof . jump ( cauchy ) function variable determiner 213 may thus implemented computer software executable least one data processor , processor entity , hardware , combination software hardware . jump ( cauchy ) function variable determiner 213 configured supply suitable pdf jump function based lambda λ input , example using following expression : py=pyy=∏k=1nλπλ2+yk−yk−12 , yk ( k = 0 , ... . , n ) value discretization point . pdf p ( ) may passed convolution determiner 215. convolution determiner 215 may implemented hardware special purpose circuit , software , logic combination thereof . furthermore aspect convolution determiner 215 may implemented hardware , aspect may implemented firmware software may executed controller , microprocessor computing device , although invention limited thereto . various aspect convolution determiner 215 may implemented , non-limiting example , hardware , software , firmware , special purpose circuit logic , general purpose hardware controller computing device , software module ( e.g . library function ) , physical circuitry ( e.g . asic ) , programmed microprocessor combination thereof . convolution determiner 215 may thus implemented computer software executable least one data processor , processor entity , hardware , combination software hardware . convolution determiner 215 configured receive smooth p ( x ) jump p ( ) pdfs configured convolve determine pdf sum smooth x jump variable . convolution determiner 215 example may calculate convolution follows : pu=puu=∏k=1nfxk−xk−1 f convolution gaussian p ( x ) cauchy p ( ) marginal density example convolution calculated : fu=eu+iλ22σ21+e2iuλσ21−ierfiu−iλ2σ+ierfiu+iλ2σ22πσ2 erfi ( z ) imaginary error function , imaginary error function entire function defined erfi ( z ) ≡ -ierf ( z ) . another example , convolution determiner 215 may embodiment use dawson function f express result convolution : fu∝12πe−u−iλ22σ2+e−u+iλ22σ2−i fu−iλ2σ+i fu+iλ2σ , dawson function f defined fx=e−x2∫0xey2dy=12πe−x2erfix . determined convolution , prior probability density function p ( u ) may output prior probability density function determiner 203. embodiment prior probability density function determiner 203 may comprise set pre-determined parameter passed posterior probability density function determiner 205. embodiment , prior probability density function implemented look-up-table value accessed via pre-configured parameter value λ σ. word embodiment prior probability density function may implemented pre-determined value within memory . embodiment figure 3 show posterior probability density function determiner 205. posterior probability density function determiner 205 comprises likelihood function determiner 301 post likelihood posterior probability density function determiner 303. likelihood function p ( m|u ) determiner 301 may implemented hardware special purpose circuit , software , logic combination thereof . furthermore aspect likelihood function determiner 301 may implemented hardware , aspect may implemented firmware software may executed controller , microprocessor computing device , although invention limited thereto . various aspect likelihood function determiner 301 may implemented , non-limiting example , hardware , software , firmware , special purpose circuit logic , general purpose hardware controller computing device , software module ( e.g . library function ) , physical circuitry ( e.g . asic ) , programmed microprocessor combination thereof . likelihood function determiner 301 may thus implemented computer software executable least one data processor , processor entity , hardware , combination software hardware . likelihood function determiner 301 , configured receive windowed signal 210 , configured determine likelihood function . likelihood hypothetical probability event already occurred would yield specific outcome . concept differs probability probability refers occurrence future event , likelihood refers past event known outcome . likelihood function may determined model input signal ( ) . model measurement ( ) =a ( u ( ) ) +e ( ) e ( ) modelled gaussian identically distributed variable ( e.g . white noise ) , pm|u=∏t12πσe2e−mt−a ( ut22σe2⇒logpm|u=12σe2‖m−au‖2+constant , norm l2-norm ( sum square ) . determined likelihood function may passed post likelihood posterior probability density function determiner 303. embodiment post likelihood posterior probability density function determiner 303 may implemented hardware special purpose circuit , software , logic combination thereof . furthermore aspect post likelihood posterior probability density function determiner 303 may implemented hardware , aspect may implemented firmware software may executed controller , microprocessor computing device , although invention limited thereto . various aspect post likelihood posterior probability density function determiner 303 may implemented , non-limiting example , hardware , software , firmware , special purpose circuit logic , general purpose hardware controller computing device , software module ( e.g . library function ) , physical circuitry ( e.g . asic ) , programmed microprocessor combination thereof . post likelihood posterior probability density function 303 may thus implemented computer software executable least one data processor , processor entity , hardware , combination software hardware . post likelihood posterior probability density function determiner 303 , configured receive prior pdf determined likelihood function configured determine posterior probability density function p ( u|m ) using bayes theorem : pu|m=pm|upupm∝pm|upu p ( m|u ) likelihood function p ( u ) prior probability density function ( result convolution ) . posterior probability density function p ( u|m ) ( logarithm p ( u|m ) ebodiments ) may output point estimator 207. figure 4 show example point estimator 207 comprising minimiser 401. minimiser 401 may implemented hardware special purpose circuit , software , logic combination thereof . furthermore aspect minimiser 401 may implemented hardware , aspect may implemented firmware software may executed controller , microprocessor computing device , although invention limited thereto . various aspect minimiser 401 may implemented , non-limiting example , hardware , software , firmware , special purpose circuit logic , general purpose hardware controller computing device , software module ( e.g . library function ) , physical circuitry ( e.g . asic ) , programmed microprocessor combination thereof . minimiser 401 may thus implemented computer software executable least one data processor , processor entity , hardware , combination software hardware . minimiser 401 , configured receive posterior probability density function p ( u|m ) 220 , configured generate suitable point estimate minimising posterior probability density function . example embodiment minimiser 401 configured determine maximum priori ( map ) estimate maximum point posterior probability density function . practice , map estimate computed minimizing function −logpu|m=−logpm|u−logpu . minimization carried using suitable method , example , using newton 's method barzilai-borwein method . figure 5 show example flow diagram showing operation example system according embodiment . first signal ( ) , comprises piecewise-smooth signal u ( ) noise e ( ) , measured received shown figure 5 step 501. signal ( ) windowed generate windowed signal 210 shown figure 5 step 503. embodiment lambda sigma parameter smooth x jump variable chosen shown figure 5 step 502. furthermore embodiment prior pdf constructed prescribing pdfs smooth part x jump convolving pdfs generate prior probability density function p ( u ) =p ( x ) *p ( ) , word probability density function representing sum smooth jump variable shown figure 5 step 504. determined prior probability density function p ( u ) likelihood function p ( m|u ) , posterior probability density function p ( u|m ) ( log p ( u|m ) ) determined shown figure 5 step 505. minimised posterior probability density function determined order determine point estimate shown figure 5 step 507. point estimate may output shown figure 5 step 509. figure 6 show flow chart showing operation example probability density function determiner according embodiment detail . first smooth function variable x pdf p ( x ) generated ( embodiment based sigma input ) shown figure 6 step 601. jump function variable pdf p ( ) generated ( embodiment based lambda input ) shown figure 6 step 603. prior pdf generated applying convolution p ( x ) p ( ) pdfs shown figure 6 step 605. figure 7 show flow chart showing operation example posterior probability density function determiner point estimator according embodiment . first operation receiving windowed signal 210 shown figure 7 step 701. likelihood function pdf p ( m|u ) may determined windowed signal 210 shown figure 7 step 703. also prior probability density function p ( u ) received retrieved shown figure 7 step 704. method may comprise determining posterior probability density function p ( u|m ) based likelihood function p ( m|u ) prior probability density function p ( u ) shown figure 7 step 705. method may comprise estimating point ( example minimising posterior probability density function p ( u|m ) ( -log p ( u|m ) ) shown figure 7 step 707. general , various embodiment may implemented hardware special purpose circuit , software , logic combination thereof . aspect invention may implemented hardware , aspect may implemented firmware software may executed controller , microprocessor computing device , although invention limited thereto . various aspect invention may illustrated described block diagram , flow chart , using pictorial representation , well understood block , apparatus , system , technique method described herein may implemented , non-limiting example , hardware , software , firmware , special purpose circuit logic , general purpose hardware controller computing device , combination thereof . embodiment invention may implemented computer software executable data processor mobile device , processor entity , hardware , combination software hardware . computer software program , also called program product , including software routine , applet and/or macro , may stored apparatus-readable data storage medium comprise program instruction perform particular task . computer program product may comprise one computer-executable component , program run , configured carry embodiment . one computer-executable component may least one software code portion . regard noted block logic flow figure may represent program step , interconnected logic circuit , block function , combination program step logic circuit , block function . software may stored physical medium memory chip , memory block implemented within processor , magnetic medium hard disk floppy disk , optical medium example dvd data variant thereof , cd . physical medium non-transitory medium . memory may type suitable local technical environment may implemented using suitable data storage technology , semiconductor based memory device , magnetic memory device system , optical memory device system , fixed memory removable memory . data processor may type suitable local technical environment , may comprise one general purpose computer , special purpose computer , microprocessor , digital signal processor ( dsps ) , application specific integrated circuit ( asic ) , fpga , gate level circuit processor based multi core processor architecture , non-limiting example . embodiment invention may practiced various component integrated circuit module . design integrated circuit large highly automated process . complex powerful software tool available converting logic level design semiconductor circuit design ready etched formed semiconductor substrate . foregoing description provided way non-limiting example full informative description exemplary embodiment invention . however , various modification adaptation may become apparent skilled relevant art view foregoing description , read conjunction accompanying drawing appended claim . however , similar modification teaching invention still fall within scope invention defined appended claim . indeed embodiment comprising combination one embodiment embodiment previously discussed . _____c : 1. apparatus comprising least one processor , least one memory including computer program code , wherein least one memory computer program code configured , least one processor , : receive input signal , input signal comprising piecewise-smooth signal noise signal ; determine prior probability density function representation piecewise-smooth signal , representation piecewise-smooth signal comprising combination smooth , gaussian , variable jump , cauchy , variable ; determine likelihood function based input signal ; determine posterior probability density function based likelihood function prior probability density function distribution ; estimate piecewise-smooth signal based estimate posterior probability density function . 2. apparatus according claim 1 , wherein processor configured determine prior probability density function representation piecewise-smooth signal , representation piecewise-smooth signal comprising combination smooth , gaussian , variable jump , cauchy , variable configured : determine first probability density function representing sample difference smooth , gaussian , variable ; determine second probability density function representing sample difference jump , cauchy , variable ; convolve first probability density function second probability density function generate prior probability density function . 3. apparatus according claim 2 , wherein processor configured convolve first probability density function second probability density function generate prior probability density function configured determine : analytical determination using least one imaginary error function dawson function ; numerical determination using grid sample point interpolation point grid sample point . 4. apparatus claimed claim 1 3 , wherein processor configured estimate piecewise-smooth signal based estimate posterior probability density function configured estimate piecewise-smooth signal based maximum posteriori estimate posterior probability density function . 5. apparatus according claim 4 , wherein processor configured estimate piecewise-smooth signal based maximum posteriori estimate posterior probability density function configured minimise combination logarithm likelihood function logarithm prior probability density function . 6. apparatus according claim 5 , wherein processor configured minimise combination logarithm likelihood function logarithm prior probability density function configured apply one : newton 's method ; barzilai-borwein method . 7. apparatus according claim 1 6 , wherein processor configured determine posterior probability density function based likelihood function prior probability density function configured determine posterior probability density function based product likelihood function prior probability density function according bayes ' theorem . 8. apparatus according claim 1 7 , wherein processor configured window input signal . 9. apparatus according claim 1 8 , wherein smooth , gaussian , variable jump , cauchy , variable independent . 10. method comprising : receiving input signal , input signal comprising piecewise-smooth signal noise signal ; determining prior probability density function representation piecewise-smooth signal , representation piecewise-smooth signal comprising combination smooth , gaussian , variable jump , cauchy , variable ; determining likelihood function based input signal ; determining posterior probability density function based likelihood function prior probability density function distribution ; estimating piecewise-smooth signal based estimate posterior probability density function . 11. method according claim 10 , wherein determining prior probability density function representation piecewise-smooth signal , representation piecewise-smooth signal comprising combination smooth , gaussian , variable jump , cauchy , variable comprises : determining first probability density function representing sample difference smooth , gaussian , variable ; determining second probability density function representing sample difference jump , cauchy , variable ; convolving first probability density function second probability density function generate prior probability density function . 12. method according claim 11 , wherein convolving first probability density function second probability density function generate prior probability density function comprises : analytical determination using least one imaginary error function dawson function ; numerical determination using grid sample point interpolation point grid sample point . 13. method claimed claim 10 12 , wherein estimating piecewise-smooth signal based estimate posterior probability density function comprises estimating piecewise-smooth signal based maximum posteriori estimate posterior probability density function . 14. method according claim 13 , wherein estimating piecewise-smooth signal based maximum posteriori estimate posterior probability density function comprises minimising combination logarithm likelihood function logarithm prior probability density function . 15. method according claim 10 14 , wherein determining posterior probability density function based likelihood function prior probability density function comprises determining posterior probability density function based product likelihood function prior probability density function according bayes ' theorem .\", \"multi-camera device _____2019_____3503527_____487681213_____ep3500000.txt_____g06t_____g06t2200/32 : g06t2207/10012 : g06t2207/10024 : g06t2207/20221 : g06t3/00 : g06t7/33 : h04n2007/145 : h04n5/2258 : h04n5/23216 : h04n5/232935 : h04n5/232945 : h04n5/23296 : h04n5/23299 : h04n5/247 : h04n5/2624 : h04n7/147 method apparatus described comprising : obtaining first input data first camera multi-camera device ; obtaining second input data second camera multi-camera device ( wherein second camera orientated different direction first camera ) ; defining first region object interest first camera and/or second region object interest second camera ; tracking first region object interest and/or second region object interest ; generating first output data based first input data ; generating second output data based second input data ; generating multi-camera output includes first second output data . _____d : field present specification relates capturing content using camera , example using camera mobile communication device similar device . background mobile communication device including camera known . moreover , known provide camera front rear mobile communication device . however , difficult provide good framing image front rear camera , especially two camera used time . summary first aspect , specification describes method comprising : obtaining first input data first camera multi-camera device ; obtaining second input data second camera multi-camera device , wherein second camera orientated different direction first camera ; defining first region object interest first camera and/or second region object interest second camera ; tracking first region object interest and/or second region object interest ; generating first output data based first input data ; generating second output data based second input data ; generating multi-camera output includes first second output data . first camera may front camera . second camera may rear camera . first input data may data field view first camera . second input data may data field view second camera . first output data may data viewport first camera , wherein viewport first camera narrower field view first camera . alternatively , addition , second output data may data viewport second camera , wherein viewport second camera narrower field view second camera . first region object interest and/or second region object interest may defined user indication . tracking first region object interest may comprise keeping first region object interest within first output data . alternatively , addition , tracking second region object interest may comprise keeping second region object interest within second output data . method may comprise ceasing track first region object interest and/or second region object interest said region object interest move outside field view relevant camera . method may comprise resuming tracking first region object interest and/or second region object interest said region object interest move back within field view relevant camera . method may comprise providing alert first region object interest and/or second region object interest move outside field view relevant camera . method may comprise deselecting first region object interest first camera and/or second region object interest second camera ceasing track deselected region object interest . multi-camera output may present first second output data either side-by-side one top . second aspect , specification describes apparatus configured perform method described reference first aspect . third aspect , specification describes computer-readable instruction , executed computing apparatus , cause computing apparatus perform method described reference first aspect . fourth aspect , specification describes apparatus comprising : mean obtaining first input data first camera multi-camera device ; mean obtaining second input data second camera multi-camera device , wherein second camera orientated different direction first camera ; mean defining first region object interest first camera and/or second region object interest second camera ; mean tracking first region object interest and/or second region object interest ; mean generating first output data based first input data ; mean generating second output data based second input data ; mean generating multi-camera output includes first second output data . first input data may data field view first camera . second input data may data field view second camera . first output data may data viewport first camera , wherein viewport first camera narrower field view first camera . second output data may data viewport second camera , wherein viewport second camera narrower field view second camera . fifth aspect , specification describes apparatus comprising : least one processor ; least one memory including computer program code , executed least one processor , cause apparatus : obtain first input data first camera multi-camera device ; obtain second input data second camera multi-camera device , wherein second camera orientated different direction first camera ; define first region object interest first camera and/or second region object interest second camera ; track first region object interest and/or second region object interest ; generate first output data based first input data ; generate second output data based second input data ; generate multi-camera output includes first second output data . sixth aspect , specification describes computer-readable medium computer-readable code stored thereon , computer readable code , executed least one processor , causing performance : obtaining first input data first camera multi-camera device ; obtaining second input data second camera multi-camera device , wherein second camera orientated different direction first camera ; defining first region object interest first camera and/or second region object interest second camera ; tracking first region object interest and/or second region object interest ; generating first output data based first input data ; generating second output data based second input data ; generating multi-camera output includes first second output data . brief description drawing example embodiment described , way non-limiting example , reference following schematic drawing , : figure 1 block diagram system accordance example embodiment ; figure 2 show exemplary view output system figure 1 ; figure 3 block diagram system accordance example embodiment ; figure 4 show exemplary view output system figure 3 ; figure 5 block diagram system accordance example embodiment ; figure 6 show exemplary view output system figure 5 ; figure 7 show exemplary view generated output system figure 5 ; figure 8 flow chart showing algorithm accordance example embodiment ; figure 9a 9d show exemplary output algorithm figure 8 ; figure 10 flow chart showing algorithm accordance example embodiment ; figure 11a 11e show exemplary output accordance example embodiment ; figure 12a 12c show exemplary output accordance example embodiment ; figure 13a 13b show example output accordance example embodiments.figure 14 block diagram system according example embodiment ; andfigures 15a 15b show tangible medium , respectively removable memory unit company disc ( cd ) storing computer-readable code run computer perform operation according embodiment . detailed description figure 1 block diagram system , indicated generally reference numeral 10 , accordance example embodiment . system 10 comprises user device 12 , mobile communication device ( e.g . mobile phone ) . user device 12 front camera 13 rear camera 14 ( camera may video camera and/or still image camera ) . first object 15 second object 16 within viewport front camera 13. third object 17 within viewport rear camera 14. third object 17 may , example , user user device 12. user device 12 , front camera 13 rear camera 14 , example multi-camera device . example multi-camera device also possible , device two camera , device camera different location . figure 2 show exemplary view , indicated generally reference numeral 20 , output user device 12 described reference figure 1. view 20 combined view includes first view 21 provided front camera 13 second view 22 provided rear camera 14. shown figure 2 , combined view 20 display first second view side-by-side . thus , view 20 show data within viewports camera 13 14. generally , first view 21 second view 22 captured simultaneously camera 13 14 respectively . first view 21 includes first image 23 second image 24. first image 23 ( left view 21 ) representation first object 15. second image 24 representation second object 16. similar way , second view 22 includes third image 25 representation third object 17. apparent image displayed first second view adjusted moving user device 12 ( hence moving camera 13 14 ) . figure 3 block diagram system , indicated general reference numeral 30 , accordance example embodiment . system 30 includes user device 12 , first object 15 , second object 16 third object 17 described . however , system 30 , user device 12 rotated anticlockwise relative user device position figure 1. user device 12 may rotated manner order frame first object 15 within centre viewport front camera user device 12. figure 4 show exemplary view , indicated generally reference numeral 40 , output system 30. view 20 described , view 40 combined view includes first view 41 provided front camera user device 12 second view 42 provided rear camera user device . combined view 20 described , combined view 40 display first second view side-by-side , noted , first view 41 second view 42 generally captured simultaneously . first view 41 includes first image 44 representation first object 15. result rotation user device 12 , first image 44 centre first view 41 second object 16 longer visible first view 41. second view 42 includes image 45 representation third object 17. result rotation user device 12 , image 45 appears left second view 42 ( compared central position corresponding image second view 22 described ) . difficult manipulate user device 12 order frame one object interest within viewport front camera 13 time frame one object interest within viewport rear camera 14. particularly true object interest moving . moreover , framing output rear camera 14 particularly difficult since view typically presented mirror image . way example , consider scenario front camera 13 user device 12 used capture video data two child playing . child may object 15 16 described . time , rear camera 14 used capture video data parent two child . parent may object 17 described . child move , user device 12 moved keep child ( i.e . object 15 16 ) within viewport front camera 12 therefore displayed first view 21 41 combined view 20 40. however , user device 12 moved keep child within viewport , position parent within viewport rear camera 14 change . thus , framing rear camera change framing front camera change . inter-relationship framing front rear camera difficult control . thus , use system 10 ( 30 ) providing combined view ( view 20 40 ) difficult , particularly inexperienced user , particularly desired generate image ( either still image video image ) without requiring post-processing image . figure 5 block diagram system , indicated generally reference numeral 50 , accordance example embodiment . system 50 includes user device 12 described , user device 12 including front camera 13 rear camera 14. addition , system 50 comprises first object 51 , second object 52 , third object 53 , fourth object 54 fifth object 55. indicated figure 5 , front camera 13 viewport angle φviewport . rear camera 14 similar viewport ( indicated dotted line figure 5 ) . arrangement described reference figure 2 4 , object within viewport camera displayed combined output . addition viewport , front camera 13 field-of-view angle φfov . rear camera similar field view ( , indicated dotted line figure 5 ) . shown figure 5 , angle field view ( φfov ) camera 13 14 wider angle viewports ( φviewport ) camera . note although viewports first second camera similar field view first second camera similar , case . way example , viewports field view one camera ( e.g . front camera ) could wider corresponding viewports field view camera ( e.g . rear camera . ) also noted , discussed elsewhere document , one viewports may change location within field view camera . figure 6 show exemplary view , indicated generally reference numeral 60 , output system 50. view 60 combined view includes first view 61 provided front camera 13 user device 12 system 50 second view 62 provided rear camera 14 user device . combined view 20 40 described , combined view 60 display first second view side-by-side . also , accordance view 20 40 described , first view 61 show visible within viewport first camera 13 second view 62 show visible within viewport second camera 14. first view 61 includes first image 63 , second image 64 third image 65. first image 61 ( left view 61 ) representation part first object 51 ( part within viewport front camera 13 ) . second image 64 representation second object 52. third image 65 representation part third object 53 ( part within viewport front camera 13 ) . similar way , second view 62 includes fourth image 66 fifth image 67 representation fourth object 54 fifth object 55 respectively . described , data collected field view front rear camera , field view wider viewports . thus , data collected displayed . lead degree flexibility , exploited , described detail . figure 7 show exemplary view , indicated general reference numeral 70 , output system 50. view 70 include first view 71 show visible within field view first camera 13 second view 72 show visible within field view second camera 14. first view 71 includes first image 73 , second image 74 third image 75 representation first object 51 , second object 52 third object 53* respectively . similarly , second view 72 includes fourth image 76 fifth image 77 representation fourth object 54 fifth object 55 respectively . thus , first view 71 show first object 51 , second object 52 third object 53 , within field view first camera . similarly , second view 72 show fourth object 54 fifth object 55 , within field view second camera . noted , field view first second camera wider respective viewports first second camera . accordingly , within first view 71 area , indicated generally reference numeral 78 , showing visible within viewport first camera . similarly , within second view 72 area , indicated generally reference numeral 79 , showing visible within viewport second camera . apparent viewing figure 6 7 , first view 61 combined view 60 described formed view 78. similarly , second view 62 combined view 60 formed view 79. thus , first second camera 12 13 capture data displayed combined view 60. fact exploited interesting way , described . figure 8 flow chart showing algorithm , indicated generally reference numeral 80 , accordance example embodiment . figure 9a 9d show exemplary output algorithm 80. shown figure 8 , algorithm 80 start operation 82. optionally , operation 84 , user device 12 positioned . example , user device may positioned one desired object within viewport field view front camera user device . embodiment , operation 84 may omitted ( e.g . user device 12 fixed user wish change rotational position ) . operation 86 , combined view front rear camera generated displayed . example , output 60 described may generated displayed , showing visible within viewports front rear camera user device 12. thus , data obtained field view front rear camera may considered `` input data '' algorithm 80 data obtained viewports front rear camera may considered `` output data '' algorithm 80 , output ( viewport ) data subset input ( field view ) data ) . operation 88 , front object area interest selected . figure 9a show output , indicated generally reference numeral 100 , showing first , second , third , fourth fifth image 63 67 described reference figure 6. indication 101 shown figure 9a showing object area interest . indication 101 may , example , made user touching relevant screen ( option discussed ) . response indication 101 , area interest 102 around second image 64 highlighted output 100. highlighted area 102 may indicate area interest may indicate second image 64 represents object interest algorithm 80 . ( noted whether object area interest selected operation 88 may , embodiment , defined user . ) operation 90 , view reframed . example , view area object interest selected operation 88 may expanded zooming object area . way example , figure 9b show combined view , indicated generally reference numeral 110 , first view modified ( i.e . reframed ) centred zoomed area 102 ' , including zoomed second image 64 ' . note first image 63 third image 65 longer displayed . operation 92 , user device optionally rotated . example , user device may rotated one desired object within viewport field view rear camera user device . embodiment , operation 92 may omitted ( e.g . user device 12 fixed user wish change rotational position ) . ( indeed , operation 92 omitted example figure 9a 9d . ) operation 94 , rear object area selected . figure 9c show combined view , indicated generally reference numeral 120 indication 121 shown , showing object area interest . indication 101 described , indication 121 may made user touching relevant screen . response indication 121 , area interest 122 around fifth image 67 highlighted output 120. highlighted area 122 may indicate area interest may indicate fifth image 67 represents object interest algorithm 80 . ( noted whether object area interest selected operation 94 may , embodiment , defined user . ) operation 96 , view reframed . figure 9d show combined view , indicated generally reference numeral 130 , second view modified ( i.e . reframed ) centred zoomed area 122 ' including zoomed image 67 ' . algorithm end operation 98. indicated , user may indicate object area interest touching display screen . may , example , implemented using user 's finger using stylus . selection option possible . example , direction user 's gaze may determined selection implemented looking portion relevant display . option include voice command keyboard mouse instruction . touching display screen , using direction user 's gaze , voice command using keyboard mouse example user indication . subset user indication user gesture ( e.g . hand gesture ) could used form user input embodiment . option determining object area interest includes making use content analysis ( e.g . matching content view known content tracking content ) . content analysis may combined element . example , user may indicate area interest content analysis used identify object interest within region . operation algorithm 90 essential . indicated , may necessary select front object area ( operation 88 ) rear object area ( operation 94 ) . furthermore , may necessary reframe front view ( operation 90 ) and/or rear view ( operation 96 ) . particular , reframing necessarily need include zooming operation . noted least operation algorithm 90 may carried different order . example , although algorithm 90 describes operation 88 ( selection front object area interest ) carried operation 94 ( selection rear object area interest ) , operation may reversed . figure 10 flow chart showing algorithm , indicated generally reference numeral 140 , accordance example embodiment . algorithm start operation 142 , front rear display setup . example , operation 142 may implement algorithm 80 described . thus , operation 142 complete , first display combined display may display zoomed image 64 ' second display combined display may display zoomed image 67 ' . ( course , noted , zooming reframing essential embodiment . ) operation 144 , object area selected tracked kept within centre relevant display ( e.g . within centre relevant display ) . thus , example , user device 12 and/or displayed object 52 55 move , retained within centre displayed image . example display 130 , object 52 55 ( corresponding image 64 67 ) may tracked operation 144. possible since , noted , field view front rear camera user device obtain data shown display . described , always possible keep tracked area within centre relevant display . operation 144 may include keeping tracked area within relevant display ( whether tracked area within centre display ) . embodiment , viewport may moved within field view order keep tracked area within centre ( closer centre ) relevant display . noted mean essential two areas/objects interest selected . example , case exemplary display 110 , third object 64 may kept within centre first display , user may free rotate user device order change visible second display . example embodiment , user control selection ( de-selection ) areas/objects interest . consider example system 10 described first second object 15 16 child third object 17 parent . user user device may decide track child 15 16 manually moving user device 12. thus , object may selected operation 88 described . however , parent 17 may selected operation 94 parent tracked automatically . thus , provided parent within field view rear camera 14 user device 12 , parent stay within centre rear view 22. indicated , may possible keep child 15 16 parent 17 within centre field view relevant display . embodiment , viewport relevant camera ( ) may moved within field ( ) view order keep tracked area ( ) within centre ( closer centre ) relevant display . operation 146 algorithm 140 , determined whether region object interest remains within field view relevant camera . , algorithm return operation 144 ; otherwise , algorithm move operation 148. operation 148 , determined least one region object interest longer within field view relevant camera . thus , possible display region object . corrected , example , suitable rotation user device ( field view relevant camera move ) . rotation ( movement ) may initiated operation 148. figure 11a 11e show different version exemplary view output accordance exemplary embodiment . figure 11a 11e show first view 150 showing visible within field view viewport first camera 13 , second view 151 showing visible within field view viewport second camera 14 , third view 152 , combined view derived content viewports first second view . figure 11a , first view 150 includes first object 153 centre viewport . similarly , second view 151 includes second object 154 centre viewport . accordingly , third view includes first object 155 ( corresponding first object 153 ) second object 156 ( corresponding second object 154 ) . assume first object 153 tracked ( selected operation 88 described ) , second object 154 tracked ( object area selected operation 94 ) . accordance operation 144 algorithm 140 , first object interest tracked kept within centre relevant display 152. figure 11b show situation first object 153 second object 154 move left within relevant viewports . shown first view 150 , first object 153 tracked , viewport first camera effectively move within field view first object 153 stay within centre viewport . time , second object 154 tracked , view view second camera move ( see second view 151 ) . accordingly , third view 152 show first object 155 centre relevant viewport second object 156 towards left relevant viewport . figure 11c show situation first object 153 moving outside field view first camera ( see first view 150 ) second object moved back centre viewport second camera ( see second view 151 third view 152 , second object 156 displayed centre relevant viewport ) . since object 153 moved field view first camera user device 12 , longer possible track display object full . accordingly , algorithm 140 move operation 148 , cause arrow 157 displayed ( see third view 152 figure 11c ) . arrow 157 indicates object 152 moved area visible relevant camera user device . arrow 157 therefore prompt user rotate user device , wish continue track relevant object . arrow 157 example alert may provided indicate first region object interest and/or second region object interest move outside field view relevant camera . arrow 157 shown way example ; many alternative mechanism could used indicated user tracked object/area longer within field view . include visual indication ( arrow 156 flashing image ) , audio indication ( e.g . alarm ) haptics . figure 11d show situation first object 153 moved back within field view first camera ( see first view 150 ) second object still centre viewport second camera ( see second view 151 ) . thus , first object 155 second object 156 displayed centre relevant viewport third view 152 ) . assume first object 153 tracked ( selected operation 88 described ) second object 154 tracked ( selected operation 94 described ) . figure 11e show situation first object 153 second object 154 move left within relevant viewports . shown first view 150 , first object 153 tracked , viewport first camera effectively move within field view first object 153 stay within centre viewport ( manner described reference figure 11b ) . time , since second object 154 also tracked , viewport second camera effectively move similar manner . accordingly , shown third view 153 , first object 155 second object 156 appear centre relevant viewport . embodiment described include tracking object interest . implemented number way . example , template may generated identifying key feature object tracked ( e.g . size , shape , colour , texture etc . ) simple matching algorithm used identify object . way , possible maintain object within centre viewport first second camera described . figure 12a 12c show exemplary output , indicated generally reference numeral 160 , 170 180 respectively accordance example embodiment . figure 12a show arrangement 160 first object 162 tracked ( indicated square box 163 ) second object 164 tracked . figure 12b show situation first object 162 move field view relevant camera , action taken result . object longer tracked seen move display . assume object move back field view . , shown figure 12c , object ( object 162 ) tracked shown centre relevant display . described , template may generated identifying key feature object tracked ( e.g . size , shape , colour , texture etc . ) simple matching algorithm used identify object . may possible use matching algorithm identify object moving back within field view camera . embodiment described generally described object interest , tracked . also possible track region interest , regardless visible within region . thus area interest , example selected user way described herein , kept within centre display , provided area interest remains within field view relevant camera . embodiment described , combined view presented side-by-side . essential form invention . figure 13a show exemplary view output system figure 1 identical figure 2 described . thus , figure 13a combined view includes first view provided front camera 13 second view 22 provided rear camera 14. shown figure 2 , combined view 20 display first second view side-by-side . first view 21 includes first image 23 ( representation first object 15 ) second image 24 ( representation second object 16 ) . similar way , second view 22 includes third image 25 representation third object 17. figure 13b also show combined view , including first view 21 ' corresponds first view 21 described ( includes first image 23 ' second image 24 ' ) second view 22 ' corresponds second view 22 described ( includes third image 25 ' ) . combined view figure 13b differs combined view figure 13a first second view shown top one another . shown figure 13b , first view 21 ' displayed second view 22 ' . alternative embodiment , second view could displayed first view . completeness , figure 14 example schematic diagram component one module described previously , accordance example embodiment , hereafter referred generically processing system 300. processing system 300 may comprise processor 302 , memory 304 closely coupled processor comprised ram 314 rom 312 , , optionally , user input 310 display 318. processing system 300 may comprise one network interface 308 connection network , e.g . modem may wired wireless . processor 302 connected component order control operation thereof . memory 304 may comprise non-volatile memory , hard disk drive ( hdd ) solid state drive ( ssd ) . rom 312 memory 314 store , amongst thing , operating system 315 may store software application 316. ram 314 memory 304 used processor 302 temporary storage data . operating system 315 may contain code , executed processor implement aspect algorithm 80 and/or 140. processor 302 may take suitable form . instance , may microcontroller , plural microcontrollers , processor , plural processor . processing system 300 may standalone computer , server , console , network thereof . embodiment , processing system 300 may also associated external software application . may application stored remote server device may run partly exclusively remote server device . application may termed cloud-hosted application . processing system 300 may communication remote server device order utilize software application stored . communication may processed network interface 308. figure 15a 15b show tangible medium , respectively removable memory unit 365 compact disc ( cd ) 368 , storing computer-readable code run computer may perform method according embodiment described . removable memory unit 365 may memory stick , e.g . usb memory stick , internal memory 366 storing computer-readable code . memory 366 may accessed computer system via connector 367. cd 368 may cd-rom dvd similar . form tangible storage medium may used . embodiment present invention may implemented software , hardware , application logic combination software , hardware application logic . software , application logic and/or hardware may reside memory , computer medium . example embodiment , application logic , software instruction set maintained one various conventional computer-readable medium . context document , `` memory '' `` computer-readable medium '' may non-transitory medium mean contain , store , communicate , propagate transport instruction use connection instruction execution system , apparatus , device , computer . reference , relevant , `` computer-readable storage medium '' , `` computer program product '' , `` tangibly embodied computer program '' etc. , `` processor '' `` processing circuitry '' etc . understood encompass computer differing architecture single/multi-processor architecture sequencers/parallel architecture , also specialised circuit field programmable gate array fpga , application specify circuit asic , signal processing device device . reference computer program , instruction , code etc . understood express software programmable processor firmware programmable content hardware device instruction processor configured configuration setting fixed function device , gate array , programmable logic device , etc . used application , term `` circuitry '' refers following : ( ) hardware-only circuit implementation ( implementation analogue and/or digital circuitry ) ( b ) combination circuit software ( and/or firmware ) , ( applicable ) : ( ) combination processor ( ) ( ii ) portion processor ( ) /software ( including digital signal processor ( ) ) , software , memory ( y ) work together cause apparatus , server , perform various function ) ( c ) circuit , microprocessor ( ) portion microprocessor ( ) , require software firmware operation , even software firmware physically present . desired , different function discussed herein may performed different order and/or concurrently . furthermore , desired , one above-described function may optional may combined . similarly , also appreciated flow diagram figure 8 10 example various operation depicted therein may omitted , reordered and/or combined . appreciated described example embodiment purely illustrative limiting scope invention . variation modification apparent person skilled art upon reading present specification . moreover , disclosure present application understood include novel feature novel combination feature either explicitly implicitly disclosed herein generalization thereof prosecution present application application derived therefrom , new claim may formulated cover feature and/or combination feature . although various aspect invention set independent claim , aspect invention comprise combination feature described embodiment and/or dependent claim feature independent claim , solely combination explicitly set claim . also noted herein describes various example , description viewed limiting sense . rather , several variation modification may made without departing scope present invention defined appended claim . _____c : 1. method comprising : obtaining first input data first camera multi-camera device ; obtaining second input data second camera multi-camera device , wherein second camera orientated different direction first camera ; defining first region object interest first camera and/or second region object interest second camera ; tracking first region object interest and/or second region object interest ; generating first output data based first input data ; generating second output data based second input data ; generating multi-camera output includes first second output data . 2. method claimed claim 1 , wherein first input data data field view first camera second input data data field view second camera . 3. method claimed claim 2 , wherein : first output data data viewport first camera , wherein viewport first camera narrower field view first camera ; and/or second output data data viewport second camera , wherein viewport second camera narrower field view second camera . 4. method claimed one preceding claim , wherein first region object interest and/or second region object interest is/are defined user indication . 5. method claimed one preceding claim , wherein : tracking first region object interest comprises keeping first region object interest within first output data ; and/or tracking second region object interest comprises keeping second region object interest within second output data . 6. method claimed one preceding claim , comprising ceasing track first region object interest and/or second region object interest said region object interest move outside field view relevant camera . 7. method claimed claim 6 , comprising resuming tracking first region object interest and/or second region object interest said region object interest move back within field view relevant camera . 8. method claimed one preceding claim , comprising providing alert first region object interest and/or second region object interest move outside field view relevant camera . 9. method claimed one preceding claim , comprising deselecting first region object interest first camera and/or second region object interest second camera ceasing track deselected region object interest . 10. method claimed one preceding claim , wherein multi-camera output present first second output data either side-by-side one top . 11. method claimed preceding claim , wherein first camera front camera second camera rear camera . 12. apparatus comprising : mean obtaining first input data first camera multi-camera device ; mean obtaining second input data second camera multi-camera device , wherein second camera orientated different direction first camera ; mean defining first region object interest first camera and/or second region object interest second camera ; mean tracking first region object interest and/or second region object interest ; mean generating first output data based first input data ; mean generating second output data based second input data ; mean generating multi-camera output includes first second output data . 13. apparatus claimed claim 12 , wherein first input data data field view first camera second input data data field view second camera . 14. apparatus claimed claim 13 , wherein : first output data data viewport first camera , wherein viewport first camera narrower field view first camera ; and/or second output data data viewport second camera , wherein viewport second camera narrower field view second camera . 15. computer-readable medium computer-readable code stored thereon , computer readable code , executed least one processor , causing performance : obtaining first input data first camera multi-camera device ; obtaining second input data second camera multi-camera device , wherein second camera orientated different direction first camera ; defining first region object interest first camera and/or second region object interest second camera ; tracking first region object interest and/or second region object interest ; generating first output data based first input data ; generating second output data based second input data ; generating multi-camera output includes first second output data .\", \"system assessing pulmonary image _____2019_____3503022_____487681626_____ep3500000.txt_____g06t_____a61b6/032 : a61b6/50 : a61b6/504 : a61b6/5205 : a61b6/5258 : a61b6/542 : a61b6/545 : g06t2207/10081 : g06t2207/30061 : g06t2207/30168 : g06t7/0012 invention relates system assessing pulmonary image allows improved assessment respect lung nodule detectability . pulmonary image smoothed providing different pulmonary image ( 20 , 21 , 22 ) different degree smoothing , wherein signal value noise value , indicative lung vessel detectability noise image , determined used determining image quality indicative usability pulmonary image assessed detecting lung nodule . since pulmonary image show lung vessel many different vessel size many different image value , cover respective range potential lung nodule generally well , image quality determination based different pulmonary image ( 20 , 21 , 22 ) different degree smoothing allows reliable assessment pulmonary image 's usability detecting lung nodule . _____d : field invention invention relates system , method computer program assessing pulmonary image . background invention detection lung nodule pulmonary image like computed tomography image lung often used , wherein assessing image quality pulmonary image respect usability reliably detecting lung nodule noise might determined . however , determining image quality based noise often good enough , detectability lung nodule pulmonary image might also depend factor . summary invention object present invention provide system , method computer program assessing pulmonary image , allows improved assessment respect detectability lung nodule pulmonary image . first aspect present invention system assessing pulmonary image presented , wherein system comprises : pulmonary image providing unit configured provide pulmonary image comprises image element image value assigned show lung vessel , smoothing unit configured smooth provided pulmonary image providing different pulmonary image different degree smoothing , signal value determination unit configured determine signal value different pulmonary image , wherein respective pulmonary image one signal value , indicative detectability lung vessel respective pulmonary image , determined based image value respective pulmonary image , noise value determination unit configured determine noise value different pulmonary image , wherein respective pulmonary image one noise value , indicative noise respective pulmonary image , determined based image value respective pulmonary image , image quality determination unit configured determine image quality provided unsmoothed image based signal value noise value determined different pulmonary image . pulmonary image lung vessel many different vessel size many different image value like hounsfield density present , cover respective range potential lung nodule generally well . since different pulmonary image correspond different degree smoothing , include , instance , smoothing one different degree smoothing , different pulmonary image distribution size lung vessel different , i.e. , instance , `` center mass '' distribution shifted larger size increasing degree smoothing . thus , signal value noise value determined different distribution size lung vessel different pulmonary image , wherein found using signal value noise value determining image quality provided , unsmoothed pulmonary image , accurately determined whether pulmonary image suitable detecting lung nodule . pulmonary image providing unit storing unit pulmonary image stored already pulmonary image retrieved . pulmonary image providing unit also receiving unit receiving pulmonary image pulmonary image generating system like computed tomography system , wherein pulmonary image providing unit adapted provide received pulmonary image . pulmonary image providing unit also pulmonary image generating system generates pulmonary image provides generated pulmonary image . provided pulmonary image preferentially computed tomography image generated applying radiation dose patient . however , pulmonary image also image generated another imaging modality , also show lung vessel . smoothing unit configured perform spatial smoothing image value assigned image element . smoothing gaussian smoothing another kind smoothing . smoothing unit adapted generate one smoothed pulmonary image several smoothed pulmonary image , wherein several smoothed pulmonary image smoothed different degree smoothing . determining image quality image quality determination unit preferentially us one differently smoothed pulmonary image preferentially also provided , unsmoothed pulmonary image . embodiment signal value determination unit configured segment lung vessel respective pulmonary image based image value respective pulmonary image , determine one several lung vessel value indicative amount lung vessel respective pulmonary image based segmented lung vessel determine one several signal value respective pulmonary image based determined one several lung vessel value . particular , signal value determination unit configured subdivide segmented lung vessel cross-sectional subelements determine one several lung vessel value based number cross-sectional subelements . subdividing respective segmented lung vessel cross-sectional subelements predefined thickness respective cross-sectional subelement respective segmented lung vessel preferentially assumed . number cross-sectional subelements respective pulmonary image provides reliable measure detectability lung vessel pulmonary image , lead improved signal value hence improved determination image quality . signal value determination unit adapted determine single signal value respective pulmonary image determine several signal value respective pulmonary image , wherein latter case different signal value pulmonary image preferentially correspond different size lung vessel . thus , size lung vessel might subdivided size class size class pulmonary image corresponding signal value determined . instance , size cross-sectional subelements determined subdivided size class number cross-sectional subelements size respective size class respective pulmonary image regarded signal value respective size class , i.e . respective structure size , respective pulmonary image . size cross-sectional subelement determined area-equivalent diameter . however , also another size measure might used . noise value determination unit preferentially configured determine one several noise value respective pulmonary image based image value respective pulmonary image , represent segmented lung vessel . thus , preferentially one several noise value relate entire respective pulmonary image , part respective pulmonary image show lung vessel . using noise value determining image quality , determination image quality improved . particular , noise value determination unit configured determine cross-sectional subelement respective noise subvalue indicative noise respective cross-sectional subelement determine one noise value based determined noise subvalues . noise value determination unit adapted determine single noise value respective pulmonary image determine several noise value respective pulmonary image , wherein latter case different noise value pulmonary image preferentially correspond different size lung vessel . thus , size lung vessel might subdivided size class size class pulmonary image corresponding noise value determined . instance , size cross-sectional subelements determined subdivided size class average noise subvalues cross-sectional subelements size respective size class respective pulmonary image regarded noise value respective size class , i.e . respective structure size , respective pulmonary image . determining single noise value pulmonary image noise subvalues , determined pulmonary image , averaged . preferentially determination signal value based number cross-sectional subelements determination noise value based noise subvalues , determined cross-sectional subelements , wherein consideration cross-sectional subelements lead reliable determination signal value noise value hence improved determination image quality , based signal value noise value . preferred embodiment signal value determination unit adapted determine signal value different size lung vessel different pulmonary image , wherein noise value determination unit adapted determine noise value different size lung vessel different pulmonary image , wherein image quality determination unit configured provide reference signal value reference noise value different size lung vessel determine image quality indicative deviation ) determined signal value determined noise value b ) provided reference signal value reference noise value . thus , deviation determined signal noise value provided reference signal noise value determined , wherein image quality indicative deviation . provided , unsmoothed pulmonary image lung nodule , present , reliably detectable , wherein detectability lung nodule pulmonary image often depends radiation dose applied patient generating pulmonary image , i.e . higher radiation dose applied image larger detectability lung nodule pulmonary image . hand , radiation dose applied patient higher really required reliably detecting lung nodule , radiation dose adverse effect patient , i.e . radiation dose low reasonably achievable , wherein requirement also called `` alara '' principle . reference signal noise value preferentially provided pulmonary image value accordance alara principle . determined image quality measure degree conformity alara principle . preferentially , image quality determination unit configured ) determine first manifold based signal value noise value determined different size lung vessel , wherein surface determined space defined signal value dimension , noise value dimension lung vessel size dimension , ii ) determine second manifold space based reference signal value reference noise value provided different size lung vessel , iii ) determine distance first second manifold several location first manifold , iv ) determine image quality based distance . moreover , image quality determination unit preferentially configured determine area first manifold , i.e . first surface , determined distance larger predefined distance threshold , determine image quality based area . area single area comprise several separate subareas . particular , image quality determination unit configured determine image quality depending size area second manifold , i.e . second surface , and/or depending size area second manifold defined space . instance , image quality determination unit configured determine insufficient image quality , deviation measure indicates size area second manifold larger predefined size threshold . thus , deviation measure determined signal value noise value reference signal value noise value size area second manifold defined reference value and/or size area second manifold used determining image quality . using size area , comprise several separate subareas , determination image quality improved . embodiment pulmonary image providing unit configured provide slice image slice thickness pulmonary image , wherein signal value determination unit adapted normalize respective signal value respect slice thickness . slice image one several voxels slice direction , i.e . direction perpendicular plane embodiment pulmonary image mainly extends . moreover , signal value determination unit configured determine lung space space occupied lung within least one pulmonary image normalize respective signal value respect determined lung space . preferentially , lung space , might lung volume , determined provided , unsmoothed pulmonary image . lung space , i.e . size space within pulmonary image covered lung , determined using known lung segmentation technique . using normalization quality signal value hence determination image quality , based , inter alia , signal value , improved . system comprise dose level determination unit configured determine radiation dose level applied generating next pulmonary image based determined image quality . particular , pulmonary image providing unit configured provide pulmonary image image generated applying certain radiation dose level lung , wherein dose level determination unit configured determine radiation dose level applied generating next pulmonary image smaller certain radiation dose level , determined image quality larger predefined quality threshold . instance , image quality determined based size area first manifold second manifold described , wherein predefined quality threshold corresponds predefined size threshold area , image quality larger predefined quality threshold indicate image quality higher required reliably detecting lung nodule provided , unsmoothed pulmonary image , wherein allows reduction radiation dose level applied lung . also image quality determined using size area , using another measure determining deviation determined signal noise value provided reference signal noise value , determined image quality larger predefined quality threshold indicate unnecessarily high image quality , allows reduction radiation dose level . lead improved alignment generation next pulmonary image alara principle . system comprise user interface allowing user modify provided pulmonary image , wherein system adapted determine image quality based modified pulmonary image . thus , processing step carried based modified pulmonary image . allows user interactively modify pulmonary image thereby determine property provided unsmoothed pulmonary image influence image quality . instance , user interactively change setting dynamic window/level spatial screen resolution . aspect present invention method assessing pulmonary image presented , wherein method comprises : providing pulmonary image , comprises image element image value assigned show lung vessel , pulmonary image providing unit , smoothing provided pulmonary image provide different pulmonary image different degree smoothing smoothing unit , determining signal value different pulmonary image signal value determination unit , wherein respective pulmonary image one signal value , indicative detectability lung vessel respective pulmonary image , determined based image value respective pulmonary image , determining noise value different pulmonary image noise value determination unit , wherein respective pulmonary image one noise value , indicative noise respective pulmonary image , determined based image value respective pulmonary image , determining image quality provided unsmoothed image based signal value noise value determined different pulmonary image image quality determination unit . another aspect present invention computer program assessing pulmonary image presented , wherein computer program comprises program code mean causing system assessing pulmonary image defined claim 1 carry step method assessing pulmonary image defined claim 14 , computer program run computer controlling system . shall understood system claim 1 , method claim 14 , computer program claim 15 , similar and/or identical preferred embodiment , particular , defined dependent claim . shall understood preferred embodiment present invention also combination dependent claim embodiment respective independent claim . aspect invention apparent elucidated reference embodiment described hereinafter . brief description drawing following drawing : fig 1 show schematically exemplarily embodiment system assessing pulmonary image , fig 2 illustrates dependence number detectable lung vessel image smoothness , fig 3 show flowchart exemplarily illustrating embodiment method assessing pulmonary image , fig 4 show exemplarily curve illustrating change number cross-sectional subelements lung vessel noise different degree smoothing single structure size , fig 5 show exemplarily graph illustrating influence structure size curve shown fig 4 , andfigs 6 7 show exemplarily graph illustrating number detectable cross-sectional subelements lung vessel noise differ different provided pulmonary image . detailed description embodiment fig 1 show schematically exemplarily embodiment system assessing pulmonary image . system 1 comprises pulmonary image providing unit 2 configured provide pulmonary image comprises image element image value assigned show lung vessel . embodiment pulmonary image providing unit 2 storing unit pulmonary image stored , wherein storing unit adapted provide stored pulmonary image . moreover , embodiment pulmonary image computed tomography image lung patient . patient preferentially human . however , patient also animal . system 1 comprises smoothing unit 3 spatially smoothing provided pulmonary image different degree smoothing , order generate different , smoothed pulmonary image . embodiment smoothing unit 3 adapted apply gaussian image smoothing . resulting pulmonary image correspond different level scale , word , different scale space level . system 1 comprises signal value determination unit 4 configured determine signal value differently smoothed pulmonary image also provided , unsmoothed pulmonary image , wherein respective pulmonary image one several signal value , indicative detectability lung vessel respective pulmonary image , determined based image value respective pulmonary image . embodiment signal value determination unit 4 configured segment lung vessel respective pulmonary image based image value respective pulmonary image , determine lung vessel value indicative amount lung vessel respective pulmonary image based segmented lung vessel determine several signal value respective pulmonary image based determined lung vessel value . particular , signal value determination unit 4 configured subdivide segmented lung vessel cross-sectional subelements , determine size cross-sectional subelements , subdivide size size class determine size class respective lung vessel value based number cross-sectional subelements size respective size class . determining cross-sectional subelements predefined thickness respective cross-sectional subelement used . however , also another technique used determining cross-sectional subelements . fig 2 schematically exemplarily show three pulmonary image 20 , 21 , 22 , smoothed different degree smoothing , wherein degree smoothing , i.e . image smoothness , increase left right fig 2. seen , number detectable lung vessel decrease left right , i.e . increasing image smoothness . embodiment pulmonary image providing unit 2 configured provide unsmoothed pulmonary image slice image slice thickness , wherein signal value determination unit 4 adapted normalize respective signal value respect slice thickness . moreover , signal value determination unit 4 configured determine lung volume within provided , unsmoothed pulmonary image normalize respective signal value respect determined lung volume . thus , size volume occupied lung within provided , unsmoothed pulmonary image determined , instance , segmenting lung within pulmonary image , wherein resulting size used normalizing determined signal value . system 1 comprises noise value determination unit 5 configured determine noise value differently smoothed pulmonary image provided , unsmoothed pulmonary image , wherein respective pulmonary image one several noise value , indicative noise respective pulmonary image , determined based image value respective pulmonary image . particular , noise value determination unit 5 configured determine one several noise value indicative noise image value representing lung vessel respective pulmonary image . thus , noise value determination unit 5 adapted determine one several noise value respective pulmonary image based image value respective pulmonary image , represent segmented lung vessel . embodiment noise value determination unit 5 configured determine cross-sectional subelement respective noise subvalue indicative noise respective cross-sectional subelement determine noise value respective size , i.e . respective size class , based noise subvalues determined cross-sectional subelements respective size class . determining noise value absolute magnitude hounsfield laplacians used noise value determination unit 5. however , also known technique determining noise value based image value image used determining noise value noise value determination unit 5. system comprises image quality determination unit 6 configured determine image quality provided unsmoothed image based signal value noise value determined differently smoothed pulmonary image provided , unsmoothed pulmonary image . particular , image quality determination unit 6 configured provide reference signal value reference noise value different size lung vessel determine image quality indicative deviation ) signal value noise value determined different size b ) reference signal value reference noise value provided different size . embodiment image quality determination unit 6 configured ) determine first surface based signal value noise value determined different size , wherein surface determined three-dimensional space defined possible value signal value , noise value different size lung vessel , ii ) determine second surface three-dimensional space based reference signal value reference noise value provided different size , iii ) determine distance first second surface several location first surface , iv ) determine image quality based distance . instance , image quality determination unit 6 configured determine area first surface , determined distance larger predefined distance threshold determine image quality based area single area comprise several separate subareas , wherein subareas second surface , second surface wherein subareas second surface area second surface . particular , image quality determination unit 6 configured determine image quality depending size area second surface and/or depending size area second surface three-dimensional image . embodiment image quality determination unit 6 configured determine image quality high reduced , size area second surface larger predefined first size threshold . moreover , image quality determination unit 6 configured determine image quality low hence insufficient reliably detecting lung nodule , size area second surface , determined distance larger predefined distance threshold , larger predefined second size threshold . first second size threshold different . threshold like size threshold distance threshold predetermined calibration procedure and/or modifiable user . image quality determination unit 6 adapted determine image quality desired , especially accordance alara principle , size area first surface , determined distance larger predefined distance threshold , first surface smaller first size threshold second surface smaller second size threshold . system 1 comprises dose level determination unit 7 configured determine radiation dose level applied generating next pulmonary image based determined image quality . embodiment pulmonary image providing unit 2 configured provide pulmonary image image generated applying certain radiation dose level lung , wherein dose level determination unit 7 configured determine radiation dose level applied generating next pulmonary image smaller certain radiation dose level , determined image quality larger predefined quality threshold . mean particularly radiation dose level applied generating next pulmonary image determined smaller certain radiation dose level , size area first surface , determined distance larger predefined distance threshold , second surface larger first size threshold . system 1 comprises user interface 8 allowing user modify provided , unsmoothed pulmonary image , wherein system 1 adapted determine image quality based modified pulmonary image . thus , processing step carried based modified pulmonary image . user therefore change setting dynamic window/level and/or spatial screen resolution , wherein procedure like smoothing , signal value determination , noise value determination image quality determination procedure carried based modified pulmonary image . system 1 comprises input unit 9 like keyboard , computer mouse , touch screen , et cetera output unit 10 including display showing , instance , pulmonary image indicating determined image quality . output unit 10 also comprise acoustic unit acoustically indicating determined image quality . following embodiment method assessing pulmonary image described reference flowchart shown fig 3. step 101 pulmonary image , comprises image element image value assigned show lung vessel , provided pulmonary image providing unit 2. step 102 provided pulmonary image smoothed different degree smoothing smoothing unit 3 , signal value determined differently smoothed pulmonary image unsmoothed pulmonary image signal value determination unit 4 , wherein respective pulmonary image signal value , indicative detectability lung vessel respective size respective pulmonary image , determined based image value respective pulmonary image , noise value determined different pulmonary image noise value determination unit 5 , wherein respective pulmonary image noise value , indicative noise respective pulmonary image , determined different size lung vessel based image value respective pulmonary image . step 103 image quality provided unsmoothed image determined based signal value noise value determined different pulmonary image image quality determination unit 6. generally , screening lung cancer low dose computed tomography scanning recognized efficient detecting lung nodule , wherein assurance image quality highly required , especially avoiding oversight . screening lung cancer use radiation dose low reasonably achievable one hand , hand image quality level maintained , sufficient ensure detection possible lung tumor , i.e . possible lung nodule . thus , screening lung cancer accordance alara principle . system method described assessing pulmonary image allows determination whether image quality , i.e . image quality level , provided pulmonary image sufficient achieve alara principle . moreover , system method assessing pulmonary image vendor-agnostic , retrospectively applied batch readily reconstructed pulmonary image might received picture archiving communication system ( pac ) . furthermore , determined image quality specific respective pulmonary image hence respective patient respective imaging protocol used generating provided pulmonary image . lung vessel detectable provided pulmonary image course patient specific likely reliably present pulmonary image lung vessel different size well suited marker image quality . system method assessing pulmonary image provides automatic quantitative assessment pulmonary image based signal value , i.e . detectability , lung vessel different size provided pulmonary image based noise value determined part pulmonary image represented lung vessel , wherein also noise value determined different size lung vessel . lung vessel always present pulmonary image , wherein large range lung vessel size , case computed tomography , hounsfield density cover respective range potential lung nodule well . notion , clear deficiency number detectable , especially visible , lung vessel observed , strong indicator insufficient image quality reliably detecting potential nodule . vessel density may vary patient , average group patient much evenly number tumor , i.e . number lung nodule . thus , determining average image quality averaging image quality determined different image different patient lead robust quality measure , i.e . robust image quality , indicative quality imaging system , generally , , instance , screening center image generated . described first surface first manifold space spanned three dimension signal , noise structure size , i.e . size lung vessel pulmonary image , wherein first manifold used assessing quality provided pulmonary image . large part first manifold lower ideal reference manifold , i.e . second surface , assumed provided pulmonary image meet necessary image quality requirement . another embodiment second surface , i.e . reference manifold , might correspond manifold another imaging system , i.e . imaging system used generating provided pulmonary image , like imaging system another screening center , provided pulmonary image generated imaging system screening center , order compare image quality , instance , different screening center . system method assessing pulmonary image consider single signal/noise operating point , establishes compare determined first manifold , i.e . determined first surface , range possible size possible lung nodule pulmonary image . pulmonary image providing unit 2 adapted provide pulmonary image identifying lung volume provided overall thoracic computed tomography volume image segmenting identified lung volume overall thoracic computed tomography volume image , order provide pulmonary image substantially includes lung volume part outside lung volume . segmentation lung vessel respective pulmonary image regarded automatic detection image structure respective pulmonary image conformant lung vessel , wherein segmentation procedure use known segmentation technique like connected component analysis multitude hounsfield threshold wherein resulting structure , structure two-dimensional cross section area size lower predefined upper area size limit selected . corresponding resulting segmented lung vessel schematically exemplarily shown fig 2. system method assessing pulmonary image compute vessel-like structure object , i.e . described embodiment cross-sectional subelement segmented lung vessel , size noise using , instance , area-equivalent diameter size absolute magnitude hounsfield laplacians determining noise . preferred embodiment number detected vessel cross-sectional subelements per lung volume , also regarded frequency , accumulated histogram different pulmonary image depending structure size noise value . resulting graph , also considers normalization respect thickness slice pulmonary image normalization volume lung pulmonary image , exemplarily shown fig 4 single structure size . fig 4 arrow 30 indicates direction increasing number cross-sectional subelements , i.e . increasing signal value , arrow 31 indicates increasing noise , i.e . increasing noise value . moreover , fig 4 point 32 correspond differently smoothed pulmonary image point 33 corresponds initially provided , unsmoothed pulmonary image . line 34 point point . thus , fig 4 show number vessel cross section , might also regarded frequency vessel cross section normalized respect lung volume , noise value different degree smoothing single structure size , wherein example , described , noise estimated absolute magnitude hounsfield laplacians , i.e . deviation linear interpolation neighbor voxels . original pulmonary image prior smoothing highest noise highest number vessel cross section indicated point 33. increasing degree smoothing , may obtained successively applying smoothing operation like gaussian smoothing operation pulmonary image , noise level cross section reduced , also number detectable cross section diminishes . fig 4 show relation number vessel cross section , i.e . signal value , noise value specific structure size specific size class , i.e . embodiment one area-equivalent diameter diameter class cross-sectional subelements detectable pulmonary image . fig 5 show exemplarily relation number cross section noise different structure size , i.e . example different area-equivalent diameter segmented lung vessel . fig 5 arrow 35 indicates direction increasing structure size curve 36 corresponds smallest structure size curve 37 corresponds largest structure size . seen fig 5 , number detectable lung vessel hence number detectable cross-sectional subelements lung vessel decrease predominantly small structure size , staying relatively stable larger structure size . curve together span first surface , i.e . first manifold , three-dimensional space defined number vessel cross section , i.e . possible signal value , indicated arrow 30 , possible noise value indicated arrow 31 possible structure size indicated arrow 35. thus , three-dimensional space spanned signal dimension 30 , noise dimension 31 structure size dimension 35. preferentially , first surface determined based curve shown fig 5 first surface directed surface direction pointing towards higher detection frequency . first surface preferentially determined fitting signal value - noise value - structure size point three-dimensional space , wherein embodiment signal value defined number cross-sectional subelements segmented lung vessel respective structure size . fitting , instance , piecewise linear fitting , polynomial fitting , spline fitting , support vector fitting , et cetera . fig 6 exemplarily show two curve 38 , 39 , correspond different reconstruction computed tomography scan , i.e . computed tomography projection data used differently reconstructing two different pulmonary image , wherein first curve 38 determined based first reconstructed computed tomography image second curve 39 determined based second reconstructed computed tomography image . example , first reconstructed computed tomography image reconstructed corresponds relatively high spatial frequency second reconstructed image reconstructed corresponds relatively low spatial frequency . sense first image could also regarded hard image second image could also regarded soft image . respective provided , unsmoothed image indicated point 40 , 41 , respectively . first , high-frequency image noise structure , i.e . cross-sectional subelements segmented lung vessel second , low-frequency image . however , increasing degree smoothing , i.e. , instance , successive gaussian smoothings , becomes apparent curve quite close , indicating similar image quality . fig 6 show curve single structure size , i.e . embodiment single area-equivalent diameter . corresponding curve determined also structure size curve span respective surface defined three-dimensional space , wherein increasing degree smoothing two surface quite close . fig 7 show schematically exemplarily four curve 41 ... 44 single structure size , i.e . single size class , wherein different curve 41 ... 44 correspond four different provided pulmonary image . seen , curve 41 corresponds provided pulmonary image showing significantly structure almost noise level provided pulmonary image . also , various structure size ( fig 7 curve correspond single structure size ) four curve 41 ... 44 generalize four two-dimensional surface compared , instance , height relative distance . embodiment two-dimensional surface , i.e . first surface , three-dimensional space determined provided pulmonary image two-dimensional first surface compared reference surface , corresponds desired image quality , order yield metric image quality provided pulmonary image . instance , signed distance first surface second , reference surface determined used assessing image quality provided pulmonary image . reference surface , also regarded image quality surface , taken reference base certain screening center . reference optionally stratified patient cohort like male/female , age , body mass index , et cetera . thus , patient reference surface provided corresponds respective patient respect feature like gender , age , et cetera . embodiment image quality determination unit 6 adapted determine image quality provided pulmonary image insufficient lung nodule detection , many area directed surface reference quality surface , i.e . second surface . vice versa , many area surface second surface , determined image acquisition dose could lowered satisfy alara principle . determination whether many area second surface based predefined area size threshold described , might determined , instance , calibration . using user interface quality check implemented interactive viewing software . user change setting dynamic window/level , spatial screen resolution , et cetera , wherein quality check , i.e . determination image quality , performed interactively chosen setting , wherein user warned , resulting display quality appears insufficient exhaustive detection nodule . described system method assessing pulmonary image require scanning physical phantom . provided quantitative image quality measure preferentially general scan type reconstruction type , specific respective patient , i.e. , instance , patient 's size , body mass , bone , et cetera . moreover , specific actual imaging protocol , i.e. , instance , case computed tomography imaging specific tube current , tube voltage , et cetera . also specific resolution change along actual course dynamic dose modulation across scan , i.e . automatic variation dose denser area base apex lung , present . furthermore , specific possible resolution loss due suboptimal placement patient respect field-of-view centering , i.e . specific possible tapering resolution towards off-centered location . also specific sub-optimal choice reconstruction field-of-view , i.e . unnecessary large field-of-views limit voxel spacing . finally , specific respectively chosen reconstruction algorithm , i.e. , instance , kernel , whether iterative reconstruction , et cetera . system method assessing pulmonary image cover range interest size image value , i.e . case computed tomography hounsfield density , respect lung nodule detection . moreover , assessment quality pulmonary image applied retrospectively manufacturer imaging system . also applied batch image like image certain time period certain patient cohort individual image . batch image considered , image respective image quality determined , wherein image quality combined determining image quality batch image . instance , image quality averaged and/or standard deviation image quality determined determining image quality batch image . determining image quality respective image distance respective first surface reference surface combined , particular averaged . thus , deviation two-dimensional noise-resolution surface reference surface condensed single signed scalar number , instance , averaging distance reference surface , thereby generating image number , wherein number determined different image , might correspond different patient , averaged and/or standard deviation determined , order determine image quality batch image . image quality determined relatively fast , required , re-scan patient performed relatively fast , especially patient leaf hospital . system method assessing pulmonary image applied , instance , low dose screening computed tomography image , also computed tomography image , also image showing lung vessel might generated using computed tomography imaging system . although described embodiment three-dimensional space considered , defined signal dimension , noise dimension structure size dimension , another embodiment dimension considered like structure contrast , wherein case respective three-dimensional manifold , i.e . three-dimensional surface , defined corresponding four-dimensional space compared determining image quality . particular , increasing degree smoothing low contrast structure start vanish earlier high contrast structure . embodiment structure contrast defined difference mean brightness , i.e. , instance , mean hounsfield density , vessel cross section neighborhood , i.e . directly adjacent image background . variation disclosed embodiment understood effected skilled art practicing claimed invention , study drawing , disclosure , appended claim . claim , word `` comprising '' exclude element step , indefinite article `` '' `` '' exclude plurality . single unit device may fulfill function several item recited claim . mere fact certain measure recited mutually different dependent claim indicate combination measure used advantage . procedure like provision pulmonary image , smoothing provided pulmonary image , segmentation lung vessel pulmonary image , determination cross-sectional subelements lung value , determination size cross-sectional subelements , determination number cross-sectional subelements , determination noise value cross-sectional subelements , determination image quality et cetera performed one several unit device performed number unit device . instance , procedure carried single device . procedure and/or control system assessing pulmonary image accordance method assessing pulmonary image implemented program code mean computer program and/or dedicated hardware . computer program may stored/distributed suitable medium , optical storage medium solid-state medium , supplied together part hardware , may also distributed form , via internet wired wireless telecommunication system . reference sign claim construed limiting scope . invention relates system assessing pulmonary image allows improved assessment respect lung nodule detectability . pulmonary image smoothed providing different pulmonary image different degree smoothing , wherein signal value noise value , indicative lung vessel detectability noise image , determined used determining image quality indicative usability pulmonary image assessed detecting lung nodule . since pulmonary image show lung vessel many different vessel size many different image value , cover respective range potential lung nodule generally well , image quality determination based different pulmonary image different degree smoothing allows reliable assessment pulmonary image 's usability detecting lung nodule . _____c : 1. system assessing pulmonary image , system ( 1 ) comprising : - pulmonary image providing unit ( 2 ) configured provide pulmonary image comprises image element image value assigned show lung vessel , - smoothing unit ( 3 ) configured smooth provided pulmonary image providing different pulmonary image different degree smoothing , - signal value determination unit ( 4 ) configured determine signal value different pulmonary image ( 20 , 21 , 22 ) , wherein respective pulmonary image one several signal value , indicative detectability lung vessel respective pulmonary image , determined based image value respective pulmonary image , - noise value determination unit ( 5 ) configured determine noise value different pulmonary image ( 20 , 21 , 22 ) , wherein respective pulmonary image one several noise value , indicative noise respective pulmonary image , determined based image value respective pulmonary image , - image quality determination unit ( 6 ) configured determine image quality provided unsmoothed image based signal value noise value determined different pulmonary image ( 20 , 21 , 22 ) . 2. system defined claim 1 , wherein signal value determination unit ( 4 ) configured segment lung vessel respective pulmonary image based image value respective pulmonary image , determine one several lung vessel value indicative amount lung vessel respective pulmonary image based segmented lung vessel determine one several signal value respective pulmonary image based determined one several lung vessel value . 3. system defined claim 2 , wherein signal value determination unit ( 4 ) configured subdivide segmented lung vessel cross-sectional subelements determine one several lung vessel value based number cross-sectional subelements . 4. system defined claim 2 , wherein noise value determination unit ( 5 ) configured determine one several noise value respective pulmonary image based image value respective pulmonary image , represent segmented lung vessel . 5. system defined claim 3 , wherein noise value determination unit ( 5 ) configured determine cross-sectional subelement respective noise subvalue indicative noise respective cross-sectional subelement determine one several noise value based determined noise subvalues . 6. system defined claim 1 , wherein signal value determination unit ( 4 ) adapted determine signal value different size lung vessel different pulmonary image , wherein noise value determination unit ( 5 ) adapted determine noise value different size lung vessel different pulmonary image , wherein image quality determination unit ( 6 ) configured provide reference signal value reference noise value different size lung vessel determine image quality indicative deviation ) determined signal value determined noise value b ) provided reference signal value reference noise value . 7. system defined claim 6 , wherein image quality determination unit ( 6 ) configured ) determine first manifold based signal value noise value determined different size lung vessel , wherein surface determined space defined signal value dimension , noise value dimension lung vessel size dimension , ii ) determine second manifold space based reference signal value reference noise value provided different size lung vessel , iii ) determine distance first second manifold several location first manifold , iv ) determine image quality based distance . 8. system defined claim 7 , wherein image quality determination unit ( 6 ) configured determine area first manifold , determined distance larger predefined distance threshold , determine image quality based area . 9. system defined claim 8 , wherein image quality determination unit ( 6 ) configured determine image quality depending size area second manifold and/or depending size area second manifold three-dimensional space . 10. system defined claim 1 , wherein signal value determination unit ( 4 ) configured determine lung space space occupied lung within least one pulmonary image ( 20 , 21 , 22 ) normalize respective signal value respect determined lung space . 11. system defined claim 1 , wherein pulmonary image providing unit ( 2 ) configured provide slice image slice thickness pulmonary image , wherein signal value determination unit ( 4 ) adapted normalize respective signal value respect slice thickness . 12. system defined claim 1 , wherein system ( 1 ) comprises dose level determination unit ( 7 ) configured determine radiation dose level applied generating next pulmonary image based determined image quality . 13. system defined claim 12 , wherein pulmonary image providing unit ( 2 ) configured provide pulmonary image image generated applying certain radiation dose level lung , wherein dose level determination unit ( 7 ) configured determine radiation dose level applied generating next pulmonary image smaller certain radiation dose level , determined image quality larger predefined quality threshold . 14. method assessing pulmonary image , method comprising : - providing pulmonary image , comprises image element image value assigned show lung vessel , pulmonary image providing unit ( 2 ) , - smoothing provided pulmonary image provide different pulmonary image different degree smoothing smoothing unit ( 3 ) , - determining signal value different pulmonary image ( 20 , 21 , 22 ) signal value determination unit ( 4 ) , wherein respective pulmonary image one signal value , indicative detectability lung vessel respective pulmonary image , determined based image value respective pulmonary image , - determining noise value differently pulmonary image ( 20 , 21 , 22 ) noise value determination unit ( 5 ) , wherein respective pulmonary image one noise value , indicative noise respective pulmonary image , determined based image value respective pulmonary image , - determining image quality provided unsmoothed image based signal value noise value determined different pulmonary image ( 20 , 21 , 22 ) image quality determination unit ( 6 ) . 15. computer program assessing pulmonary image , computer program comprising program code mean causing system assessing pulmonary image defined claim 1 carry step method assessing pulmonary image defined claim 14 , computer program run computer controlling system .\", \"setting motion trigger level _____2019_____3503028_____487783355_____ep3500000.txt_____g06t_____g06t2207/10016 : g06t2207/20021 : g06t2207/30232 : g06t7/20 : g06t7/246 : h04l65/607 method setting motion trigger level used detection motion video stream depicting scene presented . method comprising : receiving ( 302 ) data pertaining video stream depicting scene ; dividing ( 304 ) scene plurality specific portion ; wherein image frame video stream comprises multiple block pixel , wherein specific portion scene associated one block pixel ; specific portion scene : evaluating ( 306 ) , time , statistical feature bitrate associated encoding block pixel pertaining specific portion scene ; determining ( 308 ) motion base level based evaluated statistical feature bitrate associated encoding block pixel pertaining specific portion scene ; setting ( 310 ) motion trigger level based motion base level . also circuitry configured set motion trigger level used detection motion video stream depicting scene presented . _____d : technical field present invention relates method circuitry setting motion trigger level used detection motion video stream depicting scene . background monitoring equipment used sort environment society . simplest form , video stream scene observed personnel alert subsequently raised manually . automating monitoring , cost-effective robust monitoring system may achieved . often , feature interest monitoring system related motion scene . general , reliable robust automatic motion detection difficult achieve . simple algorithm motion detection compare current image reference image simply register change pixel intensity level . however , simple algorithm may prone trigger false alarm due change related irrelevant motion , swaying tree wave puddle . false alarm may , turn , result increased cost related response . order reduce number false alarm different filter may used . example filter small object filtering swaying object filtering . implementation filter may complex even using number false alarm may high . summary invention object present inventive concept least reduce problem providing method setting motion trigger level used detection motion video stream depicting scene . according first aspect , object achieved method setting motion trigger level used detection motion video stream depicting scene . method comprising : receiving data pertaining video stream depicting scene ; dividing scene plurality specific portion , wherein image frame video stream comprises multiple block pixel , wherein specific portion scene associated one block pixel ; specific portion scene : evaluating , time , statistical feature bitrate associated encoding block pixel pertaining specific portion scene ; determining motion base level based bitrate associated encoding block pixel pertaining specific portion scene ; setting motion trigger level based motion base level . mean present method possible set motion trigger level specific portion scene . received data pertaining video stream depicting scene may comprise data pertaining video stream . hence , data video frame video stream . data may subsequently encoded using video coding . order video stream take le storage space upon storing video stream digital storage medium and/or le bandwidth upon transmitting video stream digital network . encoding two main mode typically used compress video frame video stream video frame : intra-mode inter-mode . intra-mode spatial redundancy exploited , i.e . correlation among pixel within one frame . inter-mode temporal redundancy frame exploited . usually , frame encoded partitioned independent block pixel compressed encoded individually . according non-limiting example , block pixel may comprise one coding unit . coding unit may e.g . macroblock coding tree unit . representation coding unit may used . forming temporally independent frame , called intra frame , intra coding used . hence , block pixel frame intra coded . forming temporally dependent frame , called inter frame ( e.g . h.264 p- b-frames ) , intra coding well inter coding may used . hence , least block pixel frame inter coded . however , inter frame one block pixel may intra coded . result encoding , bitrates different block pixel may determined . alternatively , additionally , received data pertaining video stream depicting scene may comprise data pertaining bitrates associated encoding block pixel video stream . dividing scene plurality specific portion , different motion base level may determined different specific portion scene . example , may possible determine one motion base level driveway , another couple tree present scene . dividing scene plurality specific portion , different motion trigger level may set different specific portion scene . example , may possible set one motion trigger level driveway , another couple tree present scene . associating block pixel specific portion scene , shape specific portion may freeform . example , specific portion may outline shape couple tree . evaluating , time , statistical feature bitrate associated encoding block pixel , motion base level may determined based information gathered extended period time . evaluating , time , statistical feature bitrate associated encoding block pixel pertaining specific portion scene , motion base level may determined relation level motion usually present specific portion scene . example , specific portion scene including motion associated tree leaf constantly moving wind may influence motion base level specific portion accordingly . specific portion scene including motion , e.g . empty driveway , may influence motion base level specific portion . evaluating , time , statistical feature bitrate associated encoding block pixel pertaining specific portion scene , motion base level may influenced real event occurring . real event may noticeable statistical feature , evaluated time , bitrate associated encoding block pixel pertaining specific portion scene , may influence motion base level specific portion scene . beneficial since motion base level may determined setting real event may occur . motion base level may determined normal monitoring scene , may require calibration set-up real event occur . motion base level may determined normal monitoring scene , real event occurs occasionally . `` real event '' construed event motion trigger desired . high bitrates associated encoding block pixel pertaining specific portion scene may related one following : complex detail , noise , pattern , movement , encoder setting . high bitrates may imply large change frame video stream . large change frame necessarily indicate change interest . large change frame may imply high motion level scene . high motion level reveal motion interest . motion base level may determined relation normal level motion specific portion scene . motion base level may determined representation statistical feature , evaluated time , motion level specific portion scene . motion trigger level may set relation normal level motion specific portion scene . `` statistical feature '' construed one following : mean , weighted mean , arithmetic mean , weighted arithmetic mean , moving mean , moving weighted mean , variance , moving variance , standard deviation , moving standard deviation , geometric mean , moving geometric mean , weighted geometric mean , moving weighted geometric mean , geometric standard deviation , moving geometric standard deviation , harmonic mean , moving harmonic mean , weighted harmonic mean , moving weighted harmonic mean , median , moving median , weighted median , moving weighted median , quartile , quartile range , mode . statistical feature bitrate associated encoding block pixel may primarily acquired inter coded frame . preferably , statistical feature bitrate associated encoding block pixel solely acquired inter coded frame . bitrate associated encoding block pixel acquired inter coded frame may related change scene . high bitrate associated inter coded frame may imply large change frame video stream . large change frame necessarily indicate change interest . large change frame may imply high motion level scene . high motion level reveal motion interest . bitrate associated encoding block pixel acquired intra coded frame may related complexity scene . example , specific portion scene comprising empty driveway may high bitrate associated encoding intra-coded frame , i.e . may , e.g. , complex detail driveway . specific portion scene comprising empty driveway may low bitrate associated encoding inter-coded frame , i.e . may small change driveway . thus , motion base level specific portion comprising empty driveway may low . example , specific portion scene comprising swaying tree may , depending complexity specific portion scene , high low bitrate associated encoding intra-coded frame . , bitrate associated encoding inter-coded frame specific portion may , since tree swaying , high . thus , motion base level specific portion comprising swaying tree may high . using bitrates inter coded frame , amount change portion scene may indicated . change portion scene may related motion level portion scene . advantage using bitrate inter coded frame may additional processing video stream needed order set motion trigger level different portion scene . encoding video stream routinely performed order save digital storage space , storing video stream digital memory , order save bandwidth transmitting video stream digital network . act determining motion base level may comprise : basing motion base level time average bitrate associated encoding block pixel pertaining specific portion scene . using time average bitrate associated encoding block pixel pertaining specific portion scene , motion base level may determined according level motion usually present specific portion scene . example , specific portion scene including motion associated tree leaf constantly moving wind may influence motion base level specific portion accordingly . specific portion scene including motion , e.g . empty driveway , may influence motion base level specific portion . using time average bitrate associated encoding block pixel pertaining specific portion scene , motion base level may influenced real event occurring . real event may noticeable time average bitrate associated encoding block pixel pertaining specific portion scene , may influence motion base level specific portion scene . hence , even true event occurring time used evaluating statistical feature , event great extent influence motion base level . one event happening time used evaluating statistical feature evened statistical sample . advantage since , calibration using event free environment needed . motion base level may determined data collected normal monitoring set-up . method may comprise : adjusting master threshold weight enable adjustment motion trigger level . master threshold weight may scale factor and/or constant offset . adjusting master threshold weight , may possible globally adjust motion trigger level . may beneficial enables fine-tuning determined motion trigger level . case trigger related false alarm , master threshold weight may adjusted . master threshold weight may affect motion trigger level number trigger related false alarm may reduced . case trigger related real event , master threshold weight may adjusted . master threshold weight may affect motion trigger level number trigger related real event may increased . adjusting master threshold weight , may possible globally adjust motion trigger level . may beneficial motion level scene higher normal , without need trigger . example , trigger caused high motion level related flock bird present scene may desired . adjusting master threshold weight , may possible globally adjust motion trigger level . may beneficial motion level scene lower normal . motion trigger level may adjusted lower setting . method may comprise : logging statistical feature bitrate associated encoding block pixel heatmap scene . advantage logging statistical feature bitrate associated encoding block pixel heatmap scene , may motion detected video stream scene may saved . graphical representation saved motion heatmap may also overlayed top graphical representation video stream , thereby highlighting motion feature scene , portion scene . heatmap may also indicate different level detected motion scene . level heatmap may also set relative motion base level portion scene . heatmap may also highlight object motion level motion trigger level . block pixel may correspond one coding unit . advantage using one coding unit block pixel may processing needed may reduced . `` coding unit '' may construed macroblocks coding tree unit . representation coding unit may used . block pixel may integer amount coding unit . act determining motion base level may comprise : basing motion base level specific time recurrent time period . specific time recurrent time period may construed one following : time day , day week , day month , week year , month year , day year , holiday , and/or season . advantage basing motion base level specific time recurrent time period may motion portion scene may alternate recurrent time period . example , may le motion scene holiday , motion base level may adjusted accordingly . may motion portion scene comprising e.g . tree summer , since tree may leaf constantly moving wind . act determining motion base level may comprise : basing motion base level specific time recurrent time period time average bitrate associated encoding block pixel pertaining specific portion scene pertaining specific time recurrent time period . motion base level specific time recurrent time period may therefore determined based motion , detected time , specific portion scene . example , one motion base level may determined based time average motion detected several weekend . different motion base level specific portion scene may determined based time average motion detected several weekday . may therefore possible determine motion base level account different level motion portion scene different specific time recurrent time period . may also possible determine motion base level account different level motion specific time recurrent time period different portion scene . act setting motion trigger level may comprise : basing motion trigger level specific time recurrent time period . may possible adjust motion trigger level depending specific time recurrent time period . example , may possible set specific motion trigger level specific portion day , different motion trigger level night . may advantageous since motion scene may different depending specific time recurrent time period , e.g . time day , day week . according second aspect present disclosure relates circuitry configured set motion trigger level used detection motion video stream depicting scene . circuitry comprising : data receiver configured receive data pertaining video stream depicting scene , wherein image frame video stream comprises multiple block pixel ; scene divider configured divide scene plurality specific portion , wherein specific portion scene associated one block pixel ; statistical feature evaluator configured , specific portion scene , evaluate , time , statistical feature bitrate associated encoding block pixel pertaining specific portion scene ; motion base level determinator configured , specific portion scene , determine motion base level based bitrate associated encoding block pixel pertaining specific portion scene ; motion trigger level controller configured , specific portion scene , set motion trigger level based motion base level corresponding specific portion scene . circuitry may comprise encoder configured encode video stream depicting scene using intra- inter-coding . data receiver may comprise image sensor configured capture video stream depicting scene . comprising image sensor , capturing video stream may performed device comprising circuitry performs evaluation motion detection . example , circuitry may comprised camera processing unit configured perform method method disclosed . mentioned feature advantage method , applicable , apply second aspect well . order avoid undue repetition , reference made . according third aspect present disclosure relates non-transitory computer readable storing medium stored thereon program implementing one method described text , executed device processing capability . mentioned feature advantage method , applicable , apply third aspect well . order avoid undue repetition , reference made . according fourth aspect present disclosure relates method motion detection . method comprising : installing camera configured capture video stream depicting scene , setting , according method first aspect , motion trigger level associated specific portion scene , triggering motion event upon bitrate associated encoding block pixel pertaining specific portion scene motion trigger level associated specific portion scene.triggering motion event may result real event occurring . act setting , according method first aspect , motion trigger level associated specific portion scene , may performed calibration phase . act triggering motion event upon bitrate associated encoding pixel pertaining specific portion scene motion trigger level associated specific portion scene , may performed motion detection phase . motion detection phase may occur calibration phase . motion trigger level may updated motion detection phase , repeating resuming calibration phase . repeating resuming calibration phase may done continuing motion detection phase . thus , may possible detect motion calibration phase . act setting , according method first aspect , motion trigger level associated specific portion scene , may performed plurality specific portion scene . setting motion trigger level associated specific portion scene plurality specific portion scene , may possible adjust motion trigger level specific portion scene . therefore , triggering motion event may performed relative normal motion level specific portion scene . example , specific motion level first specific portion scene may trigger motion event , specific motion level second specific portion scene may trigger motion event . act setting motion trigger level may performed predetermined time period . predetermined time period may order hour , day , week . predetermined time period may calibration phase . understood calibration phase may repeated resumed motion detection phase . calibration phase may repeated specific time recurrent time period . example , calibration phase may repeated monday every fourth week . repetition calibration phase may manually initiated . example , may desired update motion trigger level case many false alarm occur . act triggering may performed predetermined time period . act triggering may performed motion detection phase . mentioned feature advantage method , applicable , apply fourth aspect well . order avoid undue repetition , reference made . scope applicability present invention become apparent detailed description given . however , understood detailed description specific example , indicating preferred embodiment invention , given way illustration , since various change modification within scope invention become apparent skilled art detailed description . hence , understood invention limited particular component part device described step method described device method may vary . also understood terminology used herein purpose describing particular embodiment , intended limiting . must noted , used specification appended claim , article `` , '' `` , '' `` , '' `` said '' intended mean one element unless context clearly dictate otherwise . thus , example , reference `` unit '' `` unit '' may include several device , like . furthermore , word `` comprising '' , `` including '' , `` containing '' similar wording exclude element step . brief description drawing aspect present invention described detail , reference appended drawing showing embodiment invention . figure considered limiting invention specific embodiment ; instead used explaining understanding invention . illustrated figure , size layer region exaggerated illustrative purpose , thus , provided illustrate general structure embodiment present invention . like reference numeral refer like element throughout . figure 1 illustrates image frame video stream depicting scene.figure 2 block diagram circuitry configured set motion trigger level.figure 3 flow block scheme method setting motion trigger level.figure 4 illustrates diagram graphical representation bitrate , , time , t.figure 5 illustrates diagram multiple motion base level multiple motion trigger levels.figure 6 flow block scheme method motion detection . detailed description present invention described fully hereinafter reference accompanying drawing , currently preferred embodiment invention shown . invention may , however , embodied many different form construed limited embodiment set forth herein ; rather , embodiment provided thoroughness completeness , fully convey scope invention skilled person . figure 1 illustrates image frame 101 video stream depicting scene 100. data image frame represented pixel data . data may encoded using video coding . order video stream take le storage space upon storing video stream digital storage medium and/or le bandwidth upon transmitting video stream digital network . mentioned , encoding two main mode typically used compress video frame video stream video frame : intra-mode inter-mode . usually , frame encoded partitioned independent block pixel compressed encoded individually . according non-limiting example , block pixel may comprise one coding unit . coding unit may e.g . macroblock coding tree unit . representation coding unit may used . encoding , bitrates different block pixel may determined . scene 100 comprises multiple specific portion 102 , 106 , 108 , 110 , 112. schematically illustrated figure 1 , specific portion 102 , partly depicting road , composed block pixel 104. specific portion 106 , 108 , 110 , 112 also composed respective block pixel . specific portion may comprise one block pixel . since different specific portion 102 , 106 , 108 , 110 , 112 respective block pixel , bitrates associated encoding block pixel specific portion 102 , 106 , 108 , 110 , 112 identified . bitrates pertaining specific portion 102 , 106 , 108 , 110 , 112 may therefore used determining motion base level respective specific portion 102 , 106 , 108 , 110 , 112. example , specific portion 106 comprises road , may traffic day compared night . likewise , traffic may change depending day week . motion base level may therefore depend time day and/or day week . another example specific portion 108 comprising bush . motion base level specific portion 108 may depend season , since bush may trembling leaf winter . shown specific portion 110 , scene 100 may divided specific portion freeform shape . specific portion 112 comprises area motion may expected . thus , single scene 100 , may different specific portion 102 , 106 , 108 , 110 , 112 different motion base level requirement corresponding motion trigger level . figure 2 illustrates circuitry 200 configured set motion trigger level used detection motion video stream depicting scene . circuitry 200 may arranged camera configured capture video stream depicting scene . alternatively , circuitry 200 may arranged device separate camera . , device may configured receive data camera via digital network via kind connection camera capturing video stream device comprising circuitry 200. scene may e.g . scene like scene 100 illustrated figure 1. circuitry 200 comprises data receiver 202 , memory 203 , scene divider 204 , statistical feature evaluator 206 , motion base level determinator 208 , motion trigger level controller 210. circuitry 200 may comprise one processor 214. one processor 214 may single core processor multicore processor . one processor 214 may suitable processor performing digital data processing . circuitry 200 may comprise encoder 212. encoder may configured encode pixel data video stream encoded version video stream using intra inter mode encoding discussed . circuitry 200 may comprise motion trigger controller 218. motion trigger controller 218 may trigger motion event upon bitrate associated encoding block pixel pertaining specific portion scene motion trigger level associated specific portion scene . circuitry 200 may configured trigger motion event comprising motion trigger controller 218. one data receiver 202 , scene divider 204 , statistical feature evaluator 206 , motion base level determinator 208 , motion trigger level controller 210 , motion trigger controller 218 may implemented dedicated hardware circuit and/or software module . case software module software may run one processor 214. shall also noted dedicated hardware circuit may part comprise software portion run one processor 214. memory 203 may kind volatile non-volatile memory . , memory 203 may comprise plurality memory unit . memory 203 may used buffer memory buffering data performing processing one processor 214. memory 203 may used buffer memory buffering data performing processing one data receiver 202 , scene divider 204 , statistical feature evaluator 206 , motion base level determinator 208 , motion trigger level controller 210 , motion trigger controller 218. memory 203 , one processor 214 , data receiver 202 , scene divider 204 , statistical feature evaluator 206 , motion base level determinator 208 , motion trigger level controller 210 , motion trigger controller 218 may arranged communicate data bus 216. data receiver 202 configured receive data pertaining video stream depicting scene . scene may scene 100 depicted figure 1. received data may previously encoded video data representing video stream . alternatively , received data may pixel data representing different pixel image sensor used capturing video stream . pixel data yet encoded . image frame video stream comprises multiple block pixel . case data comprising previously encoded video data , data may comprise information pertaining bitrates associated encoding different block pixel video stream . information pertaining bitrates associated encoding different block pixel video stream may time resolved . received data may stored memory 203. scene divider 204 configured divide scene plurality specific portion . specific portion associated one multiple block pixel . plurality specific portion may e.g . region scene interest . whole scene may divided plurality specific portion . alternatively , part scene may divided plurality specific portion . specific portion may equal area , i.e . comprising equal number pixel ( block pixel ) . specific portion may equal shape . example , specific portion may quadrangle . specific portion may different shape . shape may one quadrangle , rhomb , circle , free form shape . different specific portion may different normal motion level . different specific portion may portion scene increased , decreased , interest . word , monitoring point view may different requirement specific portion scene . scene divider 204 may operated based user input . example , user may define one specific portion graphical representation video stream . alternatively , scene divider 204 may configured define different specific region scene using e.g . feature detection algorithm detecting feature scene . feature identification algorithm may cluster block pixel order cover identified feature . clustered block pixel may proposed specific portion scene . clustered block pixel may automatically divided specific portion scene . example feature may e.g . road , tree , fence , and/or entrance house . statistical feature evaluator 206 configured , specific portion scene , evaluate , time , statistical feature bitrate associated encoding block pixel pertaining specific portion scene . statistical feature evaluator 206 may use information pertaining bitrates associated encoding different block pixel video stream received data receiver 202. alternatively , circuitry 200 may configured encode pixel data pertaining video stream depicting scene using encoder 212. statistical feature evaluator 206 configured receive statistical feature bitrate associated encoding block pixel pertaining specific portion scene encoder 212. , encoder 212 , received statistical feature evaluated , time , statistical feature evaluator 206. hence , statistical feature evaluator 206 configured produce time resolved statistical feature bitrates associated encoding different block pixel video stream . time period evaluation statistical feature occurs may manually selected user operating circuitry 200. time period evaluation statistical feature occurs may predetermined . time period may , e.g. , certain number second , minute , hour , day , week , month , year . statistical feature may , e.g. , comprise moving average , evaluation statistical feature occurs essentially continuously . statistical feature may also determined cumulative manner . thereby , time , increasing data set upon statistical feature based . encoding block pixel may either intra-mode encoding inter-mode encoding . bitrates associated encoding block pixel may different specific portion , therefore indicate motion level respective specific portion . thus , possible first bitrate pertaining first specific portion scene relatively high , simultaneously second bitrate pertaining second specific portion scene relatively low . may interpreted motion level first specific portion relatively high , motion level second specific portion relatively low . understood bitrates pertaining different specific portion may vary independently one another . motion base level determinator 208 configured , specific portion scene , determine motion base level based evaluated statistical feature bitrate associated encoding block pixel pertaining specific portion scene . motion base level may therefore based output statistical feature evaluator 206. motion base level may , e.g. , single statistical feature , combination plurality statistical feature . thus , motion base level may arithmetic mean certain time period , , e.g. , one hour . motion base level may also formed combination first statistical feature , e.g . arithmetic mean , second statistical feature , e.g . standard deviation . understood plurality different mathematical combination statistical feature possible . understood one statistical feature may scaled constant and/or variable factor . understood one statistical feature may offset constant and/or variable term . thus , motion base line may shifted , e.g. , adding constant , scaled , e.g. , multiplying factor varying time . motion trigger level controller 210 configured , specific portion scene , set motion trigger level based motion base level corresponding specific portion scene . motion trigger level may therefore set relative motion base level . example , motion trigger level may include one evaluated statistical feature output statistical feature evaluator 206. one non-limiting example motion trigger level specific portion scene comprise setting motion trigger level sum motion base level specific portion scene multiple standard deviation bitrate pertaining specific portion scene . understood many different combination evaluated statistical feature , output statistical feature evaluator 206 , used forming motion trigger level . motion trigger controller 218 may configured trigger motion event upon bitrate associated encoding block pixel pertaining specific portion scene motion trigger level associated specific portion scene . motion trigger controller 218 may compare motion trigger level specific portion scene bitrate associated encoding block pixel specific portion scene . thereby , motion trigger controller 218 may trigger motion event case bitrate associated encoding block pixel specific portion scene motion trigger level associated specific portion scene . motion trigger controller 218 may trigger motion event one specific portion scene . understood motion trigger controller 218 may trigger motion event one specific portion scene independent specific portion scene . circuitry 200 therefore capable individually setting motion trigger level customized motion base level respective one plurality specific portion scene . circuitry 200 may capable individually triggering motion event comprising motion trigger controller 218. figure 3 flow block scheme method 300 setting motion trigger level used detection motion video stream depicting scene . method 300 comprising following act : receiving 302 data pertaining video stream depicting scene . scene may scene 100 depicted figure 1. data may encoded received 302. data may encoded received 302. image frame video stream comprises multiple block pixel . block pixel may correspond one coding unit . coding unit may , e.g. , macroblock coding tree unit . case data comprising previously encoded video data , data may comprise information pertaining bitrates associated encoding different block pixel video stream . information pertaining bitrates associated encoding different block pixel video stream may time resolved . specific portion scene may depict different region interest scene . specific portion scene may different normal motion level . act dividing 304 scene plurality specific portion may based user input . act dividing 304 scene plurality specific portion may made based feature identified scene . feature may identified feature identification algorithm . feature identification algorithm may cluster block pixel order cover identified feature . clustered block pixel may proposed specific portion scene . clustered block pixel may automatically divided specific portion scene . example feature may e.g . road , tree , fence , and/or entrance house . specific portion scene associated one block pixel . thus , act dividing 304 scene plurality specific portion thereby assigns block pixel specific portion . specific portion scene , evaluating 306 , time , statistical feature bitrate associated encoding block pixel pertaining specific portion scene . evaluation 306 may based information pertaining bitrates associated encoding different block pixel video stream received connection act receiving 302. alternatively , evaluation 306 may based statistical feature bitrate associated encoding pixel data block pixel received connection act receiving 302. time period evaluation statistical feature occurs may selected user . alternatively , time period may predetermined time period . time period may , e.g. , certain number second , minute , hour , day , week , month , year . statistical feature may , e.g. , comprise moving average , evaluation statistical feature occurs essentially continuously . statistical feature may also determined cumulative manner . thereby data set upon statistical feature based , may increase time . bitrates associated encoding block pixel may different specific portion , therefore indicate motion level respective specific portion . thus , possible first bitrate pertaining first specific portion scene relatively high , simultaneously second bitrate pertaining second specific portion scene relatively low . may interpreted motion level first specific portion relatively high , motion level second specific portion relatively low . understood bitrates pertaining different specific portion may vary independently one another . statistical feature bitrate associated encoding block pixel may primarily acquired inter-coded frame . statistical feature bitrate associated encoding block pixel may solely acquired inter-coded frame . specific portion scene , determining 308 motion base level based evaluated statistical feature bitrate associated encoding block pixel pertaining specific portion scene . motion base level may , e.g. , arithmetic mean bitrate , monitored , e.g. , hour , associated encoding block pixel . understood motion base level may updated regularly , e.g . specific hour every day , specific day every week , specific time recurrent time period . update motion base level may initiated manually user . determining 308 motion base level may comprise basing motion base level time average bitrate associated encoding block pixel pertaining specific portion scene . motion base level may based specific time recurrent time period . motion base level may based specific time recurrent time period time average bitrate associated encoding block pixel pertaining specific portion scene pertaining specific time recurrent time period . thus , first motion base level may determined first time period , second motion base level may determined second time period . example , first motion base level may determined daytime , second motion base level may determined nighttime . first second motion base level may determined based time average bitrates associated encoding block pixel pertaining specific portion scene pertaining respective specific time recurrent time period . specific portion scene , setting 310 motion trigger level based motion base level . motion trigger level may based specific time recurrent time period . understood motion trigger level may updated regularly , e.g . specific hour every day , specific day every week , specific time recurrent time period . update motion trigger level may initiated user . understood update motion base level may adjust motion trigger level accordingly . method 300 may comprise adjusting 312 master threshold weight enable joint adjustment motion trigger level specific portion scene . thus , possible globally adjust motion trigger level adjusting 312 master threshold weight . method 300 may comprise logging 314 evaluated statistical feature bitrate associated encoding block pixel heatmap scene . act logging 314 heatmap may comprise saving onto memory . memory may memory 203 circuitry discussed connection figure 2. level heatmap may set relative global motion base level scene . level heatmap may set relative motion base level specific portion scene . thereby level motion scene may logged , order retrieve historical motion level . figure 4 illustrates diagram 400 graphical representation bitrate , , time , , specific potion scene . needle say , every specific potion scene may unique diagram graphical representation bitrate , , time , t. diagram 400 comprises graphical representation 402 bitrate associated encoding block pixel pertaining specific portion scene . motion base level 406 motion trigger level 408 also illustrated figure 4. graphical representation 402 bitrate varies time . around time te , graphical representation 402 bitrate peak 404. peak 404 graphical representation 402 bitrate motion trigger level 408. thus , event causing trigger occurs around peak 404. figure 5 illustrates diagram 500 multiple motion base level multiple motion trigger level specific potion scene . , every specific potion scene may unique motion base level multiple motion trigger level . time period t1 diagram 500 motion base level 512 corresponding motion trigger level 522. motion trigger level 522 lie relatively close motion base level 512. configuration thereby sensitive trigger small amount detected motion specific portion scene . configuration may , e.g. , active scene night , motion expected . motion base level 514 time period t2 higher motion base level 512. motion trigger level 524 time period t2 higher motion trigger level 522. thus , configuration t2 sensitive configuration t1 . configuration t2 may , e.g. , active scene morning rush hour . time period t3 t4 motion base level 516. motion trigger level 526 528 different . configuration may , e.g. , active occasional high motion level scene t3 , t4 , motion base level 516 remains . figure 6 flow block scheme method 600 motion detection . method comprising following act : installing 602 camera configured capture video stream depicting scene . scene may scene 100 depicted figure 1. camera may comprise circuitry 200. setting 610 , according method first aspect , motion trigger level associated specific portion scene . specific portion scene may depict different region interest scene . specific portion scene may different normal motion level . triggering 620 motion event upon bitrate associated encoding block pixel pertaining specific portion scene motion trigger level associated specific portion scene . bitrate associated encoding block pixel pertaining specific portion scene may graphical representation 402 figure 4. motion trigger level associated specific portion scene may graphical representation 408 figure 4. triggering 620 motion event may performed close graphical representation 402 bitrate peak 404 motion trigger level 408. act setting 610 , according method first aspect , motion trigger level associated specific portion scene , may performed plurality specific portion scene . act setting 610 may performed predetermined time period . predetermined time period may order hour , day , week . predetermined time period may calibration phase method . act triggering 620 may performed predetermined time period . act triggering 620 may performed motion detection phase method . motion detection phase may occur calibration phase . calibration phase may repeated resumed motion detection phase . person skilled art realizes present invention mean limited preferred embodiment described . contrary , many modification variation possible within scope appended claim . example , method 300 may comprise optional act encoding 305 received data pertaining video stream depicting scene . encoding performed using intra inter mode encoding discussed . additionally , variation disclosed embodiment understood effected skilled person practicing claimed invention , study drawing , disclosure , appended claim . _____c : 1. method setting motion trigger level used detection motion video stream depicting scene , method comprising : receiving ( 302 ) data pertaining video stream depicting scene ; dividing ( 304 ) scene plurality specific portion ; wherein image frame video stream comprises multiple block pixel , wherein specific portion scene associated one block pixel ; specific portion scene : evaluating ( 306 ) , time , statistical feature bitrate associated encoding block pixel pertaining specific portion scene ; determining ( 308 ) motion base level based evaluated statistical feature bitrate associated encoding block pixel pertaining specific portion scene ; setting ( 310 ) motion trigger level based motion base level . 2. method according claim 1 , wherein statistical feature bitrate associated encoding block pixel primarily , preferably solely , acquired inter coded frame . 3. method according claim 1 2 , wherein act determining motion base level comprises : basing motion base level time average bitrate associated encoding block pixel pertaining specific portion scene . 4. method according one claim 1-3 , method comprising : adjusting ( 312 ) master threshold weight enable adjustment motion trigger level . 5. method according one claim 1-4 , method comprising : logging ( 314 ) evaluated statistical feature bitrate associated encoding block pixel heatmap scene . 6. method according one claim 1-5 , wherein block pixel corresponds one coding unit . 7. method according one claim 1-6 , wherein act determining motion base level comprises : basing motion base level specific time recurrent time period . 8. method according one claim 1-7 , wherein act determining motion base level comprises : basing motion base level specific time recurrent time period time average bitrate associated encoding block pixel pertaining specific portion scene pertaining specific time recurrent time period . 9. method according one claim 1-8 , wherein act setting motion trigger level comprises : basing motion trigger level specific time recurrent time period . 10. circuitry configured set motion trigger level used detection motion video stream depicting scene , circuitry comprising : data receiver ( 202 ) configured receive data pertaining video stream depicting scene , wherein image frame video stream comprises multiple block pixel ; scene divider ( 204 ) configured divide scene plurality specific portion , wherein specific portion associated one block pixel ; statistical feature evaluator ( 206 ) configured , specific portion scene , evaluate , time , statistical feature bitrate associated encoding block pixel pertaining specific portion scene ; motion base level determinator ( 208 ) configured , specific portion scene , determine motion base level based evaluated statistical feature bitrate associated encoding block pixel pertaining specific portion scene ; motion trigger level controller ( 210 ) configured , specific portion scene , set motion trigger level based motion base level corresponding specific portion scene . 11. circuitry according claim 10 , wherein data receiver ( 202 ) comprises image sensor configured capture video stream depicting scene . 12. non-transitory computer readable storing medium stored thereon program implementing method according one claim 1-9 , executed device processing capability .\"]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GsY2q4p6hJ77","executionInfo":{"status":"ok","timestamp":1615220436850,"user_tz":-60,"elapsed":880,"user":{"displayName":"MD Salem Messoud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBYoVQGAImGxJfhgMMK1daBThGNpiaASkBXr4EYpE=s64","userId":"15534871031717137879"}},"outputId":"633853f2-56b5-4953-fda7-68dc20ff968e"},"source":["# import random\n","sentsplit = re.compile('[\\n.;]')\n","\n","# Select only first 12 patents\n","g06tsents = [li.strip() for pat in g06tpatents_normalized[:12] for li in sentsplit.split(pat) if len(li)>25 and '____' not in li]\n","random.shuffle(g06tsents)\n","print(len(g06tsents))\n","open('g06tpatents_normalized-3000.txt','w').write('\\n'.join(g06tsents))\n","g06tsents[:5]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["3778\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['mean shape volume estimated using voxel colour consistency check described improves sfs approach result accurate generation three-dimensional model even sparse camera setup',\n"," 'completeness , figure 14 example schematic diagram component one module described previously , accordance example embodiment , hereafter referred generically processing system 300',\n"," 'background 3d model may rendered textured 3d mesh',\n"," 'seen magnitude 206 brightness modification example also defined maximum distance dynamic contrast curve 204 , 205 , i',\n"," '6 % , although uniqueness increased incorporating feature area surrounding node 202']"]},"metadata":{"tags":[]},"execution_count":60}]},{"cell_type":"markdown","metadata":{"id":"hSf-xebQDEv-"},"source":["let's see annotate again and see how the pretreatment affects the performance"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DQx2Q8K1pmtW","executionInfo":{"status":"ok","timestamp":1615224292018,"user_tz":-60,"elapsed":631,"user":{"displayName":"MD Salem Messoud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBYoVQGAImGxJfhgMMK1daBThGNpiaASkBXr4EYpE=s64","userId":"15534871031717137879"}},"outputId":"695385f2-21fe-4dca-c1b1-6b400db5a260"},"source":["ngrok.kill()\n","public_url = ngrok.connect(addr='8000', proto=\"http\", options={\"bind_tls\": True})\n","print(\"Tracking URL:\", public_url)"],"execution_count":98,"outputs":[{"output_type":"stream","text":["Tracking URL: NgrokTunnel: \"http://e56868e6c82b.ngrok.io\" -> \"http://localhost:8000\"\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o5ELkCA2ruju","executionInfo":{"status":"ok","timestamp":1615224749708,"user_tz":-60,"elapsed":446339,"user":{"displayName":"MD Salem Messoud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBYoVQGAImGxJfhgMMK1daBThGNpiaASkBXr4EYpE=s64","userId":"15534871031717137879"}},"outputId":"4f75a7a9-706f-453f-9725-b87e1be4bb89"},"source":["! PRODIGY_PORT=8000 PRODIGY_HOST=127.0.0.1 prodigy sense2vec.teach terms2g06t s2v_old --seeds \"linear programming, operating systems, curved screen, light sensor, software programming, product design, video quality, embedded system, physics simulation, readable code, graphic design, graphics engine, digital computers, procedural programming, linear algebra, human visual system, embedded device, electrical circuits, design software, embedded software, graph theory, single screen, specialized software, digital logic, application software, additive manufacturing, science fiction, user experience, industry standard, rendering engines, software program, software tools, white balance, remote sensing, data storage, mechanical systems, mobile computing, interactive media, wireless communications, input data, color correction, artificial neural networks, stochastic processes, programming languages, rear camera, lighting design, camera model, computer animation, circuit design, visual artist, digital technology, electric circuits, spatial data, cloud processing, distributed systems, graphics pipeline, pure data, computing technology, numerical methods, mobile platform, image processing, development environment, photo manipulation, computer graphics, machine vision, screen size, instruction sets, computer programming, file format, main processor, raw image, texture mapping, simulation software, wireless power, augmented reality, computer vision, motion detection, electronic viewfinder, computer language, digital signal processing, reference material, video streaming, handheld devices, visual communication, information technology, video production, product designer, genetic algorithms, user testing, high frame rate, color grading, graphical design, color profile, memory management, modular design, logic programming, computer program, dynamic range, visual perception, computer simulations, classical mechanics, wireless technology, video conferencing, software project, machine learning algorithms, wireless networking, color space, user interfaces, small screen, word processing, computer power, image sensor, sound processing, digital media, visual media, software tool, camera tracking, optical system, functional analysis, camera sensors, high resolution, eye tracking, programming environment, color depth, mechanical design, computer modeling, complex analysis, trigonometric functions, software implementation, projection mapping, inverse kinematics, video encoding, online learning, audio processing, digital video, computer architecture, data processing, processing speed, analog electronics, computing power, finite element analysis, color display, computer networking, computational power, live view, mechanical engineering, software development, programming language, signal processing, computer technology, image quality, image stabilization, software architecture, high dynamic range, raw format, behavior analysis, digital processing, error correction, computer code, virtual environments, digital circuits, high performance computing, external device, software stack, big data analysis, base station, color management, 3d models, real camera, software level, optical image stabilization, control theory, file management, computer programs, image editing, unsupervised learning, video processing, computer science, camera lenses, computer hardware, haptic feedback, movie making, vector images, visual effects, video recording, rapid prototyping, picture quality, system performance, mobile processor, computer scientists, computer languages, positional tracking, distributed computing, image processor, sensor size, spatial reasoning, image manipulation, deep learning, image compression, natural language processing, render target, parallel processing, optical viewfinder, differential equations, interface design, data structures, chromatic aberration, visual feedback, body tracking, mouse input, human-computer interaction, rendering pipeline, display size, dynamical systems, computational complexity, texture maps, vector graphics, light meter, reinforcement learning, edge detection, high precision, processing power, face detection, design process, digital system, analog circuits, software engineering, color temperature, software design, display device, user interface design, manual focus, input device, contrast ratio, artificial intelligence, embedded devices, civil engineering, color calibration, portable devices, software suite, dynamic contrast, global shutter, computational methods, neural networks, camera system, batch processing, visual design, wireless communication, image recognition, computer software, face tracking, game engine, process control, state machines, color image, image analysis, depth map, interior design, embedded hardware, colour correction, virtual reality, deferred rendering, parallel computing, assembly language, physical design, digital systems, audio engineering, video stabilization, numerical analysis, fluid dynamics, open standard, mathematical modeling, web browsing, motion picture, still life, processor speed, motion pictures, pixel density, data manipulation, human vision, media players\""],"execution_count":99,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n","  \"\"\")\n"],"name":"stdout"},{"output_type":"stream","text":["t=2021-03-08T17:25:08+0000 lvl=warn msg=\"failed to open private leg\" id=2065c84f00e6 privaddr=localhost:8000 err=\"dial tcp 127.0.0.1:8000: connect: connection refused\"\n","t=2021-03-08T17:25:10+0000 lvl=warn msg=\"failed to open private leg\" id=31da9206767e privaddr=localhost:8000 err=\"dial tcp 127.0.0.1:8000: connect: connection refused\"\n"],"name":"stderr"},{"output_type":"stream","text":["Starting with seed keys: ['linear_programming|NOUN', 'operating_systems|NOUN', 'curved_screen|NOUN', 'light_sensor|NOUN', 'software_programming|NOUN', 'product_design|NOUN', 'video_quality|NOUN', 'embedded_system|NOUN', 'physics_simulation|NOUN', 'readable_code|NOUN', 'graphic_design|NOUN', 'graphics_engine|NOUN', 'digital_computers|NOUN', 'procedural_programming|NOUN', 'linear_algebra|NOUN', 'human_visual_system|NOUN', 'embedded_device|NOUN', 'electrical_circuits|NOUN', 'design_software|NOUN', 'embedded_software|NOUN', 'graph_theory|NOUN', 'single_screen|NOUN', 'specialized_software|NOUN', 'digital_logic|NOUN', 'application_software|NOUN', 'additive_manufacturing|NOUN', 'science_fiction|NOUN', 'user_experience|NOUN', 'industry_standard|NOUN', 'rendering_engines|NOUN', 'software_program|NOUN', 'software_tools|NOUN', 'white_balance|NOUN', 'remote_sensing|NOUN', 'data_storage|NOUN', 'mechanical_systems|NOUN', 'mobile_computing|NOUN', 'interactive_media|NOUN', 'wireless_communications|NOUN', 'input_data|NOUN', 'color_correction|NOUN', 'artificial_neural_networks|NOUN', 'stochastic_processes|NOUN', 'programming_languages|NOUN', 'rear_camera|NOUN', 'lighting_design|NOUN', 'camera_model|NOUN', 'computer_animation|NOUN', 'circuit_design|NOUN', 'visual_artist|NOUN', 'digital_technology|NOUN', 'electric_circuits|NOUN', 'spatial_data|NOUN', 'cloud_processing|NOUN', 'distributed_systems|NOUN', 'graphics_pipeline|NOUN', 'pure_data|NOUN', 'computing_technology|NOUN', 'numerical_methods|NOUN', 'mobile_platform|NOUN', 'image_processing|NOUN', 'development_environment|NOUN', 'photo_manipulation|NOUN', 'computer_graphics|NOUN', 'machine_vision|NOUN', 'screen_size|NOUN', 'instruction_sets|NOUN', 'computer_programming|NOUN', 'file_format|NOUN', 'main_processor|NOUN', 'raw_image|NOUN', 'texture_mapping|NOUN', 'simulation_software|NOUN', 'wireless_power|NOUN', 'augmented_reality|NOUN', 'computer_vision|NOUN', 'motion_detection|NOUN', 'electronic_viewfinder|NOUN', 'computer_language|NOUN', 'digital_signal_processing|NOUN', 'reference_material|NOUN', 'video_streaming|NOUN', 'handheld_devices|NOUN', 'visual_communication|NOUN', 'information_technology|NOUN', 'video_production|NOUN', 'product_designer|NOUN', 'genetic_algorithms|NOUN', 'user_testing|NOUN', 'high_frame_rate|NOUN', 'color_grading|NOUN', 'graphical_design|NOUN', 'color_profile|NOUN', 'memory_management|NOUN', 'modular_design|NOUN', 'logic_programming|NOUN', 'computer_program|NOUN', 'dynamic_range|NOUN', 'visual_perception|NOUN', 'computer_simulations|NOUN', 'classical_mechanics|NOUN', 'wireless_technology|NOUN', 'video_conferencing|NOUN', 'software_project|NOUN', 'machine_learning_algorithms|NOUN', 'wireless_networking|NOUN', 'color_space|NOUN', 'user_interfaces|NOUN', 'small_screen|NOUN', 'word_processing|NOUN', 'computer_power|NOUN', 'image_sensor|NOUN', 'sound_processing|NOUN', 'digital_media|NOUN', 'visual_media|NOUN', 'software_tool|NOUN', 'camera_tracking|NOUN', 'optical_system|NOUN', 'functional_analysis|NOUN', 'camera_sensors|NOUN', 'high_resolution|NOUN', 'eye_tracking|NOUN', 'programming_environment|NOUN', 'color_depth|NOUN', 'mechanical_design|NOUN', 'computer_modeling|NOUN', 'complex_analysis|NOUN', 'trigonometric_functions|NOUN', 'software_implementation|NOUN', 'projection_mapping|NOUN', 'inverse_kinematics|NOUN', 'video_encoding|NOUN', 'online_learning|NOUN', 'audio_processing|NOUN', 'digital_video|NOUN', 'computer_architecture|NOUN', 'data_processing|NOUN', 'processing_speed|NOUN', 'analog_electronics|NOUN', 'computing_power|NOUN', 'finite_element_analysis|NOUN', 'color_display|NOUN', 'computer_networking|NOUN', 'computational_power|NOUN', 'live_view|NOUN', 'mechanical_engineering|NOUN', 'software_development|NOUN', 'programming_language|NOUN', 'signal_processing|NOUN', 'computer_technology|NOUN', 'image_quality|NOUN', 'image_stabilization|NOUN', 'software_architecture|NOUN', 'high_dynamic_range|NOUN', 'raw_format|NOUN', 'behavior_analysis|NOUN', 'digital_processing|NOUN', 'error_correction|NOUN', 'computer_code|NOUN', 'virtual_environments|NOUN', 'digital_circuits|NOUN', 'high_performance_computing|NOUN', 'external_device|NOUN', 'software_stack|NOUN', 'big_data_analysis|NOUN', 'base_station|NOUN', 'color_management|NOUN', '3d_models|NOUN', 'real_camera|NOUN', 'software_level|NOUN', 'optical_image_stabilization|NOUN', 'control_theory|NOUN', 'file_management|NOUN', 'computer_programs|NOUN', 'image_editing|NOUN', 'unsupervised_learning|NOUN', 'video_processing|NOUN', 'computer_science|NOUN', 'camera_lenses|NOUN', 'computer_hardware|NOUN', 'haptic_feedback|NOUN', 'movie_making|NOUN', 'vector_images|NOUN', 'visual_effects|NOUN', 'video_recording|NOUN', 'rapid_prototyping|NOUN', 'picture_quality|NOUN', 'system_performance|NOUN', 'mobile_processor|NOUN', 'computer_scientists|NOUN', 'computer_languages|NOUN', 'positional_tracking|NOUN', 'distributed_computing|NOUN', 'image_processor|NOUN', 'sensor_size|NOUN', 'spatial_reasoning|NOUN', 'image_manipulation|NOUN', 'deep_learning|NOUN', 'image_compression|NOUN', 'natural_language_processing|NOUN', 'render_target|NOUN', 'parallel_processing|NOUN', 'optical_viewfinder|NOUN', 'differential_equations|NOUN', 'interface_design|NOUN', 'data_structures|NOUN', 'chromatic_aberration|NOUN', 'visual_feedback|NOUN', 'body_tracking|NOUN', 'mouse_input|NOUN', 'human-computer_interaction|NOUN', 'rendering_pipeline|NOUN', 'display_size|NOUN', 'dynamical_systems|NOUN', 'computational_complexity|NOUN', 'texture_maps|NOUN', 'vector_graphics|NOUN', 'light_meter|NOUN', 'reinforcement_learning|NOUN', 'edge_detection|NOUN', 'high_precision|NOUN', 'processing_power|NOUN', 'face_detection|NOUN', 'design_process|NOUN', 'digital_system|NOUN', 'analog_circuits|NOUN', 'software_engineering|NOUN', 'color_temperature|NOUN', 'software_design|NOUN', 'display_device|NOUN', 'user_interface_design|NOUN', 'manual_focus|NOUN', 'input_device|NOUN', 'contrast_ratio|NOUN', 'artificial_intelligence|NOUN', 'embedded_devices|NOUN', 'civil_engineering|NOUN', 'color_calibration|NOUN', 'portable_devices|NOUN', 'software_suite|NOUN', 'dynamic_contrast|NOUN', 'global_shutter|NOUN', 'computational_methods|NOUN', 'neural_networks|NOUN', 'camera_system|NOUN', 'batch_processing|NOUN', 'visual_design|NOUN', 'wireless_communication|NOUN', 'image_recognition|NOUN', 'computer_software|NOUN', 'face_tracking|NOUN', 'game_engine|NOUN', 'process_control|NOUN', 'state_machines|NOUN', 'color_image|NOUN', 'image_analysis|NOUN', 'depth_map|NOUN', 'interior_design|NOUN', 'embedded_hardware|NOUN', 'colour_correction|NOUN', 'virtual_reality|NOUN', 'deferred_rendering|NOUN', 'parallel_computing|NOUN', 'assembly_language|NOUN', 'physical_design|NOUN', 'digital_systems|NOUN', 'audio_engineering|NOUN', 'video_stabilization|NOUN', 'numerical_analysis|NOUN', 'fluid_dynamics|NOUN', 'open_standard|NOUN', 'mathematical_modeling|NOUN', 'web_browsing|NOUN', 'motion_picture|NOUN', 'still_life|NOUN', 'processor_speed|NOUN', 'motion_pictures|NOUN', 'pixel_density|NOUN', 'data_manipulation|NOUN', 'human_vision|NOUN', 'media_players|NOUN']\n","\n","✨  Starting the web server at http://127.0.0.1:8000 ...\n","Open the app in your browser and start annotating!\n","\n","\n","\u001b[38;5;2m✔ Saved 203 annotations to database SQLite\u001b[0m\n","Dataset: terms2g06t\n","Session ID: 2021-03-08_17-25-11\n","\n","^C\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i6p_vGY6u-Wy","executionInfo":{"status":"ok","timestamp":1615224807411,"user_tz":-60,"elapsed":3413,"user":{"displayName":"MD Salem Messoud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBYoVQGAImGxJfhgMMK1daBThGNpiaASkBXr4EYpE=s64","userId":"15534871031717137879"}},"outputId":"ddc02b38-1852-4206-d8c9-c4cc99fa22d1"},"source":["!prodigy sense2vec.to-patterns terms2g06t en_core_web_sm TECH --output-file terms2g06tpatterns.jsonl"],"execution_count":102,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n","  \"\"\")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZEsmRfh7wL-Z","executionInfo":{"status":"ok","timestamp":1615224819436,"user_tz":-60,"elapsed":910,"user":{"displayName":"MD Salem Messoud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBYoVQGAImGxJfhgMMK1daBThGNpiaASkBXr4EYpE=s64","userId":"15534871031717137879"}},"outputId":"7ae2d069-ca3e-4dca-fce7-d686833b2718"},"source":["ngrok.kill()\n","public_url = ngrok.connect(addr='8000', proto=\"http\", options={\"bind_tls\": True})\n","print(\"Tracking URL:\", public_url)"],"execution_count":104,"outputs":[{"output_type":"stream","text":["Tracking URL: NgrokTunnel: \"http://b0bfeb2c2e10.ngrok.io\" -> \"http://localhost:8000\"\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RrR0d6_rwQjk","executionInfo":{"status":"ok","timestamp":1615225358037,"user_tz":-60,"elapsed":536925,"user":{"displayName":"MD Salem Messoud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBYoVQGAImGxJfhgMMK1daBThGNpiaASkBXr4EYpE=s64","userId":"15534871031717137879"}},"outputId":"51686408-c9a5-4d4e-a640-c2b55e6e86c2"},"source":["! PRODIGY_PORT=8000 PRODIGY_HOST=127.0.0.1  prodigy ner.manual annotatedg06t2 en_core_web_sm g06tpatents_normalized-3000.txt --loader txt --patterns terms2g06tpatterns.jsonl --label TECH"],"execution_count":105,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n","  \"\"\")\n","Using 1 label(s): TECH\n"],"name":"stdout"},{"output_type":"stream","text":["t=2021-03-08T17:33:43+0000 lvl=warn msg=\"failed to open private leg\" id=1359b90d7586 privaddr=localhost:8000 err=\"dial tcp 127.0.0.1:8000: connect: connection refused\"\n","t=2021-03-08T17:33:45+0000 lvl=warn msg=\"failed to open private leg\" id=53f9b64d85c9 privaddr=localhost:8000 err=\"dial tcp 127.0.0.1:8000: connect: connection refused\"\n"],"name":"stderr"},{"output_type":"stream","text":["\n","✨  Starting the web server at http://127.0.0.1:8000 ...\n","Open the app in your browser and start annotating!\n","\n","\n","\u001b[38;5;2m✔ Saved 87 annotations to database SQLite\u001b[0m\n","Dataset: annotatedg06t2\n","Session ID: 2021-03-08_17-33-43\n","\n","^C\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CmaGqmYjy1_d","executionInfo":{"status":"ok","timestamp":1615225366651,"user_tz":-60,"elapsed":3422,"user":{"displayName":"MD Salem Messoud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBYoVQGAImGxJfhgMMK1daBThGNpiaASkBXr4EYpE=s64","userId":"15534871031717137879"}},"outputId":"dc10dbeb-6855-4888-befe-a283c7839ec3"},"source":["! prodigy db-out annotatedg06t2 > annotatedg06t2.jsonl"],"execution_count":106,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n","  \"\"\")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"PeKMowlu1_-9","executionInfo":{"status":"ok","timestamp":1615225368347,"user_tz":-60,"elapsed":607,"user":{"displayName":"MD Salem Messoud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBYoVQGAImGxJfhgMMK1daBThGNpiaASkBXr4EYpE=s64","userId":"15534871031717137879"}},"outputId":"10c99e0d-93d7-4bfa-907a-2bd9a4a5256c"},"source":["#shutil.copy2('annotatedg06t2.jsonl', basedir)"],"execution_count":107,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/MyDrive/Information Retrieval/inforet/inforet7/annotatedg06t2.jsonl'"]},"metadata":{"tags":[]},"execution_count":107}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vHcEnM91-_VS","executionInfo":{"status":"ok","timestamp":1615226859456,"user_tz":-60,"elapsed":2257,"user":{"displayName":"MD Salem Messoud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBYoVQGAImGxJfhgMMK1daBThGNpiaASkBXr4EYpE=s64","userId":"15534871031717137879"}},"outputId":"81e852ea-4395-43ba-ff41-c8c9b5cebe69"},"source":["!prodigy db-in annotatedg06t2 annotatedg06t2.jsonl"],"execution_count":126,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n","  \"\"\")\n","\u001b[38;5;2m✔ Imported 87 annotations to 'annotatedg06t2' (session\n","2021-03-08_18-07-38) in database SQLite\u001b[0m\n","Found and keeping existing \"answer\" in 87 examples\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DGTdHyWr2J0u","executionInfo":{"status":"ok","timestamp":1615226978713,"user_tz":-60,"elapsed":108486,"user":{"displayName":"MD Salem Messoud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBYoVQGAImGxJfhgMMK1daBThGNpiaASkBXr4EYpE=s64","userId":"15534871031717137879"}},"outputId":"4ded5a28-13b4-4f43-b99d-519a2b71f322"},"source":["!prodigy train ner annotatedg06t2 en_vectors_web_lg --init-tok2vec ./tok2vec_cd8_model289.bin --output ./tmp_model --eval-split 0.2"],"execution_count":127,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n","  \"\"\")\n","⠼ Loading 'en_vectors_web_lg'...tcmalloc: large alloc 1285169152 bytes == 0x557ac4634000 @  0x7f22c1a1f1e7 0x7f22be4d446e 0x7f22be528e7c 0x7f22be529aaf 0x7f22be5cb470 0x557ab37690e4 0x557ab3768de0 0x557ab37dd6f5 0x557ab37d7b0e 0x557ab376a77a 0x557ab37d986a 0x557ab37d7b0e 0x557ab37e09fc 0x7f22ad9157da 0x7f22ad91e80a 0x557ab37697f2 0x557ab37dcd75 0x7f22ad9134c8 0x7f22ad91589a 0x7f22ad92557d 0x557ab36aa337 0x7f22adc29be0 0x557ab3769050 0x557ab385a99d 0x557ab37dcfe9 0x557ab37d7e0d 0x557ab376a77a 0x557ab37d8a45 0x557ab376a69a 0x557ab37dce50 0x557ab37d7e0d\n","\u001b[2K\u001b[38;5;2m✔ Loaded model 'en_vectors_web_lg'\u001b[0m\n","Created and merged data for 85 total examples\n","Using 68 train / 17 eval (split 20%)\n","Component: ner | Batch size: compounding | Dropout: 0.2 | Iterations: 10\n","\u001b[38;5;2m✔ Initializing with tok2vec weights ./tok2vec_cd8_model289.bin\u001b[0m\n","embed_rows: 10000 | require_vectors: True | cnn_maxout_pieces: 3 |\n","token_vector_width: 128 | conv_depth: 8 | nr_feature_tokens: 3 |\n","pretrained_vectors: en_vectors_web_lg.vectors | pretrained_dims: 300\n","\u001b[38;5;4mℹ Baseline accuracy: 0.000\u001b[0m\n","\u001b[1m\n","=========================== ✨  Training the model ===========================\u001b[0m\n","\n","#    Loss       Precision   Recall     F-Score \n","--   --------   ---------   --------   --------\n"," 1     261.21       0.000      0.000      0.000\n"," 2      52.71       0.000      0.000      0.000\n"," 3      74.13       0.000      0.000      0.000\n"," 4      46.13       0.000      0.000      0.000\n"," 5      30.20      33.333     11.111     16.667\n"," 6      27.01     100.000     22.222     36.364\n"," 7      26.39     100.000     22.222     36.364\n"," 8      17.70     100.000     11.111     20.000\n"," 9      16.87     100.000     11.111     20.000\n","10      19.34     100.000     22.222     36.364\n","\u001b[1m\n","============================= ✨  Results summary =============================\u001b[0m\n","\n","Label   Precision   Recall   F-Score\n","-----   ---------   ------   -------\n","TECH      100.000   22.222    36.364\n","\n","\n","Best F-Score   \u001b[38;5;2m36.364\u001b[0m\n","Baseline       0.000              \n","\n","\u001b[38;5;2m✔ Saved model: /content/tmp_model\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9ZFkfsKpDjGk"},"source":["train-curve"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kY-umT5iDczy","executionInfo":{"status":"ok","timestamp":1615227589017,"user_tz":-60,"elapsed":206173,"user":{"displayName":"MD Salem Messoud","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBYoVQGAImGxJfhgMMK1daBThGNpiaASkBXr4EYpE=s64","userId":"15534871031717137879"}},"outputId":"ced9a4c8-7563-4593-fe48-3b72e213f1af"},"source":["!prodigy train-curve ner annotatedg06t2 en_vectors_web_lg --init-tok2vec ./tok2vec_cd8_model289.bin  --eval-split 0.2\n"],"execution_count":128,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n","  \"\"\")\n","\u001b[38;5;2m✔ Starting with model 'en_vectors_web_lg'\u001b[0m\n","Training 4 times with 25%, 50%, 75%, 100% of the data\n","\u001b[1m\n","=============================== ✨  Train curve ===============================\u001b[0m\n","%      Accuracy   Difference\n","----   --------   ----------\n","tcmalloc: large alloc 1285169152 bytes == 0x5641cc560000 @  0x7f79399291e7 0x7f79363de46e 0x7f7936432e7c 0x7f7936433aaf 0x7f79364d5470 0x5641b9d440e4 0x5641b9d43de0 0x5641b9db86f5 0x5641b9db2b0e 0x5641b9d4577a 0x5641b9db486a 0x5641b9db2b0e 0x5641b9dbb9fc 0x7f792581f7da 0x7f792582880a 0x5641b9d447f2 0x5641b9db7d75 0x7f792581d4c8 0x7f792581f89a 0x7f792582f57d 0x5641b9c85337 0x7f7925b33be0 0x5641b9d44050 0x5641b9e3599d 0x5641b9db7fe9 0x5641b9db2e0d 0x5641b9d4577a 0x5641b9db3a45 0x5641b9d4569a 0x5641b9db7e50 0x5641b9db2e0d\n","  0%       0.00   \u001b[38;5;250mbaseline\u001b[0m\n"," 25%       0.00   \u001b[38;5;1m+0.00\u001b[0m\n","tcmalloc: large alloc 1285169152 bytes == 0x56422385c000 @  0x7f79399291e7 0x7f79363de46e 0x7f7936432e7c 0x7f7936433aaf 0x7f79364d5470 0x5641b9d440e4 0x5641b9d43de0 0x5641b9db86f5 0x5641b9db2b0e 0x5641b9d4577a 0x5641b9db486a 0x5641b9db2b0e 0x5641b9dbb9fc 0x7f792581f7da 0x7f792582880a 0x5641b9d447f2 0x5641b9db7d75 0x7f792581d4c8 0x7f792581f89a 0x7f792582f57d 0x5641b9c85337 0x7f7925b33be0 0x5641b9d44050 0x5641b9e3599d 0x5641b9db7fe9 0x5641b9db2e0d 0x5641b9d4577a 0x5641b9db3a45 0x5641b9d4569a 0x5641b9db7e50 0x5641b9db2e0d\n"," 50%       0.00   \u001b[38;5;1m+0.00\u001b[0m\n"," 75%      33.33   \u001b[38;5;2m+33.33\u001b[0m\n","100%      36.36   \u001b[38;5;2m+3.03\u001b[0m\n","\n","\u001b[38;5;2m✔ Accuracy improved in the last sample\u001b[0m\n","As a rule of thumb, if accuracy increases in the last segment, this could\n","indicate that collecting more annotations of the same type will improve the\n","model further.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bhK0AW2pD8Et"},"source":["it is sure that in this part we have clearly reduced the number of annotations. Annotating more will improve the accuracy. The decrease in accuracy is also due to the inter-annotator agreement, and annotation errors. \n","Moreover, we can see that the pretreatment improved the precision, but the recall leaves as desired"]}]}